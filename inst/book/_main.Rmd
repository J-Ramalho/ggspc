--- 
title: "indust<b style='color:#38598CFF'>R</b>ial data science"
subtitle: Case studies in product development and manufacturing
author: "João Ramalho"
date: "`r Sys.Date()`"
# output: pdf_document
site: bookdown::bookdown_site
output: 
  bookdown::gitbook:
    includes:
      in_header: includeme.html
    css: industRialds.css
    fontsettings:
      theme: white
      family: sans
      size: 1
# documentclass: book
bibliography:
- book.bib
- packages.bib
biblio-style: apalike
link-citations: yes
editor_options:
  chunk_output_type: console
nocite: |
  @Broc2016, @Cano2012, @Cano2015, @Munro2015, @Scrucca2004, @Bass2007
description: ''
github-repo: "J-Ramalho/industRialds"
cover-image: "img/cover3.png"
---

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	comment = NA
	# out.width = "80%"
)


# # automatically create a bib database for R packages
# knitr::write_bib(c(
#   .packages(), 'bookdown', 'knitr', 'rmarkdown'
# ), 'packages.bib')



# book website:
# https://j-ramalho.github.io/industRialstatistics/
```

# (PART) INTRODUCTION {-}

# Welcome {-}

<a href="">
  <img src="img/cover3.png" width="250" style="margin: 0 1em 0 1em" align="right"/>
</a> 

This book provides examples on how to use Data Science tools and techniques in Product Development and Manufacturing. These examples are grouped in Case Studies in a "cookbook" approach, making it easier to directly adopt the tools. They come from varied manufacturing industries, mostly where repetitive production in massive quantities is involved, such as:

+ pharmaceuticals
+ food
+ electronics
+ watch making
+ automotive

Product Development and Manufacturing are very important activities in society because bringing innovative products to the market has an immense potential to improve the quality of life of everyone. 

Additionally Data Science brings new powerful approaches to the engineering and manufacturing of consumer goods, helping minimising environmental impact, improving quality and keeping costs under control.

**How to use this book**

We assume the reader is familiar with product development and manufacturing quality methodologies such as dmaic and six sigma and the associated statistical concepts. Furthermore it is considered that he brings at least a beginner knowledge on R. The book focus is on putting these areas together.

Being a collection of case studies, the book is better used as a reference book. To get to the desired section either use the later navigation bar or refer to the detailed table of contents which includes a full list of Case Studies, Datasets and R functions. 

For a first reading, the book can be taken from beginning to the end as the Case Studies are organized according the a logical product development flow. The book starts with Case Studies in the domain of Design for Six Sigma. These are some practical tools in that help prioritizing problems and get focus on how to tackle them. The next group of Case Studies is in the domain of Measurement System Analysis, an initial important step when developing a product or manufacturing process. Here is discussed how to analyse the response of a measurement device in terms of its bias and its uncertainty. The next big group of Case Studies is the Design of Experiments. This corresponds to the core of the R&D activities and provides approaches to minimize the quantity of trials and time to reach to a sufficient knowledge of how the product or system works and how to obtain the right balance on its features and properties to obtain the desired output. A final group of Case Studies presents ways to get the manufacturing process in control according to what was defined in the product development phase. These are the well known Statistical Process Control and Capability studies.

Additionally we refer to several good quality books on both Data Science and Product Development that have served to provide the required theoretical  background. These cover key disciplines such as six sigma, statistics and computer programming. This book aims complementing them and showcase how to benefit from recent software in this area.

**Acknowledgements**

I would like to express my gratitude to the instructors and colleagues who have spent time sharing their knowledge, answering my questions and giving me inputs: Enrico Chavez, Iegor Rudnytskyi, Giulia Ruggeri, Harry Handerson and Bobby Stuijfzand from the EPFL ADSCV team; Jean-Vincent Le Bé, Jasmine Petry, Yvan Bouza, James Clulow and Akos Spiegel from the Nestlé STC team; Frank Paris from DOQS; Théophile Emmanouilidis and Sélim Ach from Thoth.

To report any issue or make suggestions please open an issue on the book repository:
[industRialds/issues](https://github.com/J-Ramalho/industRialds/issues)

**About the authors**

<b style="color:#104e8b">João Ramalho</b> is a Senior Industrial Data Scientist with more than 20 years of experience in the manufacturing industry. He's been in varied positions in R&D, Operations and IT at Philip Morris, Rolex and Nestlé. He holds a Master in Mechanical Engineering from the IST of Lisbon, a PMP certification from the Project Management Institute and a Data Science certification from DataCamp. He's currently specializing in Data Visualization at the Swiss technical university EPFL. See full profile at [j-ramalho.github.io](https://j-ramalho.github.io/)

<!--chapter:end:index.Rmd-->

# R Toolbox

## R programming

Why R? Many tools exist to do Data Analysis and Statistics with different degrees of power and difficulty such as:

* Spreadsheets: Excel, Libreoffice, Numbers
* Proprietary software: Minitab, Mathlab
* Programming languages: Visual Basic, R, Python, Julia
* Databases: sqlite, postgre, mysql, mongodb

Choosing the right set of tools for Data Science is often not a very scientific task. Mostly is a matter of what is available and what our colleagues, customers or suppliers use. As with everything it is important to remain open to evaluate new tools and approaches and even to be able to combine them.

In this book we've chosen to provide all examples in R which is a free software environment for statistical computing and graphics https://www.r-project.org/
Besides taste and personnal preference R brings a significant number of specific advantages in the field of Industrial Data Science:

1. R allows for reproducible research

This because the algorithms and functions defined to make the calculations can be inspected and all results can be fully reproduced and audited. This is known as reproducible research and is a critical aspect in all areas where a proof is needed such as in equipment validation and product quality reporting.

2. R functions, tools and programs can be adapted and improved

Being an open source language, all R libraries and packages added to the basic environment can be not only audited but adapted and improved This is very important as when we enter into details every industry has a slight different way of doing things, different naming conventions, different coeficients and so on.

3. R is extensible

R is compatible with most other software on the market and is an excellent "glue" tool allowing for example for data loading from excel files, producing reports in pdf and even building complete dashboards in the form of web pages. 

```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	comment = NA
	# out.width = "80%"
)
```

```{r}
library(tidyverse)
library(readxl)
library(knitr)
library(qcc)
filter <- dplyr::filter
select <- dplyr::select
```

## R Datasets

All examples presented throughout the book are published either fully anonymized or the with the original entities authorization and reference. All datasets are made available for further exploration. The books original datasets are available in the books github repository. Instructions for downloading them are presented specifically for each dataset, the general approach is described below.

Get from github the book companion package:

```
devtools::install_github("J-Ramalho/industRial")
```

Datasets become immediatly available by invoking the package name and the dataset, e.g.:

```{r}
head(industRial::dial_control)
```

If repetitive use is required then it is best to directly load the package in the current session:

```{r}
library(industRial)
```

as the package is loaded the dataset is in memory and ready to go, e.g.:

```{r}
dial_control %>%
  head() %>%
  kable()
```

The dateset can be used and manipulated like any other dataset created in the session or loaded otherwise. For example it can be filtered and assigned to a new variable name:

```{r}
dial_peter <- dial_control %>%
  filter(Operator == "Peter") 
dial_peter %>%
  head(2) %>%
  kable()
```

## R Packages

All tools applied throughout this book are available in the form of packages of the programming language R. They're all available for downloaded at no cost.

In all sections packages are loaded as they become needed. We've prefered this approach instead of a grouped loading in the begining of the chapter because this helps reminding which package the functions come from. 

When needed we also present code required to handle the masking of functions with the same name from those different packages. In the example below the filter and select functions are made explicit to avoid having them masked by the stats package in case it has been loaded before.

### Industrial packages

#### industRial

The `{industRial}` package is an original companion package developed for this book that besides containing the datasets from all cases has varied functions to print and customise the aesthetics of spc charts. They're built on top of the tidyverse package. They aim for an easier modification of the control chart rules and customisation of the plot aesthetics when compared with other more advanced packages for spc such as the qcc. The industRial package can be simply downloaded from github with the following command:

```{}
devtools::install_github("J-Ramalho/industRial")
```

#### six sigma

SixSigma is a very complete and robust package by Emilio L.Cano [@Cano2015]. It provides well many well tested functions in the area of quality management.

#### qcc

qcc is another extremely complete and solid package. It was developped and is maintained by Luca Scrucca and offers a very large range of statistical process control charts and capability analysis. Short examples in its vignette: [qcc vignette](https://luca-scr.github.io/qcc/articles/qcc.html)

#### qicharts2

I recommend qichart2 specifically for the nice pareto plots. As many niche packages we need to be awere that the number of contributers is small meaning that it cannot be as thouroughly tested as community packages.

#### DoE.base

This package is one of the most complete and vast packages in Design of Experiements. It is a first of a large suite of packages on the topic, it has vast functionality and is extremely well documented.

[DoE.base](http://prof.beuth-hochschule.de/groemping/software/doe/?L=1&print=1)

### All purpose packages

The amount of packages available is extremely large and growing very fast. When selecting new packages it is recommended to check the latest package update. Packages that have had no improvements since more than a couple of years should be questionned. The field evolves rapidly and compatibility and other issues can become painfull. A way to obtain statistics on package history is on [metacran](https://www.r-pkg.org/) or [RStudio package manager](https://packagemanager.rstudio.com/). 

Below a comprehensive list of the different packages used in the book:

```{r}
installed_packages <- bind_rows(
  installed.packages(lib.loc = .Library) %>% as_tibble(),
  installed.packages(lib.loc = "/home/joao/R/x86_64-pc-linux-gnu-library/4.0/") %>% 
    as_tibble()
)

recommended_packages <- read_excel("data/rpackages.xlsx") %>%
  filter(Used == "y") %>%
  left_join(installed_packages) %>%
  select(Package, Area, Version) %>%
  arrange(Area)

recommended_packages %>%
  kable()
```

### Publishing packages

This book has been written using the R package `{Bookdown}` from @Xie2016 further customized with a layout developped by [Matthew J. C. Crump](https://community.rstudio.com/t/bookdown-contest-submission-gitbook-style-tufte-style-for-web-book/11666). Plot themes have been adapted from the package `{cowplot}` by Claus O.Wilke.

## R Session

An important aspect to ensure reproducibility of the examples along the time and between users is to have the same programming setup. We're showing below our setup at the time of rendering the book. 

```{r}
devtools::session_info()[[1]]
```

## Disclaimer

This book presents a variety of software tools and recommended approaches for industrial data analysis. It is incumbent upon the user to execute judgement in their use. The author does not provide any guarantee, expressed or implied, with regard to the general or specific applicability of the software, the range of errors that may be associated with it, or the appropriateness of using them in any subsequent calculation, design, or decision process. The author accepts no responsibility for damages, if any, suffered by any reader or user of this handbook as a result of decisions made or actions taken on information contained therein.





<!--chapter:end:1_Rtoolbox.Rmd-->

```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	comment = NA
	# out.width = "80%"
)
```

# Statistical Process Control {#SPC}

Keeping the variability of an industrial process under control is one of the most important objectives in manufacturing. Based on expert knowledge or on detailed functional analysis the product and process parameters that are critical to quality are identified and selected for close follow-up. The most common and effective way for such follow-up is the Statistical Process Control which is done by using control charts.

**The syringe injection molding process**

<div class="marginnote">

```{r echo=FALSE, out.width="100%", fig.align='center', fig.cap="A clean room for syringe injection molding with fully automatised handling"}
knitr::include_graphics("img/syringe_molding_bw.jpg")
# sources:
# https://en.wikipedia.org/wiki/Injection_moulding
# https://en.wikipedia.org/wiki/Syringe
# https://www.sio2ms.com/wp-content/uploads/2020/09/Dimensional-Stability-of-a-Multicavity-Injection-Molded-Article.pdf
# https://www.starrapid.com/wp-content/uploads/2014/08/Star-PIM-Tolerance-Tables-2014-1.pdf
# http://galachem.ru/upload/pdf/Hamilton-Syringe-and-Needle-Catalog.pdf
# https://www.schott.com/schweiz/french/news/press.html?NID=com5453&freturl=%2Fschweiz%2Ffrench%2Fcontact%2Fmail_form.html
# (image) https://www.manufacturingchemist.com/news/article_page/Syringe_benefits/42354
```

</div>

## Control charts

There are many types of control charts and in this case study we're demonstrating the xbar and R charts. These two charts are often used together and are suited to the control the mean and the variability of a continuous variable.

Bamako Lightening is a company that manufactures lamps. The weight of each lamp is critical to the quality of the product. The Production Operator monitors the production process using xbar and R-charts. Samples are taken of six lamps every hour and their means and ranges plotted on control charts. Data is available representing samples taken a period of 25 hours of production.

Loading packages for data loading and cleaning:

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(industRial)
library(stats)
filter <- dplyr::filter
select <- dplyr::select
```

Looking at the first five lines to confirm and assess the quality of our data for further processing. 

```{r}
head(syringe_diameter) %>%
    kable()
```

We see that in this table each line corresponds to a sampling hour and each column corresponds to a sample number. 

We're now going to pass this data to the control chart plotting function qcc(). As this function takes a dataset of observations so we're removing the Hour column with the select function from tidyverse:

```{r}
syringe_clean <- syringe_diameter %>% 
  select(-Hour) %>%
  mutate(across(starts_with("S"), round, 2))
```

Now we load the qcc package that has the required quality control tools:

<b>Calibration run</b>

In order to establish a control chart it is recommended to run a "calibration run". The calibration run is used to calculate the control limits before entering "regular production". Using the first 10 samples we call the qcc() function to make the required calculations.

### xbar chart

[]{#xbarchart}

```{r}
library(qcc)
```

```{r}
syringe_xbar <- qcc(
  syringe_clean[1:10, ], 
  type = "xbar", 
  title = "Lamp weight \n xbar chart", 
  xlab = "Sample group",
  plot = FALSE
  )
```

Before we step ahead and simply plot the SPC chart and interpret the results lets look a bit in detail in the calculations done to established the Control Chart. To do this we're going to go in the details of what we've obtained in the previous chunk. 

A first step is to read the begining of the qcc() help file typing ?qcc in the console. It says "Create an object of class 'qcc' to perform statistical process control' (in R technical terms function is a helper that generates an S3 R object). 

The key point here is that this means we can inspect the calculations separately from the plot itself. We can start by confirming the class and the type of the qcc object:

```{r}
class(syringe_xbar)
typeof(syringe_xbar)
```

It is confirmed it is an object of class qcc with the R type list. Looking into the structure of the list:

```{r}
str(syringe_xbar)
```

The output is not easy to read but we present it here just to show that inside the list there are several tables with the statistical analysis required for our plot. If we want like to see for instance the standard deviation we can extract it separately:

```{r}
syringe_xbar$std.dev
```

And if we want like to see a summary of all the data stored in the object we could apply the summary method:

```{r}
summary(syringe_xbar)
```

We are now ready to finally we can see this all together in a plot:

```{r}
plot(syringe_xbar)
```

### R chart 

Using the same 10 first samples we also obtain the corresponding R chart:

[]{#Rchart}

```{r}
syringe_R <- qcc(
  syringe_clean[1:10, ], 
  type = "R", 
  title = "Lamp weight \n R chart",
  xlab = "Sample group"
  )
```

<b>Regular production</b>

Now that the calibration data has been plotted we can consider that the control limits are defined. They can become fixed and reused in new plots for the future production runs. Samples from those future runs can then be assessed against this limits and the control chart rules can be verified (in this example the shewhart rules are used). We now add the remaining data points to our chart by specifying which lines we're refering too in our dataframe in the 'newdata' argument:

```{r}
syringe_xbar <- qcc(
  data = syringe_clean[1:10, ],
  newdata = syringe_clean[11:25,],
  type = "xbar", 
  title = "Lamp weight \n xbar chart", 
  xlab = "Sample group"
  )
```

We can see that the data point corresponding to the average of the measurements of the samplegroup 17 is plotted in red because it is outside of the control limits.

Now we plot the R chart to assess the variability:

```{r}
syringe_R <- qcc(
  data = syringe_clean[1:10, ],
  newdata = syringe_clean[11:25,], 
  type = "R", 
  title = "Lamp weight \n R chart",
  xlab = "Sample group"
  )
```

In this case all the points are within the previously defined control limits.

<b>Warnings and specification limits</b>

More tight controls can be put in place by clearly identifying warning limits in a narrower range than the control limits. These measures need to be accompaigned by clear decision criteria and proper training to avoid the typical problem of overeacting and destabilizing the process by introducing unintented special causes of variation.

### Limits on xbar chart 

We add warning limits in the plot with as follows:

[]{#limitsxbar}

```{r}
warn.limits <- limits.xbar(
  syringe_xbar$center, 
  syringe_xbar$std.dev, 
  syringe_xbar$sizes, 
  2
  )
plot(
  syringe_xbar, 
  restore.par = FALSE,
  title = "Lamp weight \n xbar chart",
  xlab = "Sample group")
abline(h = warn.limits, lty = 3, col = "chocolate")
```

A manufacturing process under control has a variation that is lower than the product specifications and ideally it is centered. Therefore it is usually good practice to follow the control chart rules refering to the process control limits. 

In some cases nevertheless there may be desired or interesting to add the specification limits. This can be done as follows, first we establish the specifications:

```{r}
spec_max <- 5.6
spec_min <- 5.3
spec_tgt <- (spec_max - spec_min) / 2 + spec_min
specs <- c(spec_min, spec_tgt, spec_max)
```

and replot the control chart with visible specification limits and targets:

```{r}
plot(
  syringe_xbar,
  restore.par = FALSE,
  title = "Lamp weight \n xbar chart",
  xlab = "Sample group",
  ylim = c(specs[1], specs[3])
  )
abline(h = specs, lty = 3, col = "red")
```

In the previous example we see a situation that happens in practice and that requires action: the data plotted is still within the min max specification limits for this relativelly small number of data points. Furthermore the variation is overall well contained within the process limits. Nevertheless we see it is extremelly off centered when compared with the product specification. A process capability study should help determining the causes for this offcentering and help correcting it.

Adapted from @Bass2007

In this chapter we're going to go more in depth in the study of the manufacturing process variability. We're going to make a comparison between the product specifications and the process variability. We're looking for opportunities to tigthen the product specifications. 
Tightening a product specification without increasing the cost of a manufacturing cost can be a source of competitive advantage.

## Process Capability

### Out of specification

```{r}
syringe_long <- syringe_diameter %>%
  pivot_longer(cols = starts_with("Sample"),
               names_to = "sample",
               values_to = "value")
```

**variables**

```{r}
syringe_mean = syringe_long %>% pull(value) %>% mean()
syringe_sd = syringe_long %>% pull(value) %>% sd()
syringe_n <- length(syringe_long)
theor_n = 1000000
```

**calculation: probability of being between the limits**

```{r}
within_limits <- function(UCL, LCL, mean, sd) {
  (pnorm(UCL, mean, sd) - pnorm(LCL, mean, sd))*100
}
```

```{r}
syringe_within <- within_limits(spec_max, spec_min, syringe_mean, syringe_sd)
syringe_off_spec <- 100 - syringe_within
```

```{r}
syringe_theor <- rnorm(n = theor_n, mean = syringe_mean, sd = syringe_sd) %>% 
  as_tibble()
```

```{r}
plot_subtitle <- paste(
  "Spec: [", spec_min, ";", spec_max, 
  "], Proportion off-spec = ",
  signif(syringe_off_spec, digits = 2), "%"
  )
```

Note that we deliberately twick the plot colors to make it look like the plots from minitab and from the qcc package. We provide this theme in the book companion package `industRial` with the name theme_qcc. 

```{r}
syringe_long %>%
  ggplot(aes(x = value, y = ..density..)) +
  geom_histogram(
    bins = 30,
    fill = "white",
    color = "grey20") +
  geom_density(data = syringe_theor, linetype = 2) +
  geom_vline(xintercept = {spec_min}, color = "red", linetype = 3) +
  geom_vline(xintercept = {spec_max}, color = "red", linetype = 3) +
  geom_vline(xintercept = {spec_tgt}, color = "red", linetype = 2) +
  scale_x_continuous(n.breaks = 10) +
  theme_qcc() +
  labs(
    title = "Out of specification (Expected)", 
    subtitle = {plot_subtitle})
```

By looking at the histogram of the Bamako lightning dataset we confirm the extreme offcentering of the production. We also see that although there are no measurements beyond the lower specification limit (LSL) it is very likely this will happen soon.

We can also calculate the Cpk

```{r}
capability <- function(UCL, LCL, mean, sd) {
  pmin(
    (abs(mean - abs(LCL)) / (3 * sd)),
    (abs((abs(UCL) - mean)) / (3 * sd))
   )
}
```

```{r}
capability(spec_max, spec_min, syringe_mean, syringe_sd)
```

And convert the percentage out of spec in parts per million. We're not considering the 1.5 shift that sometimes is presented in the literature but rather making a simple direct conversion of the proportion out of spec found before:

```{r}
per_mio_off_spec <- function(percent_within) {
  formatC(
  ((100 - percent_within) * 10000),
  format = "d",
  big.mark = "'"
  )
}
```

```{r}
per_mio_off_spec(syringe_within)
```

The expected population below the LSL is 1,3% which is very high for industry standards. In fact this corresponds to 12'649 parts per million (ppm) whereas a common target would be 1 ppm. Naturally these figures are indicative and they depend of the context criteria such as severity of the problem, cost, difficulty to eliminate the problem and so on. 

We can now establish a simple table using the functions created before, to present the expected percentage that falls within certain limits. To make it usefull we're putting this limits at +/- 1 to 6 standard deviations

```{r}
mean <- 0
sd <- 1
sigma_conversion_table <- 
  tibble(UCL = c(1,2,3, 4,5,6),
         LCL = -UCL,
         mean = mean,
         sd = sd)
```

```{r}
sigma_conversion_table <- sigma_conversion_table %>%
  mutate(perc_in_spec = within_limits(UCL, LCL, mean, sd),
         Cpk = capability(UCL, LCL, mean, sd),
         ppm_defects = per_mio_off_spec(perc_in_spec))
sigma_conversion_table %>%
  kable(align = "c")
```

### Cpk index

```{r echo = FALSE}
syringe_xbar <- qcc(
  syringe_clean, 
  type = "xbar", 
  title = "Lamp weight \n xbar chart", 
  xlab = "Sample group", plot = FALSE
  )
```

### Process capability chart

[]{#processcapability}

```{r}
syringe_cpk <- process.capability(
  syringe_xbar,
  breaks = 10,
  spec.limits = c(spec_min, spec_max),
  target = spec_tgt,
  digits = 2,
  print = FALSE,
  std.dev = syringe_sd
)
```

A fine tuning of the forecast of the number of expected parts out of specification can be done with the parameter std.dev. The input value will be used in the probability distribution function. Different approaches can be considered: calculating the sandard deviation within each subgroup or the standard deviation of the entire population and also correcting the standard deviation dividing by n or by n - 1. In this example we re-use the standard deviation calculated on the entire set of datapoints as the group is small but for a case with more data it would be interesting to used the subgroups that tend to give smaller standard deviations.

<!--chapter:end:10_spc.Rmd-->

# (PART) APPENDIX {-}

# Exercises {-}

To go further in the exploration of key concepts of this book a set of dynamic exercises and quizzes is available on the web. This allows for exploration of the datasets and R functions without requiring any software installation.

[Pareto Charts](https://thoth.shinyapps.io/pareto/?_ga=2.123594827.1500230957.1620764767-1128688272.1609264547)

<!--chapter:end:11_exercises.Rmd-->

```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	comment = NA
	# out.width = "80%"
)
```

# Glossary {#glossary}

## Statistics

Statistic concepts are picked up and applied throught the Cases Studies on a needed basis. To get a better understanding of how they fit together we are reminding below some definitions coming from @Yakir2011. For a deep and comprehensive course on statistics we recommend the free online [kahn academy](https://www.khanacademy.org/math/statistics-probability) courses.

### Notation conventions

The arithmetic mean of a series of values x1, x2, ..., xn is often denoted by placing an "overbar" over the symbol, e.g. $\bar{x}$ , pronounced "x bar".

Some commonly used symbols for sample statistics are: sample mean $\bar{x}$, sample standard deviation s.

Some commonly used symbols for population parameters: population mean μ, population standard deviation σ.

Random variables are usually written in upper case roman letters: $X, Y$, etc.

Particular realizations of a random variable are written in corresponding lower case letters. For example, x1, x2, …, xn could be a sample corresponding to the random variable $X$. A cumulative probability is formally written P($X$≤x) to differentiate the random variable from its realization.

Greek letters (e.g. θ, β) are commonly used to denote unknown parameters (population parameters).

Placing a hat, or caret, over a true parameter denotes an estimator of it, e.g., $\hat{θ}$ is an estimator for θ.

### Descriptive statistics

**Statistic:** A numerical characteristic of the data. A statistic estimates the
corresponding population parameter.  

**Population:** The collection, or set, of all individuals, objects, or measurements whose properties are being studied.

**Sample:** A portion of the population understudy. A sample is representative
if it characterizes the population being studied.

**Frequency:** The number of times a value occurs in the data.

**Relative Frequency:** The ratio between the frequency and the size of data. $f / n$

**Median:** A number that separates ordered data into halves.

**Mean:** A number that measures the central tendency. A common name for
mean is ‘average.’ 

**Sample size:** $n$

**Sample mean:** $\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i = \sum_{x}(x\times f_x / n)$

**Population size:** $N$

**Population mean:** $\bar{\mu} = \frac{\sum_{i=1}^{N}x_i}{N}$

**Variance:** Mean of the squared deviations from the mean. 

**Sample variance:** $s^{2} = \frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})^2 = \frac{n}{n-1}\sum_{x}((x-\bar{x})^2\times(f_x/n))$

**Standard Deviation**: A number that is equal to the square root of the variance and measures how far data values are from their mean.

**Sample standard deviation:** $\sqrt[]{s^{2}}$

### Probability

**Random Variable:** The probabilistic model for the value of a measurement,
before the measurement is taken (e.g. Binomial, Poisson, Uniform, Exponential, Normal). It is denoted with latin capitals $X, Y$ and $Z$

**Expectation:** The central value for a random variable. The expectation of the
random variable X is marked by E($X$).

**Variance:** The (squared) spread of a random variable. The variance of the
random variable X is marked by Var($X$).

**Normal Random Variable:** A bell-shaped distribution that is frequently used
to model a measurement. The distribution is marked with Normal($\mu,\sigma^2$).

**Standard Normal Distribution:** The Normal(0,1). The distribution of stan-
dardized Normal measurement.

**Percentile:** Given a percent p · 100% (or a probability p), the value x is the
percentile of a random variable X if it satisfies the equation P$(X ≤ x) = p$.

**Sampling distribution**

**Random Sample:** The probabilistic model for the values of a measurements
in the sample, before the measurement is taken.

**Sampling Distribution:** The distribution of a random sample.

**Sampling Distribution of a Statistic:** A statistic is a function of the data;
i.e. a formula applied to the data. The statistic becomes a random variable
when the formula is applied to a random sample. The distribution of this
random variable, which is inherited from the distribution of the sample,
is its sampling distribution.

**Sampling Distribution of the Sample Average:** The distribution of the sample average, considered as a random variable.

**The Law of Large Numbers:** A mathematical result regarding the sampling
distribution of the sample average. States that the distribution of the av-
erage of measurements is highly concentrated in the vicinity of the expec-
tation of a measurement when the sample size is large.

**The Central Limit Theorem:** A mathematical result regarding the sampling
distribution of the sample average. States that the distribution of the average is approximately Normal when the sample size is large.

<div class="marginnote">
(note: the central limit theorem is a key notion for understanding industrial measurement and its consequences will be applied in most case studies)
</div>

**Expectation of the sample average:** the expectation of the sample mean is
equal to the theoretical expectation of its components E$(\bar{X})$ = E($X$)

**Variance of the sample average:** the variance of the sample average
is equal to the variance of each of the components, divided by the sample size Var($X$) = Var$(X)/n$

### Statistical Inference

**Statistical Inference:** Methods for gaining insight regarding the population
parameters from the observed data.

**Point Estimation:** An attempt to obtain the best guess of the value of a
population parameter. An estimator is a statistic that produces such a
guess. The estimate is the observed value of the estimator.

**Confidence Interval:** An interval that is most likely to contain the population
parameter. The confidence level of the interval is the sampling probability
that the confidence interval contains the parameter value.

**Hypothesis Testing:** A method for determining between two hypothesis, with
one of the two being the currently accepted hypothesis. A determination is
based on the value of the test statistic. The probability of falsely rejecting
the currently accepted hypothesis is the significance level of the test.

**Comparing Samples:** Samples emerge from different populations or under
different experimental conditions. Statistical inference may be used to
compare the distributions of the samples to each other.

**Regression:** Relates different variables that are measured on the same sample.
Regression models are used to describe the effect of one of the variables
on the distribution of the other one. The former is called the explanatory
variable and the later is called the response.

**Bias:** The difference between the expectation of the estimator and the value
of the parameter. An estimator is unbiased if the bias is equal to zero.
Otherwise, it is biased.

**Mean Square Error (MSE):** A measure of the concentration of the distribu-
tion of the estimator about the value of the parameter. The mean square
error of an estimator is equal to the sum of the variance and the square of
the bias. If the estimator is unbiased then the mean square error is equal
to the variance.

**Confidence Level:** The sampling probability that random confidence intervals
contain the parameter value. The confidence level of an observed interval
indicates that it was constructed using a formula that produces, when
applied to random samples, such random intervals.

**Null Hypothesis **($H0$): A sub-collection that emerges in response to the sit-
uation when the phenomena is absent. The established scientific theory
that is being challenged. The hypothesis which is worse to erroneously
reject.

**Alternative Hypothesis **($H1$): A sub-collection that emerges in response to
the presence of the investigated phenomena. The new scientific theory
that challenges the currently established theory.

**Test Statistic:** A statistic that summarizes the data in the sample in order to
decide between the two alternative.

**Rejection Region:** A set of values that the test statistic may obtain. If the
observed value of the test statistic belongs to the rejection region then the
null hypothesis is rejected. Otherwise, the null hypothesis is not rejected.

**Type I Error:** The null hypothesis is correct but it is rejected by the test.

**Type II Error:** The alternative hypothesis holds but the null hypothesis is not
rejected by the test.

**Significance Level:** The probability of a Type I error. The probability, com-
puted under the null hypothesis, of rejecting the null hypothesis. The
test is constructed to have a given significance level. A commonly used
significance level is 5%.

**Statistical Power:** The probability, computed under the alternative hypoth-
esis, of rejecting the null hypothesis. The statistical power is equal to 1
minus the probability of a Type II error.

$p$**-value:** A form of a test statistic. It is associated with a specific test statistic
and a structure of the rejection region. The p-value is equal to the signif-
icance level of the test in which the observed value of the statistic serves
as the threshold.

## Design for Six Sigma

Quality tools have been grouped under varied names and methodologies being Six Sigma one of the most comprehensives ones. One way of summarising the Six Sigma framework is presented below in a step by step approach with definitions. Each steps consists of an analyis of the product development and production process that progressively refines the final product specifications.

For a more detailed description we recommend reviewing the comprehensive Six Sigma certification reference book by @Munro2015.

### Voice of Customer

**1. Product brief**
List of the product features expected by the customer (internal or external), including qualitative indication of the acceptance limits.

**2. Functional analysis**
Translation of the product attributes into lower level functions including interactions between product components and requirements induced by each component on the others.

**3. Failure modes and effects analysis (FMEA)**
List of critical product features with failure causes, effects, detection and action plans, rated and sorted by criticality.

**4. Product specifications and parts drawings**
Implementation of the product components into unique formulations and drawings including detailed values and tolerances of it characteristics (physical, chemical or electric or others).

### Voice of Process

**1. Process mapping**  
A visual diagram of the production process with inputs and outputs for each step.

**2. Process FMEA**
List of critical production process steps with failure causes, effects, detection and action plans, rated and sorted by criticality.

**3. Quality Control plan**
List of control points including measurement method, sample size, frequency and acceptance criteria. When needed, critical control points are handled by Statistical Process Control (SPC)

**4. Measurement system analysis**
A thorough assessment of a measurement process, and typically includes a specially designed experiment that seeks to identify the components of variation in that measurement process. 

**5. Process capability analysis**
Comparison of the variability of a production process with its engineered specifications.

## Gage r&R

The definitions below are based on the articles on @minitab_gagernR which itself is based on the @AIAG2010.

Variance components assess the amount of variation contributed by each source of measurement error, plus the contribution of part-to-part variability.

**total gage r&R:** the sum of the repeatability and the reproducibility variance components.

**part:** The variation that comes from the parts, with 5 levels in this case.

**operator:** The variation that comes from the operators, with 3 levels in this case.

**replicants, n:** number of replications corresponding to the number of times each part is measured by each operator.

**repeatability (or error, or residuals):** The variation that is not explained by part, operator, or the operator and part interaction. It represents how much variability is caused by the measurement device (the same operator measures the same part many times, using the same gage). The repeteability can be measured directly from the Anova table from the residual mean squares.

**reproducibility:** how much variation is caused by the differences between operators (different operators measure the same part many times, using the same gage).

**operators:** the operators part of the reproducibility is the operators variation minus the interaction divided by the number of different parts times the replicants (zero if negative).

**parts:operators:** The variation that comes from the operator and part interaction. An interaction exists when an operator measures different parts differently. The interaction part of of the reproducibility is the interaction minus the repeatability divided by the number of replicants (zero if negative).

**part-to-part:** the variability due to different parts. Ideally, very little should be due to repeatability and reproducibility. Differences between parts should account for most of the variability (when the %Contribution from part-to-part variation is high, the measurement system can reliably distinguish between parts).

The sum of the individual variance components equals the total variation.

## Design of experiments

Below key definitions from @Montgomery2012, complemented with Wikipedia article details on the same topics.

**Randomization:** both the allocation of the experimental material and the order in which the individual runs of the experiment are to be performed are randomly determined.

**Replicate:** independent repeat run of each factor combination.

**Factorial design:** in each complete trial or replicate of the experiment all possible combinations of the levels of the factors are investigated.

**Crossed factors:** factors arranged in a factorial design.

**Coded variable:** the $\pm$1 coding for the low and high levels of the factors. Coded variables are very effective for determining the relative size of factor effects. In almost all situations, the coded unit analysis is preferable.

**Contrast:** a linear combination of parameters in the form $\tau=\sum_{i=1}^{a}c_i\mu_i$ where the contrast constants $c_1,c_2, ..., c_a$ sum to zero; that is, $\sum_{i=1}^{a}c_i=0$. 

**Orthogonal contrasts:** two contrasts with coefficients ${c_i}$ and ${d_i}$ are orthogonal if: $\sum_{i=1}^{a}c_id_i$. In a balanced one-way analysis of variance, using orthogonal contrasts has the advantage of completely partitioning the treatment sum of squares into non-overlapping additive components that represent the variation due to each contrast. Contrasts then allow for the comparison between the different means.

**sparsity of effects principle:** states that most systems are dominated by some of the main effects and low-order interactions, and most high-order interactions are negligible.

<!--chapter:end:12_glossary.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}

A good mastership of the vast domain of Industrial Data Science can take several years and can only be obtained by a strong combination of theory and practice. As mentionned in the introduction chapter, our book is focused on the practice and in this bibliography we find some the necessary supporting theory. 

The list below collects websites, books and articles referenced throughout this book. It is a curated set of some of the most relevant works available today in Six Sigma, Statistics, Data Science and programming with R.

'`


<!--chapter:end:13_references.Rmd-->


# Table of Contents {-}

```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	comment = NA,
	options(knitr.kable.NA = '')
	# out.width = "80%"
)
```

```{r include=FALSE}
library(tidyverse)
library(readxl)
library(knitr)
library(kableExtra)
filter <- dplyr::filter
select <- dplyr::select
```

```{r echo=FALSE}
read_excel("data/contents.xlsx") %>%
  select(Chapter, Unit, Dataset, Functions) %>%
  kable(align = "l") %>%
  collapse_rows(columns = c(1,2,3), valign = "middle") %>%
  row_spec(row = 0, background = "#e6e6e6", color = "#104e8b") %>%
  row_spec(row = 1:32 ,background = "white")
  # column_spec(column = 2.5, width_min = "15em") %>%
  # column_spec(column = 3, width_min = "15em") %>%
  # kable_styling(font_size = 14)
```


<!--chapter:end:2_toc.Rmd-->


```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	comment = NA
	# out.width = "80%"
)
```

# (PART) CASE STUDIES {-}

# Design for Six Sigma

## Pareto

The pareto chart has always proven an effective way of defining priorities and keeping workload under control. It is known for helping focusing on the few important elements that account for most problems. It builds on the well known insight that a few reasons explain or allow to control most of the outcome. This applies particularly well in the technological and industrial context.

**The dial polishing workshop**

The example here comes from a dial polishing workshop in the watchmaking industry. Dials are received from the stamping process and polished before being sent to the final assembly. As part of the authonomous quality control performed by the polishing operators a count of the defects observed on the dials each day is kept in a file. 

<div class="marginnote">

```{r echo=FALSE, fig.align='center', fig.cap="watch dial inspection", out.width="100%"}
knitr::include_graphics("img/assemblage_cadran_bw.jpg")
```

</div>

**Loading packages and data:**

If needed get the book companion package from github:

```
devtools::install_github("J-Ramalho/industRial")
```

load it in the current session:

```{r}
library(industRial)
```

getting the dial dataset:

```{r}
dial_control <-  industRial::dial_control
```

and additional packages to deal with the data:

```{r}
library(tidyverse)
library(knitr)
library(viridis)
filter <- dplyr::filter
select <- dplyr::select
```

A first look at the dataset now:

```{r}
head(dial_control) %>% 
  kable(align = "c", 
        caption = "dial control data", 
        booktabs = T)
```

We can see that the count includes both the deffect type and the location (the hour in the dial) and that it is traced to the day and operator.

The team leader promotes a culture of fact based assessment of the quality measurements. Every week the team looks back and observes the weekly counts. This is important because it helps moving away from perception into a more solid assessment. The volume of data is higher like this enabling trends to start becoming apparent. The team can discuss potential actions and prepare reporting to the supplier of the parts (the stamping workshop). It also helps calibrating between operators and agreeing on acceptance criteria and what is and what is not a defect.

A first example of the pareto of the types of defects:

[]{#paretochart}

```{r}
library(qicharts2)
```

```{r}
d_type <- dial_control %>% pull(Defect) %>% as.character()
d_type_p <- paretochart(d_type, 
                           title = "Watch Dial polishing",
                           subtitle = "Pareto chart", 
                           ylab = "Percentage of deffects",
                           xlab = "Deffect type",
                           caption = "Source: Dial Production Team")
d_type_p + 
  theme_industRial()
```

As often happens we can see that the first two deffects account for more than 80% of the problems. Identation and scratching are the things to tackle here.

From the available data presented before in table we can go deeper and establish a pareto of the defect location:

```{r}
d_location <- dial_control %>% pull(Location) %>% as.character()
d_location_p <- paretochart(d_location, 
                           title = "Watch Dial polishing",
                           subtitle = "Pareto chart", 
                           ylab = "Percentage of deffects",
                           xlab = "Deffect location (hour)",
                           caption = "Source: Dial Production Team")
d_location_p +
  theme_industRial()
```

Here a third bucket could be included in the priorities: to reach 80% of the count we consider the defects that appear at 4 o'clock, 3 o'clock and 5 o'clock.

During the reviews the team can also identify other types of data for follow up such as the dial model or the material type. With a simple excel file and an upload in R this can be finetuned from week to week according to the progress of the improvement measures.

## Ishikawa

Usually called Fishbone or Ishikawa diagrams this simple tool has proven to be extremely practical and helpful in structuring team discussions. 

With it we can easily identify and list the expected influencing factors for example to design an experiment. Such selection and grouping of parameters can be useful for among others in defining the right mix of ingredients in a new product or material, in selecting the machine parameters in a manufacturing line or in the definition of a draft operating procedure for a measurement. In each of these situations it helps seeing the big picture and not fall into the trap of relying only in the data and findings obtained by statistical analysis.

Below we're showing an example building on the qcc package. The package author describes in his book @Cano2012 the advantages of using R for the design of these diagrams. In our view it strongly complements the data analysis making the full case easilly reproducible and easily updatable.

**The dental prosthesis laboratory**

An optical measurement device has just been installed in a large Dental Prosthesis Manufacturing Laboratory. This is a very expensive device based on laser technology installed in a dedicated stabilized workbench. Despite all the precautions it has been reported and now demonstrated with some specific trials that the measurements have a higher variation which makes it unsuitable to be used for what is was used for: the precise measurement of dental impressions that serve as models for the production of the crowns and bridges.

<div class="marginnote">

```{r echo=FALSE, fig.align='center', fig.cap="dental impression measurement", out.width="100%"}
knitr::include_graphics("img/dental_scan2.png")
# sources:
# https://wiki.maestro3d.com/wiki/index.php?title=Maestro_3D_Easy_Dental_Scan_-_User_Manual
```

</div>

So far the Lab Team has full confidence in the equipement supplier and the Lab Manager has seen a similar equipment from the same supplier operating in another laboratory he has visited.

The supplier checked the equipment and having seen no reason for the variability proposes to work with the lab team on identifying the potential causes for the high uncertainty in their measurements. They decided to consider a larger scope that the equipment and take the full measurement method as described in the laboratory operating procedure. They list different reasons related with they're work and group them:

```{r}
operators <- c("Supplier", "Lab Technician", "Lab Manager")
materials <- c("Silicon", "Alginate", "Polyethers")
machines <- c("Brightness", "Fixture", "Dimensional algorithm")
methods <- c("Fixture", "Holding time", "Resolution")
measurements <- c("Recording method", "Rounding", "Resolution")
groups <- c("Operator", "Material", "Machine", "Method", "Measurement")
effect <- "Too high uncertainty"
```

And then load the qcc package and quickly obtain a simple diagram that allows for a quick visualisation of these influencing factors.

[]{#fishbone}

```{r}
library(qcc)
```

```{r, fig-ishikawa, fig.align='center', fig.cap = 'ishikawa diagram aplication', out.width = "100%"}
cause.and.effect(
  title = "Potential causes for uncertainty increase during a measurement",
  cause = list(
    Operator = operators,
    Material = materials,
    Machine = machines,
    Method = methods,
    Measurement = measurements
  ),
  effect = effect
)
```

The listed factors can then be adressed one by one or in combined experiments to evaluate their impact on the measurement method.

## Correlation matrix

A matrix diagram is a way to discover relationships between groups of items as described by  described in the Six Sigma book by @Munro2015. 

These matrix can be used to select which measurement to do in a design of experiments. In exploratory phases when the experiments are repeated several time with slightly different configurations, secondary outputs that are strongly correlated to main outputs can be eliminated 

In an industrial setup the cost of experimenting is often very high. With this approach engineers can keep the quantities test quantities in control by avoiding measurements until final stages of implementation. 

We provide here two different techniques, one with a tile plot and another with a network plot.

**The Perfume destilation experiment**

<div class="marginnote">

```{r echo=FALSE, fig.align='center', fig.cap="Perfume destillation line", out.width="100%"}
knitr::include_graphics("img/parfum.jpg")
```

</div>

DOEs consist in a series of trials where several inputs are combined in specific levels and important outputs are measured (further details can be seen in the DOE chapter). The case below refers to a DOE on Perfume Formulation Development. The Product Development team would like to understand the impact of the perfum manufacturing line parameters variation (e.g. temperature, pressure and others) in typical perfume sensorial characteristics such as the floral notes.

The typical DOE analysis results linking inputs to outputs are presented with effects plots and interaction plots. Here though it is another analysis that interests us: the correlation between the outputs, where there is not always necessary a cause effect but where it is interesting to see if groups of outputs move together. This type of analysis is most commonly presented in a tile plot.

In our present case note that the first DOE has not yet been executed, only a preparatory session has taken place with the project manager to review the potential outputs and anticipate what will be the DOE results. This allowed to go deeper in the technology understanding and to confirm that the plan was constructed in a meaning full way. The experts input has been captured in a 1/2 of a two entry table named "Perfume".

As the industRial package was already loaded in the Pareto section, we can then access directly to the Perfume dataset of which we're showing a subset:

```{r}
perfume_experiment[1:8, 1:8] %>%
  kable(
    caption = "perfume DoE output variables",
    booktabs = T
  ) 
```

Variables are named with coded names made of two letter. They represent the production Line Parameters and the Perfume Attributes (e.g. t = temperature, o = opening, pw = power). We can see in the table what the team has noted as expected correlation strenght, with 10 being the highest.

Now we filter correlations higher or equal to 7 which have been considered as the potential targets for the simplification in future designs:

```{r}
perfume_long <- perfume_experiment %>%
  pivot_longer(
    cols = -yy,
    values_to = "correlation",
    names_to = "xx"
  ) %>%
  filter(correlation >= 7) %>%
  mutate(correlation = as_factor(correlation))
```

[]{#tileplot}

```{r}
perfume_long %>%
  ggplot(aes(x = xx, y = yy, fill = correlation)) +
  scale_fill_viridis_d(direction = -1, name = "Correlation\nStrength") +
  geom_tile() + 
  labs(
    title = "The Perfume destilation experiment",
    subtitle = "Input variables correlation plot ",
    x = "",
    y = "",
    caption = "Data variables anonymized"
  ) +
  theme_industRial()
```

The plot shows that many parameters are expected to move together, for example with maximum correlation we have hp moving with d, oc moving with o and so on.
 
After this first DoE the real correlations will be established and the team expects to be able to avoid a significant part of the measurements that have a correlation higher than 50% from the second DoE onwards.

## Clustering

In this variant we explore a more advanced but more powerfull approach using network plots. It provides an automatic clustering of the factors and a specific way to read such clusters.

We're going to build a weighed non directional network(tbl_graph) object. Several steps of conversion are required for this approach first with igraph and then to tidygraph.

We start by loading the required packages: 

```{r}
library(igraph)
library(tidygraph)
library(ggraph)
library(ggforce)
library(ggtext)
```

The first step consists in converting the "Perfume" tibble to a matrix format:

```{r}
perfume_matrix <- perfume_experiment %>%
  column_to_rownames("yy") %>%
  as.matrix()
```

Then using the {igraph} package we convert the matrix into a graph object:

```{r}
perfume_graph <- graph_from_adjacency_matrix(
  perfume_matrix, mode = "undirected", weighted = TRUE
  )
```

to finaly convert it into a tibble graph with tidygraph package:

```{r}
perfum_tbl_graph <- as_tbl_graph(perfume_graph, add.rownames = "nodes_names")
```

The users have provided the correlation strength in a simple scale from 1 to 10 which was easier for the discussion. We're here converting it back to the 0 to 1 which is more common in the statistics community. For simplicity, negative correlations were not considered just the strength, enabling the network to be unidirectional.

```{r}
perfum_tbl_graph <- perfum_tbl_graph %>%
  activate(edges) %>%
  mutate(weight = weight/10)
perfum_tbl_graph
```

In the previous chunk output we see a preview of the tibble graph object with the first few nodes and edges.

Now we create a vector with various igraph layouts to allow for easier selection when making the plots:

```{r}
igraph_layouts <- c('star', 'circle', 'gem', 'dh', 'graphopt', 'grid', 'mds', 
                    'randomly', 'fr', 'kk', 'drl', 'lgl')
```

and do a first network plot to check data upload:

```{r, fig.width = 7, fig.height = 6, warning = FALSE}
perfum_tbl_graph %>% 
  ggraph(layout = "igraph", algorithm = igraph_layouts[7]) +
  geom_edge_link(aes(edge_alpha = weight)) +
  geom_node_label(aes(label = name), repel = TRUE) +
  theme_graph() +
  labs(title = "DOE Perfume Formulation - Inputs",
       subtitle = "Most important expected correlations")
```

Data loading is confirmed to have been done correctly, we can now move into the clustering and analysis.

We use different clusters algorithms like in part 3 to generate the groups:

```{r}
perfum_tbl_graph <- perfum_tbl_graph %>%
  activate(nodes) %>%
  mutate(group_components = group_components(),
          group_edge_betweenness = group_edge_betweenness(),
          group_fast_greedy = group_fast_greedy(),
          group_infomap = group_infomap(),
          group_label_prop = group_label_prop(),
          group_leading_eigen = group_leading_eigen(),
          group_louvain = group_louvain(),
          group_walktrap = group_walktrap()
         )
```

and produce a final plot, selecting group optimal that with some testing has proven to be the algorithm that gives the best clustering results. The correlations strengths are here represented by the edges width for optimal visualization.

```{r, fig.width = 9, fig.height = 7, warning = FALSE}
perfum_tg_2 <- perfum_tbl_graph %>% 
  activate(edges) %>% 
  mutate(weight2 = if_else(weight >= 0.8, 1, if_else(weight >= 0.5, 0.5, 0.1)))

my_palette <- c(viridis(12)[3], viridis(12)[9], 
                "gray40", "gray40", "gray40", "gray40", 
                "gray40", "gray40", "gray40", "gray40")

set.seed(48)

perfum_tg_2 %>% 
  activate(nodes) %>% 
  mutate(group = group_louvain) %>% 
  filter(group %in% c(1,2)) %>% 
  ggraph(layout = "igraph", algorithm = igraph_layouts[7]) +
   geom_mark_hull(mapping = aes(x, y, 
                               group = as_factor(group), 
                               fill = as_factor(group)),
               concavity = 0.5,
               expand = unit(4, 'mm'),
               alpha = 0.25,
               colour = 'white',
               show.legend = FALSE) + 
  geom_edge_link(aes(edge_alpha = weight2, edge_width = weight2)) + 
  geom_node_point(size = 3) +
  geom_node_label(aes(label = name), repel = TRUE) +
  scale_edge_width(range = c(0.2, 1), name = "Correlation strength") +
  scale_edge_alpha(range = c(0.05, 0.2), name = "Correlation strength") + 
  scale_fill_manual(values = my_palette) + 
  theme_graph() +
  labs(
    title = str_c("<span style='color:#433E85FF'>Line Parameters</span>", 
    " and ", "<span style='color:#51C56AFF'>Perfume Attributes</span>"),
    subtitle = "Clustering the outputs of Perfume Formulation DOE01",
    caption = "Clustering by multi-level modularity optimisation (louvain)") +
  theme(plot.title = element_markdown(family = "Helvetica",
                                      size = 14, 
                                      face = "bold")) 
```

<br>

We can see that the algorithm is grouping elements that have a strong correlation. Most stronger correlations are present within elements of each cluster with some exceptions such as oc with pw and y with pe. 

The code presented can now easily be reused once the DOE is executed to compare with the real correlations measured.








<!--chapter:end:3_sixsigma.Rmd-->

```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	comment = NA
	# out.width = "80%"
)
```

# Measurement System Analysis {#MSA}

Validation of a measurement device

## Trueness

**The juice production plant**

The Quality Control Manager of a Juice producing plant acquired a faster dry matter content measurement device from the supplier DRX. An important reduction of the control time was the rational for the acquisition and now before finally putting it into operation its performance is being assessed and validated.

<div class="marginnote">

```{r echo=FALSE, fig.align='center', fig.cap="juice bottling line", out.width="100%"}
knitr::include_graphics("img/juice_bottling_bw.jpg")
```

</div>

In this case study we will look into the assessment of the linearity which is the difference in average bias throughout the measurement range. 

Dry matter content for the company top seller juice_bottling have around  12% dry matter as for example Premium Fresh Apple juice: 12.4 % and Austrian Beetroot: 13.2% and some other specialities may have a higher content up such as Organic Carrot with 16.3%.

It has been decided to start by checking the equipement in the range of 10 to 20% dry matter content.


```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(janitor)
library(scales)
library(stats)
library(knitr)
library(industRial)
filter <- dplyr::filter
select <- dplyr::select
```

Lets look into the measurements obtained on the juice bottling process:

```{r}
summary(juice_drymatter)
```

we see raw data for the DRX and Ref equipment allowing us to calculate the difference between the two devices for each measurement:

```{r}
juice_drymatter <- juice_drymatter %>%
  mutate(bias = drymatter_DRX - drymatter_REF, part = as_factor(part))
summary(juice_drymatter$bias)
```

We immediatly see a median bias of -0.2 
Lets explore further.

### Bias plot

[]{#geom_smooth}

```{r}
juice_drymatter %>%
  ggplot(aes(x = drymatter_REF, y = bias)) +
  geom_point() +
  geom_smooth(method = "lm", se = T, ) +
  coord_cartesian(
    xlim = c(9,21),
    ylim = c(-.75,0), expand = TRUE) +
  theme_industRial() +
  labs(title = "Dry matter method validation",
       subtitle = "Gage Linearity",
       caption = "Dataset: juice_drymatter233A, Operator: S.Jonathan)")
```

The linear model is well adapted in this case, this by seing the position of the slope close to the averages of each level of the factor. Nevertheless the slope is rather steep showing a clear increase of the bias (in the negative direction) with the increase in dry matter content.

### Bias report

```{r}
juice_drymatter %>%
  group_by(drymatter_TGT) %>%
  summarise(bias_mean = mean(bias, na.rm = TRUE), 
            bias_median = median(bias, na.rm = TRUE)) %>%
  select(drymatter_TGT, bias_mean, bias_median) %>%
  kable(align = "c", digits = 2)
```

Mean and median bias are very close. A decision now needs to be taken on which systematic offset to apply depending on the operational context. If most products on the line where the device is used a simplified operational procedure with a unique offset of 0.2 can be sufficient.

## Precision

**The tablet compaction process**

Modern pharmaceutical tablet presses reach output volumes of up to 1,700,000 tablets per hour. These huge volumes require frequent in-process quality control for the tablet weight, thickness and hardness.

In our case study we're going to measurement the precision of the measurement method used to determine the thickness of the tablet.

<div class="marginnote">

```{r echo=FALSE, fig.align='center', fig.cap="Tablet thickness micrometer", out.width="100%"}
knitr::include_graphics("img/tablet_micrometer.png")
# sources: 
# https://en.wikipedia.org/wiki/Tablet_(pharmacy)#Manufacture_of_the_tablets
# https://www.ischi.com/ipc-line/uts-ip-lr-easy-to-clean/
```

</div>

According to the ISO 5725 the Precision of a measurement method is the combination of the Reproductibility & Reproducibility.

**Data loading**

We use here the data from the tablet thickness method validation:

```{r}
str(tablet_thickness)
```

We see that quite an extensive variaty of parameters was collected here including room conditions. We are mostly interested in the tablet size, thickness and operator who has performed the measurement. As these parameters are coded as characters we are going to convert them to factors:

```{r}
tablet_thickness <- tablet_thickness %>%
  clean_names() %>%
  mutate(across(c(size, tablet, operator), as_factor))
```

We want to establish an independed r&R by specification size (S, M or L) and so we first filter only for the first size the L.

```{r}
tablet_L <- tablet_thickness %>%
  filter(size == "L")
```

### Base Anova

We're feeding the aov function from the stats package with operator and tablet factors.

```{r}
tablet_L_lm <- lm(
  thickness_micron ~ (
    # main effects
    tablet +
    operator +
    # 2nd order interactions
    operator:tablet
    ),
    data = tablet_L
)
tablet_L_aov <- aov(tablet_L_lm)
summary(tablet_L_aov)
```

Below we're recreating the same analysis with the ss.rr function from the Six Sigma package. As the function allows to input the limits we're also providing in the function arguments the current upper and lower limit of the specification.

tablet L 18'000mm3 +/- 250mm3 (18.0ml +/- 0.25ml)

### gage r&R 

[]{#gageRnR}

```{r}
library(SixSigma)
```

```{r, fig.dim = c(8, 10)}
tablet_L_rr <- ss.rr(
  data = tablet_L, 
  var = thickness_micron, 
  part = tablet, 
  appr = operator, 
  alphaLim = 1,
  errorTerm = "repeatability", # very important otherwise F test not identical to base aov
  main = "Micrometer FTR600\nr&R for tablet thickness",
  sub = "Tablet L",
  lsl = 1775,
  usl = 1825
)
```

We can observe that the SixSigma package recreates exactly the same anova table, just calling Repeatability to the Residuals and adding an additional line with the total degrees of freedom and the total sum of squares. 

Note that the argument alphaLim has been set to 1 to avoid suppressing the interaction which is in this case non significative.

### Acceptance on Variance

**Criteria for measurement system acceptance:**

To evaluate your process variation, compare the Total Gage R&R contribution in the %Contrib column with the values in the list below:

* Less than 1%: the measurement system is acceptable

* Between 1% and 9%: the measurement system is acceptable depending on the application, the cost of the measurement device, cost of repair, or other factors

* Greater than 9%: the measurement system is not acceptable and should be improved.

### Acceptance on SD

The study variation table is established by calculating the square root of each variance (the standard deviation) and by multiplying it by 6 (the six sigma) and then again by comparing each variation with the total variation. Standard deviations are usualy more speaking to the industry professionals. This table also provides a comparison with the specification.

**Criteria for measurement system acceptance:**

According to the guidelines from the @AIAG2010, if your system variation is less than 10% of the process variation, then it is acceptable.

To evaluate your process variation, compare the Total Gage R&R contribution in the %StudyVar column with the values in the list below:

* Less than 10%: the measurement system is acceptable

* Between 10% and 30%: the measurement system is acceptable depending on the application, the cost of the measurement device, cost of repair, or other factors

* Greater that 30%: the measurement system is not acceptable and should be improved.

If the p-value for the operator and part interaction is 0.05 or higher, the system removes the interaction because it is not significant and generates a second ANOVA table without the interaction.

The AIAG also states that the number of distinct categories into which the measurement system divides process output should be greater or equal to 5.

The part to part variation is high which is what is expected in a study like this.

In our specific example we observe that the device cannot be accepted as the Study variation for the Total Gage r&R is 38.46% thus much higher than 30%. Furhtermore the number of distinc categories is only of 3. 

Finaly to be noted that the total variation rather low when compared with the specification.

In a nutshell two questions are answered:

* can the method be used to assess process performance: no the measurement system variation equals 38.4% of the process variation,

* can the method be used to sort good parts from bad: yes but can be improved, the measurement system variation equals 15.3% of the tolerance.

### Interaction plots

The Six Sigma package plots are similar to the interaction plots provided by other DoE packages but don't have error bars. These can nevertheless easily be established on a needed basis as in the example below where we're recreating the tablet:operator interaction plot with a +/- 1 standard deviation error bars.

```{r}
tablet_L %>%
  group_by(tablet, operator) %>%
  summarise(vol_mean = mean(thickness_micron), vol_sd = sd(thickness_micron)) %>%
  ggplot(aes(x = tablet, y = vol_mean, color = operator)) +
  geom_point(aes(group = operator), size = 2) +
  geom_line(aes(group = operator, linetype = operator)) +
  geom_errorbar(aes(ymin = vol_mean - vol_sd, 
                    ymax = vol_mean + vol_sd),
                width = .1) +
  scale_y_continuous(labels = label_number(big.mark = "'")) +
  scale_color_viridis_d(option = "C", begin = 0.1, end = 0.9) +
  # coord_cartesian(ylim = c(17950, 18150)) +
  annotate(geom = "text", x = Inf, y = -Inf, label = "Error bars are +/- 1xSD", 
    hjust = 1, vjust = -1, colour = "grey30", size = 3, 
    fontface = "italic") +
  theme_industRial() +
  labs(title = "Tablet thickness method validation",
       subtitle = "Interaction plot - L tablet x Operator",
       x = "",
       y = "thickness [mm]",
       caption = "Data source: QA Lab")
```


### Negative Variations

Two important limitations exist in the current approach:

1) when the operators reproducibility is negative it is converted to zero.

[@Montgomery2012] in page 557 adresses this case in the following way:

*Notice that the estimate of one of the variance components,is negative. This is certainly not reasonable because by definition variances are nonnegative. Unfortunately, negative estimates of variance components can result when we use the analysis of variance method of estimation (this is considered one of its drawbacks). We can deal with this negative result in a variety of ways:*

*1) one possibility is to assume that the negative estimate means that the variance component is really zero and just set it to zero, leaving the other nonnegative estimates unchanged.*

*2) Another approach is to estimate the variance components with a method that assures nonnegative estimates (this can be done with the maximum likelihood approach).*

*3) Finally, we could note that the P-value for the interaction term ... is very large, take this as evidence that really is zero and that there is no interaction effect, and then fit a reduced model of the form that does not include the interaction term. This is a relatively easy approach and one that often works nearly as well as more sophisticated methods.*

This final approach is also what the SixSigma package creators have foreseen and if we leave the argument alphaLim empty the non significant terms will be suppressed from the model, the Anova recalculated and the remaining tables updated accordingly. This can be finetuned with the argument alphaLim. Usually we consider a p value of 0.05 but we recommend to start with higher values such as 0.1 or 0.2 to avoid suppressing too quickly the factor which would result in a transfer of their variability into the repeatability.

```{r, fig.dim = c(8, 10)}
tablet_L_rr2 <- ss.rr(
  data = tablet_L, 
  var = thickness_micron, 
  part = tablet, 
  appr = operator, 
  alphaLim = 0.2, # instead of 0.05 it is recommended to start higher
  errorTerm = "repeatability",
  main = "Micrometer FTR600\nr&R for tablet thickness",
  sub = "Tablet L",
  lsl = 1775,
  usl = 1825
)
```

In our case when comparing the total gage r&R with and without the interaction we see it changing from 38.46% to 38.38%.

### Beyond two factors

The ss.rr function only accepts 2 factors so it is not possible to obtain all the tables and plots with for example the day as a factor.

This significantly limits the calculation of the total uncertainty for some measurement methods. Next steps in our study will be to prepare an R function to deal with more than 2 factors.

### Conclusions

The Six Sigma package r&R approach can be applied with no issue to simple cases with 2 factors (e.g. operators and parts) where the Variance Component of the Reproducibility is not negative.

## Uncertainty

A final step in the validation of our measurement device is now the calculation of the total measurement uncertainty. 

In some reports the terminology uncertainty is prefered instead of gage r&R.
In this case the formula usually used to evaluate the measurement uncertainty is:

$$
u^2=u_{repeat.}^2+ u_{reprod.}^2+ u_{cal.}^2
$$

where the repeatability and reproducibility members can be obtained from the variances calculated in the r&R study

$$
u_{repeat}^2 = σ_{repeat}^2\\
u_{reprod}^2 = σ_{reprod}^2
$$
These variance components can be directly obtained from the object generated by the function ss.rr of the {SixSigma} package. If we name our variable $σ_{repeat}^2$ as repeat_var in R we have:

```{r}
repeat_var <- tablet_L_rr$varComp[2,1]
repeat_var
```

and for the reproducibility we have:

```{r}
reprod_var <- tablet_L_rr$varComp[3,1]
reprod_var
```

The equipment manual mentions an accuracy of 0.001 mm. If we take this as the calibration uncertainty expressed as a standard deviation, this means we have:

$$
u_{cal} = 0.001 mm \Leftrightarrow 1 \mu m\\
u_{cal}^2 = 1^2 = 1
$$
that we can assign in R to the variable calibration_var. 
```{r}
calibration_var <- 1
```


We thus we have a uncertainty of:

```{r}
u <- sqrt(reprod_var + repeat_var + calibration_var)
u
```

Finally what is usually reported is the expanded uncertainty corresponding to 2 standard deviations. To be recalled that +/- 2 std corresponds to 95% of the values when a repeative measurement is done. In this case we have $U = 2*u$:

```{r}
U <- 2 * u
U
```

For a specific measurement of say 1'800 $\mu m$ we then say: the tablet thickness is 1'800 $\mu m$ $\pm$ `r round(U,1)` $\mu m$, at the 95 percent confidence level. Or written in short:

1'800 $\mu m$ $\pm$ `r round(U,1)` $\mu m$, at a level of confidence of 95%

Knowing that the specification is [1'775; 1'825] $\mu$m we have a specification range of 500. The expanded uncertainty corresponds to `r round(2*U/50*100,2)` %. This is another way of looking into the ratio between method variation and specification. This {SixSigma} package gave a similar result of 15.37%. To be noted that the calculation in by the package corresponds to 3 standard deviations and does not comprise the supplier calibration.

For further reading on the topic we recommend the booklet from @Bell2001.

<!--chapter:end:4_msa.Rmd-->

```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	comment = NA,
	out.width = "80%"
)
```

# Design of Experiments {#DOE}

In a design of experiements we calculate the total number of trials with the expression $n^m$ where n is the number of levels, m the number of factors. A trial represents the number of unique combinations of the factors. To obtain the final number of test runs we have to multiply the number of trials by the number of replicates per trial.

In a design with 4 factors of 2 levels we have then $2^4 = 16$ runs and $16 \times 5 = 80$ replicates. 

If the design has a combination of factors with different number of levels the number of trials is the multiplication of both such as: $n^m \times n^m$. 

For example if we added 2 additional factors with 4 levels each to the previous design we would obtain $2^4 \times 4^2 = 256$ which we would still need to multiply by the number of replicates to obtain the number of runs $256 \times 5 = 1280$.

In the literature we often see the simbolic notation $a^k$ but we've opted for mF-nL (m factors, n levels) in this book for simplification.

## Simple experiments

1 factor 2 levels

In this chapter we cover introductory designs of experiments and take it progressively until the general 2^k factorial designs. In any case this pretends to be an introduction to the topic with only a subset of the many types of DoEs used in the industry.

### Means comparison

#### t-test one sample

**The Winter Sports clothing manufacturer**

Comparing mean to specification

An engineer working the winter sports clothing industry has established a contract for PET textile raw material supply based on the following specification: the average tensile strength has to be greater than 69.0 $Mpa$ for each delivery. In the contract is also specified that the test protocol which is based on a 30 samples.

<div class="marginnote">

```{r echo=FALSE, out.width="100%", fig.align='center', fig.cap="PET tensile test"}
knitr::include_graphics("img/tensile_test_bw.jpg")
```

</div>

A first delivery is submited and the customer wants to know if the lot average tensile strength exceeds the agreed level and if so, she wants to accept the lot.

We start by loading the first packages we will need:

```{r}
library(tidyverse)
library(skimr)
library(readxl)
library(stats)
library(viridis)
filter <- dplyr::filter
select <- dplyr::select
```

```{r}
library(industRial)
```

The Quality Control department specialist at the reception starts by calcultating the average, a first criteria to reject the batch:

```{r}
pet_spec <- 69
pet_mean <- mean(pet_delivery$A)
pet_mean
```

The average is itself below the spec and the engineer could think to reject the batch right away. She decides nevertheless to observe the variability and for this she decides to plot the raw data on an histogram. An histogram is a very common plot showing counts for selected intervals.

```{r}
pet_delivery %>% 
  ggplot(aes(x = A)) +
  geom_histogram(color = viridis(12)[4], fill = "grey90") +
  geom_vline(xintercept = pet_mean, color = "darkblue", linetype = 3) +
  geom_vline(xintercept = pet_spec, color = "darkgreen", linetype = 2, 
             show.legend = TRUE) +
  # scale_x_continuous(breaks = seq(62, 74, 0.5)) +
  theme_industRial() +
  labs(title = "PET clothing case study",
       subtitle = "Raw data plot",
       x = "Treatment",
       y = "Tensile strength [MPa]")
```

The mean is just slightly below the target mean defined for acceptance and she also observes a certain variability in the batch. She decides then to perform a t-test to assess if the average calculated can be really be considered statistically different than the target value.

[]{#tTest}

```{r}
t.test(x = pet_delivery$A, mu = pet_spec)
```

The basic assumption of the test is that the means are equal and the alternative hypothesis is that the sample mean is different than the spec. The confidence interval selected is 95%.

The test result tells us that for a population average of 69, the probability of obtaining a sample with a value as extreme as 68.71 is of 29.17% (p = 0.2917). This probability value higher than the limit of 5% that she had defined to reject the null hypothesis. She cannot conclude that the sample commes from a population with a mean different than 69 and thus decides to accept the batch.

#### t-test two samples

Comparing means

In order to avoid similar situations in the future the development engineer considers a new chemical compositions of pet that potentially increases the levels of strenght.

**Data loading**

```{r}
pet_delivery_long <- pet_delivery %>%
  pivot_longer(
    cols = everything(), names_to = "sample", values_to = "tensile_strength"
  )
```

**Raw data plot**

In data analysis it is good practice to start by plotting the raw data and have a first open look at what the first plots tell us.

```{r}
pet_delivery_long %>% 
  ggplot(aes(x = sample, y = tensile_strength)) +
  geom_point() +
  theme_industRial() +
  theme(legend.position = "none") +
  labs(title = "PET clothing case study",
       subtitle = "Raw data plot",
       x = "Sample",
       y = "Tensile strength [MPa]")
```

Another way to better understanding the bond distributions is to plot a box plot. This type of plot is somehow like the histogram seen before but more compact when several groups are required to be plotted.

```{r}
pet_delivery_long %>% 
  ggplot(aes(x = sample, y = tensile_strength, fill = sample)) +
  geom_boxplot(width = 0.3) +
  scale_fill_viridis_d(begin = 0.5, end = 0.8) +
  theme_industRial() +
  theme(legend.position = "none") +
  labs(title = "PET clothing case study",
       subtitle = "Box plot",
       x = "Sample",
       y = "Tensile strength [MPa]")
```

We would like to understand if the treatment has an effect. Thus we want to compare the two population means. For that we use a t test using samples obtained independently and randomly. Before running the test we also have to check the normality of the samples distributions and equality of their variances.

To do these checks we're using the stat_qq functions from the ggplot package and plotting the qq plots for both levels in the same plot:

```{r}
pet_delivery_long %>%
  ggplot(aes(sample = tensile_strength, color = sample)) +
  geom_qq() +
  geom_qq_line() +
  coord_flip() +
  scale_color_viridis_d(begin = 0.1, end = 0.7) +
  theme_industRial() +
  labs(title = "PET clothing case study",
       subtitle = "Q-Q plot",
       x = "Residuals",
       y = "Tensile strength [MPa]")
```

We observe that for both levels of treatment the data is adhering to the straight line thus we can assume they follow a normal distribution. Also both lines in the qq plot  have equivalent slopes indicating that the assumption of variances is a reasonable one. These verifications are summary ones. We review in subsequent sessions other deeper verifications of such as the shapiro-wilk normality test.

We're now going to apply the t-test:

```{r}
library(stats)
```

```{r}
t.test(tensile_strength ~ sample, data = pet_delivery_long, var.equal = TRUE)
```

We see that p \< 0.05 thus the means differ significantly. Furthemore the mean difference is estimated with 95% confidence, to be between -0.55 and -0.01 (to be noted that zero is obviously not included in this interval). There is an effect in our treatment that explains the difference in means between the two samples.

### Variances comparison

#### F-test 

We're now confirming this with a variance test from the stats package.

[]{#FTest}

```{r}
var.test(tensile_strength ~ sample, pet_delivery_long)
```

The test null hypothesis is that the variances are equal. Since the p value is much greater than 0.05 we cannot reject the null hypotheses meaning that we can consider them equal.

The F-test is accurate only for normally distributed data. Any small deviation from normality can cause the F-test to be inaccurate, even with large samples. However, if the data conform well to the normal distribution, then the F-test is usually more powerful than Levene's test.

#### Levene test

This test is assessing the homogeneity of variances (homoscedasticity).

[]{#leveneTest}

```{r}
library(car)
```

```{r}
leveneTest(tensile_strength ~ sample, data = pet_delivery_long)
```

Pr \> 0.05 thus there is homogeneity of the variances (they do not differ significantly).

Further elaborations on the variance can be found under @minitab_variances.

<!--chapter:end:5_simpleDOE.Rmd-->

```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	comment = NA
	# out.width = "80%"
)
```

## Regression and anova

One factor multiple levels

**The e-bike frame hardening process**

Mountain bikes frames are submitted to many different efforts, namely bending, compression and vibration. Although no one expects the frame to break in regular usage, manufacturers reputation is made on less visible performance features. One of them is the duration of the bike or in more technical terms in the number of cycles of such efforts that the frame resists. 

<div class="marginnote">

```{r echo=FALSE, fig.align='center', fig.cap="e-bike frames entering hardening treatment", out.width="100%"}
knitr::include_graphics("img/bike_frames_bw.jpg")
```

</div>

### Linear regression

We will present here a first example of the utilisation of linear regression techniques and establish a linear model. These models are going to be used extensively in the upcoming cases.

**Data loading**

```{r}
library(tidyverse)
library(readxl)
library(janitor)
library(scales)
library(stats)
library(knitr)
library(industRial)
filter <- dplyr::filter
select <- dplyr::select
```

```{r}
ebike_narrow <- ebike_hardening %>%
  pivot_longer(
    cols = starts_with("g"),
    names_to = "observation",
    values_to = "cycles"
  ) %>%
  group_by(temperature) %>%
  mutate(cycles_mean = mean(cycles)) %>%
  ungroup()
```

```{r}
slice_head(.data = ebike_narrow, n = 5) %>% 
  kable(align = "c", 
        caption = "e-bike hardening experiment data")
```

#### Raw data plot
 
```{r}
ggplot(data = ebike_narrow) +
  geom_point(aes(x = temperature, y = cycles)) +
  geom_point(aes(x = temperature, y = cycles_mean), color = "red") +
  theme_industRial() +
  scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = "'")) +
  theme(legend.position = "none") +
  labs(title = "e-bike frame hardening process",
       subtitle = "Raw data plot",
       x = "Furnace Temperature [°C]",
       y = "Cycles to failure [n]")
```

#### Linear model

We start by establishing the model, ensuring for now that we leave the variable `temperature` as a numeric vector. 

[]{#linearModel}

```{r}
ebike_lm <- lm(cycles ~ temperature, data = ebike_narrow)
summary(ebike_lm)
```

With the summary function we can many different outputs such as the coefficients and the R-squared which we will look into more detail now. As usual, we first inspect the data with a first plot. In this case we're adding a smoothing geometry with the lm method:

#### Linear model plot

```{r}
ggplot(ebike_narrow) +
  geom_point(aes(x = temperature, y = cycles)) +
  geom_smooth(aes(x = temperature, y = cycles), method = "lm") +
  geom_point(aes(x = temperature, y = cycles_mean), color = "red") +
  theme_industRial() +
  scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = "'")) +
  theme_industRial() +
  theme(legend.position = "none") +
  labs(title = "e-bike frame hardening process",
       subtitle = "Raw data plot",
       x = "Furnace Temperature [°C]",
       y = "Cycles to failure [n]")
```

#### Linear model fixed effects

In our case the experiementer has selected to control the levels of the temperature variable in what is called a fixed effects model, accepting that conclusions in the comparisons of the levels cannot be extended to levels that were not tested. For this we're now going to convert the variable to a factor and establish again the model and note that it will give the same R squared but naturally different coefficients. 

```{r}
ebike_factor <- ebike_narrow %>%
  mutate(temperature = as_factor(temperature))
ebike_lm_factor <- lm(
  cycles ~ temperature, 
  data = ebike_factor,
  contrasts = list(temperature = "contr.treatment")
  )
summary(ebike_lm_factor)
```

In order to be precise, we're making explicit in the lm function that the contrasts argument is "contr.treatment", although this is the default in R. More on contrasts on the Case Study on $2^k$ designs. The current contrasts settings can be seen as follows:

```{r}
getOption("contrasts")
```

We're now ready to assess the validity of the model in order to be ready for our main task which is the comparison of the means using an anova.

### Residuals & model check

In order to assess the model performance we're going to look into the residuals. R provides direct ploting functions with the base and stats packages but in this first example we're going to break down the analysis and further customise the plots. We are also going to make usage of some additional statistical tests to confirm our observations from the plots. In subsequent chapters we'll have a more selective approach, where plots and tests are made on a needed basis.

We start by loading the package broom which will help us retrieving the data from the lm object into a data frame.

Now we build and show below an extract of the "augmented" dataframe

[]{#augment}

```{r}
library(broom)
```

```{r}
ebike_aug <- augment(ebike_lm_factor) %>%
  mutate(index = row_number())
ebike_aug %>%
  head() %>%
  kable(align = "c")
```

We can see we've obtained detailed model parameters such us fitted values and residuals for each DOE run.

#### Time sequence plot

For this plot we need to ensure that the order of plotting in the x axis corresponds exactly to the original data collection order. This plot allows us to assess for strange patterns such as a  tendency to have runs of positive of negative results which indicates that the independency assumption does not hold. If patterns emerge then there may be correlation in the residuals.

```{r}
ebike_aug %>%
  ggplot(aes(x = index, y = .resid)) +
  geom_point() +
  theme_industRial() +
  scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = "'")) +
  labs(
    title = "e-bike frame hardening process",
    subtitle = "Linear model - Residuals timeseries",
    y = "Index",
    x = "Fitted values"
  )
```

Nothing pattern emerges from the current plot and the design presents itself ^well randomised.

#### Autocorrelation test 

It is always good to keep in mind that all visual observations can be complemented with a statistical test. In this case we're going to use the durbinWatson test from the car package (Companion to Applied Regression).

[]{#residualsCorrelation}

```{r}
library(car)
```

```{r}
durbinWatsonTest(ebike_lm_factor)
```

Although the output shows Autocorrelation of -0.53 we have to consider that the p value is greater than 0.05 thus there is not enough significance to say that there is autocorrelation. 

#### Residuals vs fit plot

If the model is correct and the assumptions hold, the residuals should be structureless. In particular they should be unrelated to any other variable including the predicted response.

```{r}
ebike_aug %>%
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
  theme_industRial() +
  geom_smooth(method = "loess", se = FALSE, color = "red") +
  scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = "'")) +
  labs(
    title = "e-bike frame hardening process",
    subtitle = "Linear model - Residuals vs Fitted values",
    y = "Residuals",
    x = "Fitted values"
  )
```

In this plot we see no variance anomalies such as a higher variance for a certain factor level or other types of skweness.

#### Equality of variance test 

In the e-bike hardening process, the normality assumption is not in question, so we can apply Bartlett’s test to the etch rate data.

[]{#barlettTest}

```{r}
bartlett.test(cycles ~ temperature, data = ebike_factor)
```

The P-value is P = 0.934, so we cannot reject the null hypothesis. There is no evidence to counter the claim that all five variances are the same. This is the same conclusion reached by analyzing the plot of residuals versus fitted values.

Notes: 
* the var.test function cannot be used here as it applies to the two levels case only
* this test is sensitive to the normality assumption, consequently, when the validity of this assumption is doubtful, the Bartlett test should not be used and replace by the modified Levene test for example

#### Normality plot

As the sample size is relatively small we're going to use a qq plot instead of an histogram to assess the normality of the residuals.

```{r}
ebike_aug %>%
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line() +
  # coord_flip() +
  theme_industRial() +
  scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = "'")) +
  labs(
    title = "e-bike frame hardening process",
    subtitle = "Linear model - qq plot",
    y = "Residuals",
    x = "Fitted values"
  )
```

The plot suggests normal distribution. We see that the error distribution is aproximately normal. In the fixed effects model we give more importance to the center of the values and here we consider acceptable that the extremes of the data tend to bend away from the straight line.
The verification can be completed by a test. For populations < 50 use the shapiro-wilk normality test.

#### Shapiro test 

[]{#shapiroTest}

```{r}
shapiro.test(ebike_aug$.resid)
```

p > 0.05 indicates that the residuals do not differ significantly from a normally distributed population.

#### Std residuals vs fit plot

This specific Standardized residuals graph also help detecting outliers in the residuals (any residual > 3 standard deviations is a potential outlier).

```{r}
ebike_aug %>% 
  ggplot(aes(x = .fitted, y = .std.resid)) +
  geom_point() +
  theme_industRial() +
  geom_smooth(method = "loess", se = FALSE, color = "red") +
  labs(title = "e-bike frame hardening process",
       subtitle = "Linear model - Standardised Residuals vs Fitted values",
       y = "Standardised Residuals",
       x = "Fitted values")
```

The plot shows no outliers to consider in this DOE.

#### Outlier test

In a case where we were doubtfull we could go further and make a statistical test to assess if a certain value was an outlier. A usefull test is available in the car package.

```{r}
outlierTest(ebike_lm_factor)
```

In this case, the Bonferroni adjusted p value comes as NA confirming that there is no outlier in the data.   

#### Cooks distance plot

```{r}
ebike_aug %>% 
  ggplot(aes(x = .cooksd, y = .std.resid)) +
  geom_point() +
  geom_vline(xintercept = 0.5, color = "red") +
  theme_industRial() +
  labs(title = "e-bike frame hardening process",
       subtitle = "Residuals vs Leverage",
       y = "Standardised Residuals",
       x = "Cooks distance")
```

#### R squared 

R² the coefficient of determination

The R square can be extracted from the linear model that has been used to build the Anova model.

```{r}
summary(ebike_lm_factor)$r.squared
```

Thus, in the e-bike hardening process, the factor “temperature” explains about 88% percent of the variability in etch rate.

Anova fixed effects assumes that:
- errors are normally distributed and are independent

As the number of residuals is too small we're not checking the normality via the histogram but rather with a a Q-Q plot.

### Multiple means comparison

#### Box plot of raw data

We can also compare medians and get a sense of the effect of the treatment levels by looking into the box plot:

```{r}
ggplot(ebike_factor, 
       aes(x = temperature, y = cycles, fill = temperature)) +
  geom_boxplot() +
  theme_industRial() +
  scale_fill_viridis_d(option = "D", begin = 0.5) +
  theme_industRial() +
  scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = "'")) +
  theme(legend.position = "none") +
  labs(title = "e-bike frame hardening process",
       subtitle = "Raw data plot",
       x = "Furnace Temperature [°C]",
       y = "Cycles to failure [n]")
```

1 factor with severals levels + 1 continuous dependent variable
Similar to the t-test but extended - this test allows to compare the means between several levels of treatement for a continuous response variable (the t test is only 2 levels at a time, performing all pair wise t-tests would also not be a solution because its a lot of effort and would increase the type I error)

ANOVA principle: the total variability in the data, as measured by the total corrected sum of squares, can be partitioned into a sum of squares of the differences between the treatment averages and the grand average plus a sum of squares of the differences of observations within treatments from the treatment average

#### Anova fixed effects 

In R the anova is built by passing the linear model to the anova or aov functions. The output of the anova function is just the anova table as shown here for this first example. The output of the aov function is a list.

[]{#anova}

```{r}
ebike_aov_factor <- aov(ebike_lm_factor)
summary(ebike_aov_factor)
```

Note that the RF temperature or between-treatment mean square (22,290.18) is many times larger than the within-treatment or error mean square (333.70). This indicates that it is unlikely that the treatment means are equal. 
Also p < 0.05 thus we can reject the null hypothesis and conclude that the means are significantly different.

#### Anova (no significance)

Anova on plasma etching, modification of the example to achieve a p > 0.05:

```{r}
ebike_narrow2 <- ebike_hardening2 %>%
  pivot_longer(
    cols = starts_with("g"),
    names_to = "observation",
    values_to = "cycles"
  ) %>%
  group_by(temperature) %>%
  mutate(cycles_mean = mean(cycles)) %>%
  ungroup()
ebike_factor2 <- ebike_narrow2
ebike_factor2$temperature <- as.factor(ebike_factor2$temperature)

ebike_lm_factor2 <- lm(cycles ~ temperature, data = ebike_factor2)
anova(ebike_lm_factor2)
```

```{r}
ggplot(ebike_factor2, 
       aes(x = temperature, y = cycles, fill = temperature)) +
  geom_boxplot() +
  scale_y_continuous(n.breaks = 10) +
  scale_fill_viridis_d(option = "A", begin = 0.5) +
  theme_industRial() +
  theme(legend.position = "none") +
  scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = "'")) +
  labs(title = "e-bike frame hardening process",
       subtitle = "Raw data plot",
       x = "Furnace Temperature [°C]",
       y = "Cycles to failure [n]")
```

P > 0.05 - there is no significant difference between the means

### Pairwise comparisons

#### Tukey's test 

The Anova may indicate that the treament means differ but it won't indicate which ones. In this case we may want to compare pairs of means.

[]{#tukeyTest}

```{r}
ebike_tukey <- TukeyHSD(ebike_aov_factor, ordered = TRUE)
```

```{r}
head(ebike_tukey$temperature) %>% 
  kable(align = "c", 
        caption = "tukey test on e-bike frame hardening process", 
        booktabs = T)
```

The test provides us a simple direct calculation of the differences between the treatment means and a confidence interval for those. Most importantly it provides us with the p value to help us confirm the significance of the difference and conclude factor level by factor level which differences are significant.

Additionally we can obtain the related plot with the confidence intervals 

```{r}
plot(ebike_tukey)
```

#### Fisher's LSD 

Fisher's Least Significant difference is an alternative to Tuckey's test.

[]{#fisherLSD}

```{r}
library(agricolae)
```

```{r}
ebike_anova <- anova(ebike_lm_factor) 

ebike_LSD <- LSD.test(y = ebike_factor$cycles,
         trt = ebike_factor$temperature,
         DFerror = ebike_anova$Df[2],  
         MSerror = ebike_anova$`Mean Sq`[2],
         alpha = 0.05)
```

The Fisher procedure provides us with many additional information. A first outcome is the difference between means (of life cycles) that can be considered significant, indicated in the table below by LSD = 24.49.

```{r}
head(ebike_LSD$statistics) %>% 
  kable(align = "c", 
        caption = "Fisher LSD procedure on e-bike frame hardening: stats",
        booktabs = T)
```

Furthermore it gives us a confidence interval for each treatment level mean:

```{r}
head(ebike_LSD$means) %>% 
  # as_tibble() %>%
  rename(cycles = `ebike_factor$cycles`) %>%
  select(-Min, -Max, -Q25, -Q50, -Q75) %>%
  kable(align = "c", 
        caption = "Fisher LSD procedure on e-bike frame hardening: means", 
        booktabs = T)
```

We can see for example that for temperature 220 °C the etch rate if on average 707.0 with a probability of 95% of being between 689.7 and 724.3 A/min.

Another interesting outcome is the grouping of levels for each factor:

```{r}
head(ebike_LSD$groups) %>% 
  kable(align = "c", 
        caption = "Fisher LSD procedure on e-bike frame hardening: groups", 
        booktabs = T)
```

In this case as all level means are statistically different they all show up in separate groups, each indicated by a specific letter.

Finally we can get from this package a plot with the Least significant difference error bars:

```{r}
plot(ebike_LSD)
```

And below we're exploring a manual execution of this type of plot (in this case with the standard deviations instead).

```{r}
ebike_factor %>%
  group_by(temperature) %>%
  summarise(cycles_mean = mean(cycles), 
            cycles_sd = sd(cycles)) %>%
  ggplot(aes(x = temperature, y = cycles_mean)) +
  geom_point(size = 2) +
  geom_line() +
  geom_errorbar(aes(ymin = cycles_mean - cycles_sd, 
                    ymax = cycles_mean + cycles_sd),
                width = .1) +
  scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = "'")) +
  # scale_color_viridis_d(option = "C", begin = 0.1, end = 0.9) +
  annotate(geom = "text", x = Inf, y = -Inf, label = "Error bars are +/- 1xSD", 
    hjust = 1, vjust = -1, colour = "grey30", size = 3, 
    fontface = "italic") +
  theme_industRial() +
  labs(title = "e-bike frame hardening process",
       subtitle = "Raw data plot",
       x = "Furnace Temperature [°C]",
       y = "Cycles to failure [n]")
```

As often with statistical tools, there is debate on the best approach to use. We recommend to combine the Tukey test with the Fisher's LSD completementary R functions. The Tukey test giving a first indication of the levels that have an effect and calculating the means differences and the Fisher function to provide much more additional information on each level. To be considered in each situation the slight difference  between the significance level for difference between means and to decide if required to take the most conservative one.

To go further in the Anova F-test we recommend this interesting article from @minitab_anovaftest.

### Prediction 

Following the residuals analysis and the anova our model is validated. 

A model is usefull for predictions. In a random effects model where conclusions can applied to the all the population we can predict values at any value of the input variables. In that case reusing the model with temperature as a numeric vector we could have a prediction for various temperature values such as:

[]{#predict}

```{r}
ebike_new <- tibble(temperature = c(170, 160, 200, 210))
predict(ebike_lm, newdata = ebike_new)
```

We can see that the prediction at the tested levels is slightly different from the measured averages at those levels. This is because the linear interpolation line is not passing exactly by the averages.

Anyway this is a fixed effects model and we can only take conclusions at the levels at which the input was tested. We can check that the predictions correspond to the averages we've calculated for each level:

```{r}
ebike_new <- data.frame(temperature = as_factor(c("160", "200")))
predict(ebike_lm_factor, newdata = ebike_new)
```












<!--chapter:end:6_regression.Rmd-->

```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	comment = NA
	# out.width = "80%"
)
```

```{r}
library(tidyverse)
library(readxl)
library(stats)
library(knitr)
library(industRial)
filter <- dplyr::filter
select <- dplyr::select
```

## Interactions

Two factors multiple levels

**The solarcell output test**

<div class="marginnote">

```{r echo=FALSE, fig.align='center', fig.cap="solar panel test chamber", out.width="100%"}
knitr::include_graphics("img/solar_cell_test_bw.jpg")
# sources: 
# https://www.pse.de/test-equipment/photovoltaic-modules/uv-chamber/
# https://en.wikipedia.org/wiki/Solar_cell_efficiency
```

</div>

Load and prepare data for analysis:

```{r}
solarcell_factor <- solarcell_output %>% 
  pivot_longer(
    cols = c("T-10", "T20", "T50"),
    names_to = "temperature",
    values_to = "output"
  ) %>% mutate(across(c(material, temperature), as_factor))
```

### lm with interactions

```{r}
solarcell_factor_lm <- lm(
  output ~ temperature + material + temperature:material, 
  data = solarcell_factor
  )
summary(solarcell_factor_lm)
```

Looking at the output we see that R-squared is equal to 0.7652. This means about 77 percent of the variability in the battery life is explained by the plate material in the battery, the temperature, and the material type–temperature interaction. We're going to go more in details now to validate the model and understand the effects and interactions of the different factors.

### Outliers and model check 

We start by an assessment of the residuals, starting by the timeseries of residuals:

```{r}
plot(solarcell_factor_lm$residuals)
```

No specific pattern is apparent so now we check all the remaining plots grouped into one single output:

```{r}
par(mfrow = c(2,2))
plot(solarcell_factor_lm)
```

Residuals versus fit presents a rather simetrical distribution around zero indicating equality of variances at all levels and the qq plot presents good adherence to the centel line indicating a normal distributed population of residuals, all ok for these. The scale location plot though, shows a center line that is not horizontal which suggest the presence of outliers.

We can extract the absolute maximum residual with:

```{r}
solarcell_factor_lm$residuals %>% abs() %>% max()
```

Inspecting again the residuals plots we see that this corresponds to the point labeled with 2 for which the standardized value is greater than 2 standard deviations. 

We're therefore apply the outlier test from the car package:

[]{#outlierTest}

```{r}
library(car)
```

```{r}
outlierTest(solarcell_factor_lm)
```

which gives a high Bonferroni p value thus excluding this possibility.

### Interaction plot 

In this experiement instead of just plotting a linear regression we need to go for a more elaborate plot that shows the response as a function of the two factors. Many different approaches are possible in R and here we're starting with a rather simple one - the interaction plot from the stats package:

[]{#interactionPlot}

```{r}
interaction.plot(x.factor = solarcell_factor$temperature, 
                 trace.factor = solarcell_factor$material,
                 fun = mean,
                 response = solarcell_factor$output,
                 trace.label = "Material",
                 legend = TRUE,
                 main = "Temperature-Material interaction plot",
                 xlab = "temperature [°C]",
                 ylab = "output [kWh/yr equivalent]")
```

Although simple many important learnings can be extracted from this plot. We get the indication of the mean value of battery life for the different data groups at each temperature level for each material. Also we see immediatly that batteries tend to have longer lifes at lower temperature for all material types. We also see that there is certainly an interaction between material and temperature as the lines cross each other.

### Effects significance

As the R-squared was rather high and there were no issues with residuals we considere the model as acceptable and move ahead with the assessment of the significance of the different effects. For that we apply the anova to the linear model:

```{r}
solarcell_factor_aov <- aov(solarcell_factor_lm)
summary(solarcell_factor_aov)
```

We see in the output little stars in front of the p value of the different factors. Three stars for temperature corresponding to an extremely low p value indicating that the means of the lifetime at different levels of temperature are significantly different, confirming that temperature has an effect on lifetime. With a lower significance but still clearly impacting lifetime depends on the material. Finally it is confirmed that there is an interaction between both factors has the temperature:material term has a p value of 0.01861 which us lower than the treshold of 0.05.

The interaction here corresponds to the fact that increasing temperature from 15 to 70 decreases lifetime for material 2 but increases for material 3.

### Removing interaction

Its interesting to consider what would have been the analysis if the interaction was not put in the model. We can easily assess that by creating a new model in R without the temperature:material term.

```{r}
solarcell_factor_lm_no_int <- lm(
  output ~ temperature + material, data = solarcell_factor)
summary(solarcell_factor_lm_no_int)
```

The model still presents a reasonably high R-square of 0.64. We now apply the anova on this new model:

```{r}
battery_aov_no_int <- aov(solarcell_factor_lm_no_int)
summary(battery_aov_no_int)
```

The output naturally confirms the significance of the effects of the factors, however, as soon as a residual analysis is performed for these data, it becomes clear that the non-interaction model is inadequate:

```{r}
par(mfrow = c(2,2))
plot(solarcell_factor_lm_no_int)
```

We see in the Residuals vs Fitted a clear pattern with residuals moving from positive to negative and then again to positive along the fitted values axis which indicates that there is an interaction at play.

## Covariance

We assess here the potential utilisation of the analysis of covariance (ancova) in situations where a continuous variable may be influencing the measured value. This technique complements the analysis of variance (anova) allowing for a more accurate assessment of the effects of the categorical variables.

Below a description of the approach taken from [@Montgomery2012], pag.655:

*Suppose that in an experiment with a response variable y there is another variable, say x, and that y is linearly related to x. Furthermore, suppose that x cannot be controlled by the experimenter but can be observed along with y. The variable x is called a covariate or concomitant variable. The analysis of covariance involves adjusting the observed response variable for the effect of the concomitant variable.* 

*If such an adjustment is not performed, the concomitant variable could inflate the error mean square and make true differences in the response due to treatments harder to detect. Thus, the analysis of covariance is a method of adjusting for the effects of an uncontrollable nuisance variable. As we will see, the procedure is a combination of analysis of variance and regression analysis.*

*As an example of an experiment in which the analysis of covariance may be employed, consider a study performed to determine if there is a difference in the strength of a monofilament fiber produced by three different machines. The data from this experiment are shown in Table 15.10 (below). Figure 15.3 presents a scatter diagram of strength (y) versus the diameter (or thickness) of the sample. Clearly, the strength of the fiber is also affected by its thickness; consequently, a thicker fiber will generally be stronger than a thinner one. The analysis of covariance could be used to remove the effect of thickness (x) on strength (y) when testing for differences in strength between machines.*

```{r, fig.cap = "table 15.10"}
solarcell_fill %>% 
  kable()
```

Below a plot of strenght by thickness:

```{r}
solarcell_fill %>%
  ggplot(aes(x = fillfactor, y = output)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_industRial() +
  labs(
    title = "The solarcell output test",
    subtitle = "Output vs Fill Factor",
    x = "Fill factor [%]",
    y = "Output"
  )
```

### Correlation strenght

And a short test to assess the strenght of the correlation:

```{r}
library(stats)
```

[]{#corTest}

```{r}
cor.test(solarcell_fill$output, solarcell_fill$fillfactor)
```

Going further and using the approach from [@Broc2016] I'm faceting the scatterplots to assess if the coefficient of the linear regression is similar for all the levels of the machine factor:

```{r}
solarcell_fill %>%
  ggplot(aes(x = fillfactor, y = output)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(vars(material)) +
  theme_industRial() +
  labs(
    title = "The solarcell output test",
    subtitle = "Output vs Fill Factor, by material type",
    x = "Fill factor [%]",
    y = "Output"
  )
```

Visually this is the case, going from one level to the other is not changing the relationship between thickness and strenght - increasing thickness increases stenght. Visually the slopes are similar but the number of points is small. In a real case this verification could be extended with the correlation test for each level or/and a statistical test between slopes.

We're now reproducing in R the ancova case study from the book, still using the aov function.
The way to feed the R function arguments is obtained from https://www.datanovia.com/en/lessons/ancova-in-r/

*Three different machines produce a monofilament fiber for a textile company. The process engineer is interested in determining if there is a difference in the breaking strength of the fiber produced by the three machines. However, the strength of a fiber is related to its diameter, with thicker fibers being generally stronger than thinner ones. A random sample of five fiber specimens is selected from each machine.*

### Ancova

[]{#ancova}

```{r}
solarcell_ancova <- aov(
  output ~ fillfactor  + material, solarcell_fill
  )
summary(solarcell_ancova)
```

> Note that in the formula the covariate goes first (and there is no interaction)! If you do not do this in order, you will get different results.

* material in this table corresponds to the adjusted material mean square

Conclusions from the book in page 662:

*Comparing the adjusted treatment means with the unadjusted treatment means (the y i. ), we note that the adjusted means are much closer together, another indication that the covariance analysis was necessary.*

*A basic assumption in the analysis of covariance is that the treatments do not influence the covariate x because the technique removes the effect of variations in the x i. . However, if the variability in the x i. is due in part to the treatments, then analysis of covariance removes part of the treatment effect. Thus, we must be reasonably sure that the treatments do not affect the values x ij.*

*In some experiments this may be obvious from the nature of the covariate, whereas in others it may be more doubtful. In our example, there may be a difference in fiber diameter (x ij ) between the three machines. In such cases, Cochran and Cox (1957) suggest that an analysis of variance on the x ij values may be helpful in determining the validity of this assumption. ...there is no reason to believe that machines produce fibers of different diameters.*

(I did not go further here as it goes beyond the scope of the assessment)

### Comparison with anova

Below I'm doing the common approach we've been using at NSTC in design of experiments.

```{r}
solarcell_aov <- aov(output ~ material, solarcell_fill)
summary(solarcell_aov)
```

The anova table obtained also corresponds correctly to the book example. 

Montgomery final observations:

*It is interesting to note what would have happened in this experiment if an analysis of covariance had not been performed, that is, if the breaking strength data (y) had been analyzed as a completely randomized single-factor experiment in which the covariate x was ignored. The analysis of variance of the breaking strength data is shown in Table 15.14. We immediately notice that the error estimate is much longer in the CRD analysis (17.17 versus 2.54). This is a reflection of the effectiveness of analysis of covariance in reducing error variability.*

*We would also conclude, based on the CRD analysis, that machines differ significantly in the strength of fiber produced. This is exactly opposite the conclusion reached by the covariance analysis.*

*If we suspected that the machines differed significantly in their effect on fiber strength, then we would try to equalize the strength output of the three machines. However, in this problem the machines do not differ in the strength of fiber produced after the linear effect of fiber diameter is removed. It would be helpful to reduce the within-machine fiber diameter variability because this would probably reduce the strength variability in the fiber.*

Potential applications

In the scope of methods validations this approach could potentially be used in robustness validations when there is suspiction that a continuous variable is disturbing the measurement.

Naturally this should not be applied everywhere but only where there would to be logical a physical or chemical reason behind as in the example with thickness and strenght.

<!--chapter:end:7_interactions.Rmd-->

```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	comment = NA
	# out.width = "80%"
)
```

## General factorial designs

m factors n levels

```{r}
library(tidyverse)
library(readxl)
library(stats)
library(broom)
library(industRial)
library(patchwork)
library(knitr)
filter <- dplyr::filter
select <- dplyr::select
```

**The juice production plant**

We're comming back to our Juice Bottling context where a quality team was looking to put in operation a new measurement device for dry matter content in a juices bottling line.

After a short brainstorming using the Ishikawa tool presented before the team has identified several potential influcing parameters on the equipment bias when compared with the reference equipement: the product itself, the drymatter level on the product (its target), the speed of the filling line and the poweder particle size. In order to evaluate such impact the team has prepared a mid size experiment design with three products, three levels of drymatter, two line speed levels and two particle size levels.

First we load the DoE.base package:

```{r}
library(DoE.base)
```

and then generate the doe with the fac.design function.

### Design generaction 

[]{#fac.design}

```{r}
juice_doe <- fac.design(
  randomize = FALSE,
  factor.names = list(
    product = c("beetroot", "apple", "carrot"), 
    drymatter_target = c(10, 15, 20),
    part = c(1, 2, 3),
    speed = c(200, 250),
    particle_size = c(250, 300))
)
```

Note that the DoE generated is more than just a tibble, it belongs to a specific class called design and has many other attributes just like an lm or aov S3 objects.

```{r}
class(juice_doe)
```

The power and care given by the package authors become visible when we use an R generic function such as summary() with this object and we see it returns a tailor made output, in this case showing the levels of the different factors of our design: 

```{r}
summary(juice_doe)
```

Using this the team has simple copied the experiment plan to an spreadsheet to collect the data:

```{}
juice_doe %>% 
  write_clip() 
```

and after a few day the file completed and ready for analysis looked like:

```{r}
juice_drymatter %>%
  head() %>%
  kable()
```


```{r}
juice_drymatter <- juice_drymatter %>%
  mutate(bias = drymatter_DRX - drymatter_REF)

juice_drymatter_factor <- juice_drymatter %>%
  mutate(across(
    c(product, drymatter_TGT, speed, particle_size, part), 
    as_factor))
```

### Main effects plots

As the number of factors and levels of a design increase, more thinking is required to obtain good visualisation of the data. 

Main effects plots consist usually of a scatterplot representing the experiment output as a function of one of the inputs. In a design like this with three different inputs three plots are required:

```{r}
drymatter_TGT_plot <- juice_drymatter %>%
  group_by(drymatter_TGT) %>%
  summarise(bias_m_drymatter = mean(bias)) %>%
  ggplot(aes(x = drymatter_TGT, y = bias_m_drymatter)) +
  geom_point() +
  geom_line() +
  coord_cartesian(
    xlim = c(9,21),
    ylim = c(-1,0), expand = TRUE) +
  labs(
    title = "Juice bottling problem",
    subtitle = "Main effects plots",
    x = "drymatter_TGT [%]",
    y = "Average bias [g]"
  ) +
  theme_industRial()

particle_size_plot <- juice_drymatter %>%  
  group_by(particle_size) %>%
  summarise(particle_size_bias_mean = mean(bias)) %>%
  ggplot(aes(x = particle_size, y = particle_size_bias_mean)) +
  geom_point() +
  geom_line() +
  coord_cartesian(
    xlim = c(240,310), 
    ylim = c(-1,0), expand = TRUE) +
  labs(
    x = "particle_size",
    y = "Average bias [g]"
  ) +
  theme_industRial()

speed_plot <- juice_drymatter %>%  
  group_by(speed) %>%
  summarise(speed_bias_mean = mean(bias)) %>%
  ggplot(aes(x = speed, y = speed_bias_mean)) +
  geom_point() +
  geom_line() +
  coord_cartesian(
    xlim = c(19, 26),
    ylim = c(-1,0), expand = TRUE) +
  labs(
    x = "Speed",
    y = "Average bias [g]"
  ) +
  theme_industRial()

drymatter_TGT_plot + particle_size_plot + speed_plot
```

This kind of plots gives already important insights in to the experiement outcome, even before any deeper analysis with a linear model and anova. In our case:

* higher particle_size and higher speed result in higher bias weight deviation
* beyond 10.5% drymatter_TGT level the bias weight is always higher than the target

### Interactions plots

In designs like these with 3 factors we have 3 possible interactions (A-B, A-C, B-C) corresponding the the possible combination between them. This results in three interaction plots that we're presenting below. The approach here goes beyond the interaction.plot function from the `{stats}` package presented previously in the two factors multiple levels case. We are developping here the plots with {ggplot2} which provides much more control on the plot attibutes but on the other hand requires that additional code is added to calculate the means by group.

```{r}
drymatter_TGT_particle_size_plot <- juice_drymatter %>%  
  group_by(drymatter_TGT, particle_size) %>%
  summarise(drymatter_TGT_bias_mean = mean(bias)) %>%
  ggplot(aes(x = drymatter_TGT, y = drymatter_TGT_bias_mean)) +
  geom_point(aes(group = particle_size)) +
  geom_line(aes(group = particle_size, linetype = as_factor(particle_size))) +
  scale_linetype(name = "particle_size") +
  coord_cartesian(
    xlim = c(9,21),
    ylim = c(-1,0), expand = TRUE) +
  labs(
    title = "Juice bottling problem",
    subtitle = "Interaction plots",
    x = "drymatter_TGT",
    y = "Average bias deviation [g]"
  ) +
  theme_industRial() +
  theme(legend.justification=c(1,0), legend.position=c(1,0))

drymatter_TGT_speed_plot <- juice_drymatter %>%  
  group_by(drymatter_TGT, speed) %>%
  summarise(drymatter_TGT_bias_mean = mean(bias)) %>%
  ggplot(aes(x = drymatter_TGT, y = drymatter_TGT_bias_mean)) +
  geom_point(aes(group = speed)) +
  geom_line(aes(group = speed, linetype = as_factor(speed))) +
  scale_linetype(name = "Speed") +
  coord_cartesian(
    xlim = c(9, 21),
    ylim = c(-1,0), expand = TRUE) +
  labs(
    x = "drymatter_TGT",
    y = "Average bias deviation [g]"
  ) +
  theme_industRial() +
  theme(legend.justification=c(1,0), legend.position=c(1,0))

speed_particle_size_plot <- juice_drymatter %>%  
  group_by(speed, particle_size) %>%
  summarise(speed_bias_mean = mean(bias)) %>%
  ggplot(aes(x = speed, y = speed_bias_mean)) +
  geom_point(aes(group = particle_size)) +
  geom_line(aes(group = particle_size, linetype = as_factor(particle_size))) +
  scale_linetype(name = "particle_size") +
  coord_cartesian(
    xlim = c(19, 26),
    ylim = c(-1,0), expand = TRUE) +
  labs(
    x = "Speed",
    y = "Average bias deviation [g]"
  ) +
  theme_industRial() +
  theme(legend.justification=c(1,0), legend.position=c(1,0))

drymatter_TGT_particle_size_plot + drymatter_TGT_speed_plot + speed_particle_size_plot
```

The plots indicate no interaction between the different factors as all lines do not intercept and are mostly parallel.

In most cases the anova would be performed first and only the plot for the significant interactions would be plotted, if any.

### Anova with 3rd level interactions

The sources of variation for the Anova table for three-factor fixed effects model are: A, B, C, AB, AC, BC, ABC. To be noted that like in the two-factors we must have at least two parts (n>2) to determine the sum of squares due to error if all possible interactions are to be included in the model.

We are now fully prepared for an assessment of the effect of the different factors with the anova. To reduce the amount of coding we're inputing the model directly in the aov function:

```{r}
juice_drymatter_aov <- aov(
  bias ~ drymatter_TGT * speed * particle_size,
  data = juice_drymatter)
summary(juice_drymatter_aov)
```

The observations of the plots are confirmed and completed with statistical input: we see that the percentage of drymatter_TGT and the particle_size significantly affect the bias volume (p < 0.05). The drymatter_TGT-particle_size interactions are non significative.

As expected the anova confirms strong influence of the dissolution level on the bias.

From the analysis all interactions could be removed from the model in order to establish a predictive model.

<!--chapter:end:8_generalDOE.Rmd-->

```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	comment = NA
	# out.width = "80%"
)
```

## Two level designs

### Coding factors

2 factors 2 levels

The $2^{k}$ designs are particularly useful in the early stages of experimental work when many factors are likely to be investigated. It provides the smallest number of runs with which k factors can be studied in a complete factorial design. Consequently, these designs are widely used in factor screening experiments.

The validity of the analysis depends on the following assumptions:

* the factors are fixed
* the designs are completely randomized
* the usual normality assumptions are satisfied
* the response is approximately linear over the range of the factor levels chosen

Analysis Procedure for a 2 k Design

1. Estimate factor effects
2. Form initial model (full model)
  a. If the design is replicated, fit the full model
  b. If there is no replication, form the model using a normal probability plot of the effects
3. Perform statistical testing (Anova)
4. Refine model (remove non significant effects)
5. Analyze residuals
6. Interpret results

DEF - Sparsity of effects principle: most systems are dominated by some of the main effects and low-order interactions, and most high-order interactions are negligible.

In this first Case Study dedicated to $2^k$ designs we're going to explore the contrasts settings in the linear model functions.

**The PET clothing improvement plan**

In this case study factors have only 2 levels. 

```{r}
library(tidyverse)
library(readxl)
library(stats)
library(industRial)
library(knitr)
filter <- dplyr::filter
select <- dplyr::select
```

Below we start by preparing our dataset:

```{r}
library(DoE.base)
```

```{r}
pet_doe <- fac.design(
  randomize = FALSE,
  factor.names=list(A=c("-","+"), 
                    B=c("-","+"),
                    replicate = c("I", "II", "III"))
  )
```

```{r}
yield <- c(64.4,82.8,41.4,71.3,57.5,73.6,43.7,69.0,62.1,73.6,52.9,66.7)

pet_doe <- bind_cols(
  pet_doe,
  "yield" = yield,
)
```

#### Factors as +/-

In this first model we're using a design where the inputs levels have been defined as plus and minus, sometimes also called high and low. The actual naming is not important, what is critical is to ensure that those input parameters are coded as factors. 

```{r}
pet_fct <- pet_doe %>%
  mutate(across(c(A,B), as_factor))
```

Another detail is to put the higher level as the reference otherwise we will get inverted signs in the lm output:

[]{#relevel}

```{r}
pet_fct$A <- relevel(pet_fct$A, ref="+")
pet_fct$B <- relevel(pet_fct$B, ref="+")
```


and one final step is need which is the setup of the contrasts. As our design is ortogonal and we want the contrasts to add up to zero we have to indicate that on the factor so that the coefficients of the linear model are correctly calculated. The current definition of the contrasts is:

```{r}
contrasts(pet_fct$A)
```

So we change this with:

[]{#contrasts}

```{r}
contrasts(pet_fct$A) <- "contr.sum"
contrasts(pet_fct$B) <- "contr.sum"
contrasts(pet_fct$A)
contrasts(pet_fct$A)
```

Now we can run our linear model:

```{r}
pet_ctr_lm <- lm(
  formula = yield ~ A * B, 
  data = pet_fct
  )
summary(pet_ctr_lm)
```

We can observe in the output that the p value of the effects is the same in the lm and in the the aov functions. This confirms that the contrasts have been correctly specified with contr.sum

Note that we've had to adjust the contrasts in the lm function with contr.sum which applies to cases where the sum of the contrasts is zero (the R default is contr.treatment which applies to cases where the levels are coded as 0 and 1).

and now going to apply a prediction:

```{r}
predict(pet_ctr_lm, newdata = list(A = "+", B = "+"))
```

#### Factors as +/- 1 

In this example we convert the levels to factors still using the +/-1 notation. This will also be helpfull to apply what are called the Yates tables.

```{r}
coded <- function(x) { ifelse(x == x[1], -1, 1) }
```

We again convert them to factors and put the upper level as the reference. Regarding the contrasts we show a simpler and more direct approach now by defining them directly in the lm() function.

```{r}
pet_fct <- pet_fct %>% mutate(cA = coded(A), cB = coded(B))
pet_fct2 <- pet_fct %>% mutate(across(c(cA, cB), as_factor))
pet_fct2$cA <- relevel(pet_fct2$cA, ref = "1")
pet_fct2$cB <- relevel(pet_fct2$cB, ref = "1")

pet_ctr2_lm <- lm(
  formula = yield ~ cA * cB, 
  data = pet_fct2,
  contrasts = list(cA = "contr.sum", cB = "contr.sum")
  )
summary(pet_ctr2_lm)
```

Note that a coefficient in a regression equation is the change in the response when the corresponding variable changes by +1. Special attention to the + and - needs to be taken with the R output.

As A or B changes from its low level to its high level, the coded variable changes by 1 − (−1) = +2, so the change in the response is twice the regression coefficient.

So the effects and interaction(s) from their minumum to their maximum correspond to  twice the values in the “Estimate” column. These regression coefficients are often called effects and interactions, even though they differ from the definitions used in the designs themeselves.

Checking now with coded factors:

```{r}
predict(pet_ctr2_lm, newdata = list(cA = "1", cB = "1"))
```

#### Factors as +/- 1 numeric

In this example we're going to code the levels with +1/-1 but we're going use the numeric coding:

```{r}
pet_num <- pet_fct %>% mutate(cA = coded(A), cB = coded(B))
pet_num_lm <- lm(
  formula = yield ~ cA * cB, 
  data = pet_num
  )
summary(pet_num_lm)
```

In this case we did not define any contrasts. Looking into the lm We can see we've obtained exactly the same outputs.

```{r}
predict(pet_num_lm, newdata = list(cA = 1, cB = 1))
```

As the inputs are coded as numeric this behaves just like the first simple linear model we've seen in the Case Study on One Factor with Multiple levels. In particular when we feed the predictions function with numeric values.

This is very intuitive as it corresponds to the original units of the experiments (also called natural or engineering units). On the other hand coding the design variables provides another advange: generally, the engineering units are not directly comparable while coded variables are very effective for determining the relative size of factor effects.

We can see that these three ways of coding the variable levels lead to equivalent results both in lm and prediction. Our preference goes to use numeric values as it is more intuitive and allows for easier prediction between the fixed levels. 

And now in order to better understand the coding of factors in this unit, we're going to establish a simple regression plot of our data:

```{r}
pet_num %>% 
  unclass() %>% 
  as_tibble() %>%
  mutate(cA = coded(A), cB = coded(B)) %>%
  pivot_longer(
    cols = c("cA", "cB"),
    names_to = "variable",
    values_to = "level") %>% 
  ggplot() +
  geom_point(aes(x = level, y = yield)) +
  geom_smooth(aes(x = level, y = yield), 
              method = "lm", se = FALSE, fullrange = TRUE) +
  # coord_cartesian(xlim = c(-2, 2)) +
  # geom_hline(yintercept = 27.5, color = "grey50") +
  # scale_y_continuous(n.breaks = 20) +
  facet_wrap(vars(variable)) +
  theme_industRial()
```

Note that we had to extract the data from the S3 doe object, which we've done with using unclass() and then as_tibble()

The intercept passes at 27.5 as seen on the lm summary. We're going now to put the B factor at its maximum and replot:

```{r}
pet_num %>% 
  unclass() %>%
  as_tibble() %>%
  mutate(cA = coded(A), cB = coded(B)) %>%
  filter(cB == 1) %>%
  pivot_longer(
    cols = c("cA", "cB"),
    names_to = "variable",
    values_to = "level") %>% 
  ggplot() +
  geom_point(aes(x = level, y = yield)) +
  geom_smooth(aes(x = level, y = yield), 
              method = "lm", se = FALSE, fullrange = TRUE) +
  coord_cartesian(xlim = c(-2, 2)) +
  scale_y_continuous(n.breaks = 10) +
  facet_wrap(vars(variable)) +
  theme_industRial()
```

As seen on the plot the output of our prediction is 69 corresponding the high level of A when B is at 1. To be precise we need to multiply all the coefficients by the levels of the factors as : 63.250 + 9.583x(+1) - 5.750x(+1) + 1.917

#### sd bars in interaction plots 

Here we're making a step further in the representation of interaction plots, we're adding error bars to the means. There are many ways to do this and we're providing a simple approach with the function plotMeans from the package RcmdrMisc.

[]{#plotMeans}

```{r}
library(RcmdrMisc)
```

We select standard error as argument for the error.bars argument.

```{r}
par(mfrow = c(1,1), bty = "l")
plotMeans(response = pet_fct$yield,
          factor2 = pet_fct$A,
          factor1 = pet_fct$B,
          error.bars = "se",
          xlab = "A - Reactant",
          legend.lab = "B - Catalist\n(error bars +/-se)",
          ylab = "Yield",
          col = viridis::viridis(12)[4],
          legend.pos = "bottomright",
          main = "The PET clothing improvement plan")
```

### Coding natural values

3 factors 2 levels

The plasma etching example

A - gap in cm
B - flow
C - power in W
response - etch rate in Angstrom/m

```{r}
pls <- read.csv("~/Documents/data_science/industRial/data-raw/6-3_plasma.csv")

plsn <- pls %>% 
  gather(replicate, etch, Rep1, Rep2)
plsn_fct <- plsn %>%
  mutate(across(c(A,B,C), as_factor))
```

```{r eval=FALSE}
pls <- fac.design(
  randomize = FALSE,
  factor.names=list(A=c(-1,1), 
                    B=c(-1,1),
                    C=c(-1,1),
                    replicate = c("Rep1", "Rep2"))
  )
```

```{r eval=FALSE}
etch <- c(550, 669, 633, 642, 1037, 749, 1075, 729, 604, 650 , 601, 635, 1052, 868, 1063, 860)

plsn_fct <- bind_cols(
  pls2,
  "etch" = etch
)
```

#### lm and anova

```{r}
plsn_lm <- lm(
  formula = etch ~ A * B * C, 
  data = plsn
  # contrasts = list(A = "contr.sum", B = "contr.sum", C = "contr.sum")
  )
summary(plsn_lm)
```

```{r}
plsn_aov <- aov(plsn_lm)
summary(plsn_aov)
```

The main effects of Gap and Power are highly significant (both have very small P-values). The AC interaction is also highly significant; thus, there is a strong interaction between Gap and Power.

#### R^2 and Adjusted R^2

The ordinary R^2 is 0.9661 and it measures the proportion of total variability explained by the model. A potential problem with this statistic is that it always increases as factors are added to the model, even if these factors are not significant. The adjusted R^2 is obtained by dividing the Sums of Squares by the degrees of freedom, and is adjusted for the size of the model, that is the number of factors.

```{r}
plsn_reduced_lm <- lm(
  formula = etch ~ A + C + A:C, 
  data = plsn
  )
```

[]{#glance}

Besides the base summary() function, R squared and adjusted R squared can also be easily retrieved with the glance function from the {broom} package. We're extracting them here for the complete and for reduced model:

```{r}
library(broom)
```

```{r}
glance(plsn_lm)[1:2] %>%
  bind_rows(glance(plsn_reduced_lm)[1:2], 
            .id = "model")
```

Adjusted R² has improved. Removing the nonsignificant terms from the full model has produced a final model that is likely to function more effectively as a predictor of new data.

#### Coding natural values

Now that we have model often we will want to predict the response at a certainly specific level between the coded factor levels of $\pm$ 1.

To do that we need to convert that specific the natural value into a coded value. Lets calculate the coded value for the factor A (gap) of which the natural value is nA = 0.9, between the natural levels of nA = 0.8 and nA = 1.2. We choose to do this for a fixed level of C of 1, corresponding to its maximum of 325W.

```{r}
natural2coded <- function(xA, lA, hA) {(xA - (lA + hA) / 2) / ((hA -  lA) / 2)}
```

```{r}
# Converting natural value xA into coded value cA:
lA <- 0.8
hA <- 1.2
xA <- 0.9

cA <- natural2coded(xA, lA, hA)
cA
```


To be noted that the opposite conversion looks like:

```{r}
coded2natural <- function(cA, lA, hA) {cA * ((hA - lA) / 2) + ((lA + hA)/2)} 
```

```{r}
# Converting back the coded value cA into its natural value xA
lA <- 0.8
hA <- 1.2
cA <- -0.5

nA <- coded2natural(cA, lA, hA)
nA
```

#### Coded values prediction

And now we can feed our linear model and make predictions:

```{r}
plsn_lm <- lm(
  formula = etch ~ A * C, 
  data = plsn,
  # contrasts = list(A = "contr.sum", B = "contr.sum", C = "contr.sum")
  )
summary(plsn_lm)
```

```{r}
plsn_new <- tibble(A = cA, C = 1)
pA <- predict(plsn_lm, plsn_new)
pA
```

We can visualize this outcome as follows:

```{r}
plsn %>%
  filter(C == 1) %>%
  ggplot() +
  geom_point(aes(x = A, y = etch, color = as_factor(C))) +
  geom_smooth(aes(x = A, y = etch), method = "lm") +
  geom_point(aes(x = cA, y = pA)) +
  scale_y_continuous(n.breaks = 10) + 
  scale_color_discrete(guide = FALSE) +
  theme_industRial() +
  theme(plot.title = ggtext::element_markdown()) +
  labs(
    title = "3^k factorial design",
    subtitle = "Prediction with reduced model")
```

#### Response surface plot 

We are introducing here response surface plots which is yet another way to visualize the experiment outputs as a function of the inputs. We're doing this with the persp() function from base R which provides an extremely fast rendering, easy parametrization and a readable output. 

[]{#persp}

```{r}
ngrid <- 20
Agrid <- Bgrid <- seq(from = -1, to = 1, length = ngrid)
yield <- predict(plsn_lm, expand.grid(A = Agrid, C = Bgrid))
yield <- matrix(yield, length(Agrid), length(Bgrid))

persp(
  x = Agrid, 
  y = Bgrid, 
  z = yield, 
  theta = -40, phi = 20, r = 10,
  ticktype = "d", xlab = "Gap", ylab = "Power",
  main = "Plasma etching experiment"
)
```

Due to the interaction between factors A and C the surface is slightly bent. This is exactly what we observe in the interactions plots of which the one below corresponds to slicing the surface at the min and the max of Power:

```{r}
interaction.plot(x.factor = plsn$A, 
                 trace.factor = plsn$C,
                 fun = mean,
                 response = plsn$etch,
                 trace.label = "Power",
                 legend = TRUE,
                 xlab = "Gap",
                 ylab = "Yield",
                 main = "Plasma etching experiment")
```

Just like in the surface plot we can see here in the interaction plot that the response of yield on gap is different depending on the level of power. When power is high it decreases and when power is low it increases. As a reminder this is what is called an interaction between these two factors.

### Single replicate designs

m factors 2 levels 

Possible approaches:
- graphical methods–normal and half-normal probability plots; no formal tests;
- assume some high-order interactions are zero, and fit a model that excludes them; degrees of freedom go into error, so testing is possible (not recommended)

The Filtration example

```{r}
flt <- read.csv("~/Documents/data_science/industRial/data-raw//6-4_filtration.csv")
flt_nf <- flt %>%
  mutate(across(-filtration, as_factor))
```

#### lm

```{r}
flt_lm <- lm(
  formula = filtration ~ A * B * C * D, 
  data = flt)
summary(flt_lm)
```

We can see that being a single replicate design no statistics have been calculated for the effects in the model. A recommended approach in this case is to look into the normal probability plot of the model effects. 

#### qqPlot 

Here we are going to prepare this plot with the function qqPlot() from the {car} package:

[]{#qqPlot}

```{r}
flt_eff <- flt_lm$coefficients[2:16]
flt_eff_names <- names((flt_lm$coefficients)[2:16])
main_effects_plot <- qqPlot(
  flt_eff, envelope = 0.70, 
  id = list(
    method="y", n=5, cex=1, col=carPalette()[1], location="lr"), 
    grid = FALSE,
  col = "black",
  col.lines = "black",
  main = "Chemical vessel - Normal plot of effects"
  )
```

In plot we can see that the effects that have the highest influence on the output are the effects A, C and D and their interactions. We can still confirm these observations with a calculation of the percentage contribution of each effect as follows:

```{r}
flt_lm_tidy <- flt_lm %>%
  tidy() %>%
  filter(term != "(Intercept)") %>%
  mutate(
    effect_estimate = - 2 * estimate,
    effect_estimate_sum = sum(effect_estimate), 
    effect_contribution_perc = abs((effect_estimate/effect_estimate_sum)*100) %>%
      round(2)
  )
main_effects_table <- flt_lm_tidy %>%
  select(term, effect_estimate, effect_contribution_perc) %>%
  arrange(desc(effect_contribution_perc)) %>%
  head(8) %>%
  kable()
```

#### Reduced model

Following the previous analysis we are removing the factor B from the model and keeping only the 2nd order interactions assuming the system also respects the sparcity of effects principle.

```{r}
flt_red_lm <- lm(
  formula = filtration ~ A + C + D + A:C + A:D, 
  data = flt)
summary(flt_red_lm)
```

We can now see that we've regained degrees of freedom and obtained a sort of hidden replication allowing to calculate statistics and error terms on the model.

#### Residuals analysis

Checking the residuals we see the significant effect of the remaining interactions. The residuals are not completely normal but the in the standardized residuals the deviations are contained within 1.2 sd.

```{r}
par(mfrow = c(2,2))
plot(flt_red_lm)
```

We can now establish the main effects and interaction plots and conclude on the optimal settings to maximize the output: A and D should be on the max and C on the min.


<!--chapter:end:9_twolevelDOE.Rmd-->

