[
["index.html", "industRial data science Case studies in product development and manufacturing Welcome", " industRial data science Case studies in product development and manufacturing João Ramalho 2021-05-31 Welcome This is the online version of industRial data science, a book with tools and techniques for data analysis in Product Development and Manufacturing. It is organized around Case Studies in a “cookbook” approach, making it easier to directly adopt the tools. The examples come from varied manufacturing industries, mostly where repetitive production in massive quantities is involved, including: pharmaceuticals, food, electronics, watch making and automotive. Product Development and Manufacturing are very important activities in society because bringing innovative products to the market has an immense potential to improve the quality of life of everyone. Additionally Data Science brings new powerful approaches to the engineering and manufacturing of consumer goods, helping minimizing environmental impact, improving quality and keeping costs under control. How to use this book We assume the reader is familiar with product development and manufacturing quality methodologies such as dmaic and six sigma and the related statistical concepts. We also assume the reader is already a user of the R programming language. The Case Studies then bring all the areas together in a practical way. This book is better used as a reference book by using the navigation bar on the left to go a specific industrial domain. To reproduce the examples all the case studies data sets, example functions and the textbook original files can be downloaded as a package called {industRial}. For guidelines on how to use them refer to the sections Datasets and Functions Complementing the text, a series of tutorials can be accessed either online or locally to practice dynamically key statistical concepts. For the online option no specific software installation is required. A list of web links and detailed instructions on local installation can be seen in the section Tutorials. In the appendix we provide a detailed Index and a Glossary and we refer to several good quality books on both Data Science and Product Development that have served to provide the required theoretical background. These cover key disciplines such as six sigma, statistics and computer programming. This book aims complementing them and showcase how to benefit from recent software in this area. A full list can be found in References. Content overview The case studies are organized according the a logical product development flow. The text starts with case studies in the domain of Design for Six Sigma. These are some practical tools in that help prioritizing problems and get focus on how to tackle them. The next group of case studies covers the domain of Measurement System Analysis, an initial important step when developing a product or manufacturing process. Here is discussed how to analyze the response of a measurement device in terms of its bias and its uncertainty. The next large group of case studies is the Design of Experiments. This corresponds to the core of the R&amp;D activities and provides approaches to minimize the quantity of trials and time to reach to a sufficient knowledge of how the product or system works. Also on how to obtain the right balance on its features and properties to get the desired output. A final group of case studies presents ways to get the manufacturing process in control according to what was defined in the product development phase. These are the well known Statistical Process Control and Capability studies. Acknowledgements I would like to express my gratitude to the instructors and colleagues who have spent time sharing their knowledge, answering my questions and giving me inputs: Enrico Chavez, Iegor Rudnytskyi, Giulia Ruggeri, Harry Handerson and Bobby Stuijfzand from the EPFL ADSCV team; Jean-Vincent Le Bé, Jasmine Petry, Yvan Bouza, James Clulow and Akos Spiegel from the Nestlé STC team; Frank Paris from DOQS; Théophile Emmanouilidis and Sélim Ach from Thoth. To report any issue or make suggestions please open an issue on the book repository: industRialds/issues About the authors João Ramalho is a Senior Industrial Data Scientist with more than 20 years of experience in the manufacturing industry. He’s been in varied positions in R&amp;D, Operations and IT at Philip Morris, Rolex and Nestlé. He holds a Master in Mechanical Engineering from the IST of Lisbon, a PMP certification from the Project Management Institute and a Data Science certification from DataCamp. He’s currently specializing in Data Visualization at the Swiss technical university EPFL. See full profile at j-ramalho.github.io ## Error in get(genname, envir = envir) : objet &#39;testthat_print&#39; introuvable "],
["toolbox.html", "Toolbox R and RStudio Packages Datasets Functions", " Toolbox R and RStudio Many tools exist to do Data Analysis and Statistics with different degrees of power and difficulty such as: Spreadsheets: Excel, Libreoffice, Numbers Proprietary software: Minitab, Mathlab Programming languages: Visual Basic, R, Python, Julia Databases: sqlite, postgre, mysql, mongodb Choosing the right set of tools for Data Science is often not a very scientific task. Mostly is a matter of what is available and what our colleagues, customers or suppliers use. As with everything it is important to remain open to evaluate new tools and approaches and even to be able to combine them. In this book we’ve chosen to provide all examples in R which is a free software environment for statistical computing and graphics. Besides taste and personal preference R brings a significant number of specific advantages in the field of Industrial Data Science: R allows for reproducible research This because the algorithms and functions defined to make the calculations can be inspected and all results can be fully reproduced and audited. This is known as reproducible research and is a critical aspect in all areas where a proof is needed such as in equipment validation and product quality reporting. R functions and tools can be audited and improved Being an open source language, all R libraries and packages added to the basic environment can be audited, adapted and improved. This is very important because when we enter into details every industry has a slight different way of doing things, different naming conventions, different coeficients and so on. R is extensible R is compatible with most other software on the market and is an excellent “glue” tool allowing for example for data loading from excel files, producing reports in pdf and even building complete dashboards in the form of web pages. Large documentation is available on installing and learning R, starting with the official sites R-project and RStudio. Packages All industry specific tools applied throughout this book are available in the form of packages of the programming language R. As with all open source code, they’re all available for download with freedom to modification and at no cost. The amount of packages available is extremely large and growing very fast. When selecting new packages it is recommended to check the latest package update. Packages that have had no improvements since more than a couple of years should be questioned. The field evolves rapidly and compatibility and other issues can become painful. Two ways of obtaining statistics on package history are metacran and RStudio package manager. Additionally an original package named {industRial} has been developed as a companion package for this book. Installation The companion package to this book can downloaded from github with the following command: devtools::install_github(&quot;J-Ramalho/industRial&quot;) The list below identifies which are the remaining packages required to run all the coded of the book examples. Note that it is not required to download them all at once. The column Download precises if a package is downloaded automatically when the {industRial} package is downloaded (imports) or if it needs be downloaded manually by the reader (suggests). In technical terms this differentiation corresponds to the DESCRIPTION file and allows for a progressive installation of the required software. Download Package Domain Version Depends imports viridis colors 0.5.1 R (&gt;= 2.10), viridisLite (&gt;= 0.3.0) imports readr data loading 1.3.1 R (&gt;= 3.1) imports dplyr data wrangling 1.0.2 R (&gt;= 3.2.0) imports forcats data wrangling 0.5.0 R (&gt;= 3.2) imports janitor data wrangling 2.0.1 R (&gt;= 3.1.2) imports magrittr data wrangling 1.5 NA imports tibble data wrangling 3.0.3 R (&gt;= 3.1.0) imports tidyr data wrangling 1.1.1 R (&gt;= 3.1) imports purrr data wrangling 0.3.4 R (&gt;= 3.2) imports glue data wrangling 1.4.1 R (&gt;= 3.1) imports stringr data wrangling 1.4.0 R (&gt;= 3.1) imports patchwork plotting 1.0.1 NA imports ggplot2 plotting 3.3.2 R (&gt;= 3.2) imports scales plotting 1.1.1 R (&gt;= 3.2) imports ggtext plotting 0.1.0 R (&gt;= 3.5) imports broom statistics 0.7.0 R (&gt;= 3.1) imports stats statistics 4.0.2 NA imports skimr statistics 2.1.3 R (&gt;= 3.1.2) suggests qicharts2 industrial stats 0.7.1 R (&gt;= 3.0.0) suggests qcc industrial stats 2.7 R (&gt;= 3.0) suggests SixSigma industrial stats 0.9-52 R (&gt;= 2.14.0) suggests DoE.base industrial stats 1.1-6 R (&gt;= 2.10), grid, conf.design suggests rsm industrial stats 2.10.2 NA suggests agricolae industrial stats 1.3-3 R (&gt;= 2.10) suggests ggraph networks 2.0.3 R (&gt;= 2.10), ggplot2 (&gt;= 3.0.0) suggests tidygraph networks 1.2.0 NA suggests igraph networks 1.2.5 methods suggests ggforce networks 0.3.2 ggplot2 (&gt;= 3.0.0), R (&gt;= 3.3.0) suggests bookdown publishing 0.20 NA suggests knitr publishing 1.29 R (&gt;= 3.2.3) suggests kableextra publishing NA NA suggests gt publishing 0.2.2 R (&gt;= 3.2.0) suggests car statistics 3.0-9 R (&gt;= 3.5.0), carData (&gt;= 3.0-0) suggests RcmdrMisc statistics 2.7-1 R (&gt;= 3.5.0), utils, car (&gt;= 3.0-0), sandwich In the book text we don’t see the loading instructions for the installed packages over and over again everytime an example is given to avoid repetition (e.g. running library(dplyr) before each code chunk). Be sure to load at minimum the packages below before trying any example: ds_pkgs &lt;- c(&quot;tidyverse&quot;, &quot;scales&quot;, &quot;janitor&quot;, &quot;knitr&quot;, &quot;stats&quot;, &quot;industRial&quot;, &quot;viridis&quot;, &quot;broom&quot;, &quot;patchwork&quot;) purrr::map(ds_pkgs, library, character.only = TRUE) Beware of the common issue of function masking. This happens more often in R when compared to python. As we tend to load all the sets of functions from each package we end up with conflicting function names. In the scope of this text it is mostly the function filter() from {dplyr} which conflicts with the function with the same name from {stats}. We tackle this with the simple technique of adding filter &lt;- dplyr::filter in the beginning of our script to precise which function we want to give priority and we pre-append the package name to all calls of the other function such as stats::filter. For more sophisticated ways to handle this issue we suggest the package {import}. Highlights We’re highlighting now some specific packages that are used in the book and that bring powerful features in analysis of data from R&amp;D and Operations. Wherever they are required in the book we loaded them explicitly in the text to help tracking where the specific functions come from. six sigma {SixSigma} is a complete and robust R package. It provides many well tested functions in the area of quality and process improvement. We’re presenting a full example with the gage r&amp;R function in our MSA Case Study. As many other industrial packages, the {SixSigma} package is from before the {tidyverse} era and its plots have been not been developed with {ggplot2}. This makes integration in newer approaches harder. The data output is still nevertheless fully exploitable and very useful. The package is part of an excellent book with the same name published by Emilio L. Cano (2012). qcc The Quality Control Charts package, {qcc} is another very complete and solid package. It offers a very large range of statistical process control charts and capability analysis. Several examples of the control charts are available in its vignette: qcc vignette that we develop further in our SPC Case Studies. qicharts2 We recommend {qichart2} specifically for the good pareto plots. The package also provides statistical process control charts which are based on {ggplot2} and can serve as an easier alternative to the {qcc} package. As many niche packages we need to be aware that the number of contributors is small meaning that it cannot be as thoroughly tested as community packages. DoE.base This package is one of the most complete and vast packages in Design of Experiments. {DoE.base} is a first of a large suite of packages on the topic, it has vast functionality and is very well documented. We do some exploration of the automatic generation of designs in the DOE case studies. Full documentation available under: DoE.base agricolae Agricolae is a long tested package in the domain of design of experiments. It has been developed for the domain of Agricultural Research but can be used elsewhere. We make a small use, specifically to obtain the function fisher LSD but believe the package has a wealth of functions and methodologies to be explored. rsm The Response Surface Methods {rsm} is the best option in our view to produce 3D plots from linear models. It has specific features to use directly the models removing all the work to produce the data and feed generic 3D plotting functions. The package is larger than this and contains many support functions in the domain of design of experiments. car The {car} package which stands for Companion for Applied Regression is also used in many occasions as it contains many useful functions to assess the performance of linear models and anova. This package is combined with a complete book by Fox and Weisberg (2019). RcmdrMisc This package by the same author of the {car} provides additional miscellaneous functions for statistical analysis. Although it is part of a point and click interface for R we value it for its functions and plots in the domain of linear regression. broom The mission of {broom} is to Convert statistical objects into tidy tibbles. This is quite usefull when we want to reuse the output of the statistical analysis such as the R-squared in data pipelines with {tidyverse} packages. It becomes specially handy to obtain printing quality outputs in {Rmarkdown} documents with tables rendered with {kable} and in {shiny} apps. Several examples are present throughtout our book and mostly in the tutorials. skimr This package comes from ropensci a strong and open community supported by large global organizations such as the NASA. {skimr} is an interesting and powerful alternative to the base summary() function. Two main features make it a strong candidate for regular utilization: the first is its tight integrated with {tidyverse} and {knitr} making it possible to integrate it in pipelines, filtering and so on and in Rmarkdown chunks with specific printing arguments; the second is its extensive customization capabilities allowing to add and remove indicators, data types and presentation formats and agregration levels. stats Many functions from the packages discussed before are built on the large and extremely well tested {stats} package. This package is installed directly with R and consolidates software code that has been improving and tested for decades. As an example the source code of the lm function has close to 1000 lines. The complete package has more than 400 functions that can be listed with library(help = \"stats\") or ls(\"package:stats\") Datasets All datasets presented throughout the book are fully anonymized. Once the package is correctly installed it can be loaded in the R session as usual with the library() function. devtools::install_github(&quot;J-Ramalho/industRial&quot;) library(industRial) The primary goal of {industRial} is to make easily available all the data sets from all case studies. We can easily look for a data set by typing industRial:: and tab. The complete list can also be obtained with the snippet below: data(package = &quot;industRial&quot;) %&gt;% pluck(&quot;results&quot;) %&gt;% as_tibble() %&gt;% select(Item, Title) %&gt;% kable() Item Title battery_charging Charging time of a lithium-ion battery. dial_control Collection of visual defects on watch dial production. ebike_hardening Cycles to failure of ebikes frames after temperature treatment. ebike_hardening2 Cycles to failure of ebikes frames after temperature treatment. juice_drymatter Dry matter content of different juices obtained with two different measurement devices. perfume_experiment Correlation matrix of the input variables of an experiment design in perfume formulation. pet_delivery Tensile strength values on PET raw material for the clothing industry. pet_doe A factorial design for the improvement of PET film tensile strength. solarcell_fill Yearly outputs and fills factor of solarcells of different types. solarcell_output Yearly outputs of solarcells of different types. syringe_diameter Production measurements of the inner diameter of syringes barrels. tablet_thickness Thickness measurements of pharmaceutical tablets tablet_weight Weight measurements of pharmaceutical tablets Once the package is loaded the data objects become immediately available in memory and can directly be used in the examples presented or for further exploration. Lets confirm this invoking the first data set: dial_control %&gt;% head() %&gt;% kable() Operator Date Defect Location id Jane 2018.01.31 Indent 3h D2354 Jane 2018.02.02 Indent 3h D2355 Jane 2018.02.02 Indent 4h D2356 Peter 2018.02.02 Indent 10h D2357 Jane 2018.02.03 Scratch 3h D2358 Jane 2018.02.03 Indent 3h D2359 The dateset can be used and manipulated like any other dataset created in the session or loaded otherwise. For example it can be filtered and assigned to a new variable name: dial_peter &lt;- dial_control %&gt;% filter(Operator == &quot;Peter&quot;) dial_peter %&gt;% head(2) %&gt;% kable() Operator Date Defect Location id Peter 2018.02.02 Indent 10h D2357 Peter 2018.02.03 Scratch 4h D2360 Functions Besides the data sets the {industRial} package also contains toy functions to plot Statistical Process Control (SPC) charts. The objective here is to showcase how to build such functions and their scope of application is limited to the book case studies. For complete and robust SPC functions we recommend using the {QCC} package also described below. Additionally the package contains theme functions to print and customize the aesthetics of spc charts and other charts. These themes are built on top of the {ggplot2} by H.Wickham and {cowplot} package by Claus O.Wilke. The main objective is to give the reader a starting point for customization of charts in this domain. A functions can conveniently be accessed on the console with industRial:: and then tab. The complete list of themes and functions can be seen with: lsf.str(&quot;package:industRial&quot;) %&gt;% unclass() %&gt;% as_tibble() %&gt;% kable() value chart_Cpk chart_I chart_IMR off_spec process_Cpk process_stats process_stats_table ss.rr.plots theme_industRial theme_qcc For each function a help page is available and can be obtained the same way as any other R data sets, themes and functions with ?&lt;object&gt; (e.g. ?chart_xbar) To go even deeper and get access to all the code, the original book Rmd files are also bundled in the package and can be seen in the book folder. A way to get the exact folder path is: paste0(.libPaths()[1], &quot;/industRial/book&quot;) [1] &quot;C:/R/R-4.0.2/jr_library/industRial/book&quot; "],
["tutorials.html", "Tutorials On the web Locally", " Tutorials A set of practical exercises on key concepts presented throughout this book is available either on the web or locally, instructions follow. On the web the tutorials in the list below are published on the shinyapp.io server and can be freely accessed with the links below: Topic/Link Content Pareto chart This tutorial builds on the The dial polishing workshop case study from the Design for Six Sigma chapter, train building pareto charts using the {qichart2} package and explore how playing with different variables gives new insights into apparently simple data collections. DOE Anova This tutorial explores how the p value is calculated by playing with a dynamic anova chart. This exercise is based on the The e-bike frame hardening process of the DOE Interactions chapter. Response Surface This tutorial tests 3D visualization skills by playing with 3D response surface plots and the related interaction plots using the battery_charging dataset and the {rsm} package. Process Capability In this tutorial we can play with the process centering variability and see how this is translated in the process indicators “percentage out of spec” and Cpk. Locally The same set of tutorials can also be run locally which can be convenient as they load faster. This also allows for further exploration as the original tutorial code becomes available. For downloading instructions refer to the packages (#installation) session. Next load the packages: library(industRial) library(learnr) and list the tutorials with: learnr::available_tutorials(package = &quot;industRial&quot;) Available tutorials: * industRial - anova : &quot;industRial practice&quot; - capability : &quot;industRial practice&quot; - pareto : &quot;industRial practice&quot; - surface : &quot;industRial practice&quot; choose a tutorial and run it as follows: learnr::run_tutorial(package = &quot;industRial&quot;, &quot;anova&quot;) The original files are available in the package tutorials folder. Their names correspond to the tutorial names listed before so there is a simple way to open the desired file, e.g.: rstudioapi::navigateToFile( paste0(.libPaths()[1], &quot;/industRial/tutorials/anova/anova.Rmd&quot;) ) ## Error in get(genname, envir = envir) : objet &#39;testthat_print&#39; introuvable "],
["design-for-six-sigma.html", "Design for Six Sigma Pareto Ishikawa Correlation Clustering", " Design for Six Sigma Quality tools have been grouped under varied names and methodologies being Six Sigma one of the most well known and comprehensives ones. The domain is vast as seen in the many tools collected and described in the Six Sigma book by Roderik A.Munro and J.Zrymiak (2015). For this section we’ve selected a few cases that strongly support Measurement System Analysis, Design of Experiments and Statistical Process Control. Beside supporting the remaining sections, they also pretend to showcase how R can be used also for other purposes than data wrangling and visualization in the domain of industrial data science, typically to obtain easily reproducible diagrams. We start with a case on a dial workshop in the watch making industrial where the pareto chart comes handy. We then move to a dental prosthesis laboratory to see how a simple fishbone diagram can help pinpoint special causes of the measurement variation of an optical device and we finish we two different approaches on how to optimize experiment execution by assess the correlation between the outputs in order to minimize the parameters to measure. Pareto Case study: dial polishing workshop Watch dials are received from the stamping process and polished before being sent to the final assembly. As part of the autonomous quality control performed by the polishing operators a count of the defects observed on the dials each day is logged in a spreadsheet. The pareto chart has always proven an effective way of defining priorities and keeping workload under control. It is known for helping focusing on the few important elements that account for most problems. It builds on the well known insight that a few reasons explain or allow to control most of the outcome. This applies particularly well in the technological and industrial context. Often in workshop setups the priorities of day are set by the informal discussions between team members. It is important to be sensitive to the last problem observed or the latest request from management but it is also important to look at data, particularly over a period of time. When looking at simple things as counts and frequencies we sometimes get surprised of how different our perception is from the reality shown by the data collected. Collecting data can be done in many different forms and there’s no right or wrong. It can be noted on a board, log book, spreadsheet or in a dedicated software. In a dial polishing workshop of a watchmaking manufacture, the assembly operators have been collecting dial defects in a spreadsheet. Logging a defect doesn’t mean the dial is directly scrapped. Putting away parts has strong impact on the cost of the operation and has to be done on clear criteria. Sometimes the parts can be rework with minor effort. The datalog corresponds to the status of the dials as they arrive from the stamping and before entering the polishing operation. Their dataset with the name dial_control shows each dial unique number and the general and defect information noted by the operators. Collecting defects All datasets are available by loading the book companion package with library(industRial). Full instructions in the datasets session. head(dial_control) %&gt;% kable(align = &quot;c&quot;, caption = &quot;dial control data&quot;, booktabs = T) Table 1: dial control data Operator Date Defect Location id Jane 2018.01.31 Indent 3h D2354 Jane 2018.02.02 Indent 3h D2355 Jane 2018.02.02 Indent 4h D2356 Peter 2018.02.02 Indent 10h D2357 Jane 2018.02.03 Scratch 3h D2358 Jane 2018.02.03 Indent 3h D2359 We can see that the count includes both the deffect type and the location (the hour in the dial) and that it is traced to the day and operator. The team leader promotes a culture of fact based assessment of the quality measurements. Every week the team looks back and observes the weekly counts. When the quantity of data get bigger trends to start becoming apparent. The team can discuss potential actions and prepare reporting to the supplier of the parts (the stamping workshop). It also helps calibrating between operators and agreeing on acceptance criteria and what is and what is not a defect. Recently there have been lots of talk about scratched dials and there’s a big focus on how to get rid of them. For their weekly review Christophe has prepared a pareto chart in R. Pareto chart See {qicharts2} for more details on this R package library(qicharts2) d_type &lt;- dial_control %&gt;% pull(Defect) %&gt;% as.character() d_type_p &lt;- paretochart(d_type, title = &quot;Watch dial Defects&quot;, subtitle = &quot;Pareto chart&quot;, ylab = &quot;Percentage of deffects&quot;, xlab = &quot;Deffect type&quot;, caption = &quot;Source: dial polishing workshop&quot;) d_type_p + theme_industRial() As often happens we can see that the first two defects account for more than 80% of the problems. Scratching levels are in fact high but they realize indentation is even higher. Is it clear what indentation is? Have we been noting sometimes indentation for scratches? Where to draw the line? and are the causes of these two defects the same? The decides to go deeper in the analysis and Peter says that a potential cause is the fixing tool that holds the dial on the right. To check Peter’s hypothesis Jane prepares another plot by location for the next week review. d_location &lt;- dial_control %&gt;% pull(Location) %&gt;% as.character() d_location_p &lt;- paretochart(d_location, title = &quot;Watch dial deffects&quot;, subtitle = &quot;Pareto chart&quot;, ylab = &quot;Percentage of deffects&quot;, xlab = &quot;Deffect location (hour)&quot;, caption = &quot;Source: Dial workshop&quot;) d_location_p + theme_industRial() Effectively there are many defects at 3h corresponding to the position on the right of the dial (and even more at 4h). Peter’s assumption may be right, the team decides to gather in the first polishing workbench and share openly how each of them fixes the dial to try to understand if there is a specific procedure or applied force that creates the defect. This example shows how data collecting can be simple and effective. if no one in the team is using R yet, a simple pareto chart could be done more simply with a spreadsheet. What R brings is the possibility to quickly scale up: handling very large and constantly changing files for example and also the possibility to directly and simply produce pdf reports or dynamic web applications to collect and visualize the data. To practice and go further in the exploration of pareto charts checkout the tutorials section. Ishikawa Case study: dental prosthesis laboratory An optical measurement device has just been installed in a large Dental Prosthesis Manufacturing Laboratory. It is precise but expensive device based on laser technology which has been installed in a dedicated stabilized workbench. Usually called Fishbone or Ishikawa diagrams this simple tool has proven to be extremely practical and helpful in structuring team discussions. With it we can easily identify and list the expected influencing factors in various contexts such as the preparation of an experiment design. Selection and grouping input parameters can be useful in defining for example the right mix of ingredients in a new product, in selecting manufacturing parameters in an industrial production line or in the definition of a draft operating procedure for a measurement device. In each of these situations it helps seeing the big picture and not fall into the trap of relying only in the data and findings obtained by statistical analysis. In this case study we’re exploring the creation of Ishikawa diagrams with the {qcc} package. Emilio L. Cano (2012) recommends the utilization of R even for such simple diagrams with clear arguments on reproducibility and ease of update. If R and programming is already part of the working culture and there’s someone in the team this makes perfect sense. The lab manager of a dental prosthesis laboratory has acquired a optical device for the precise measurement of the dental impressions that serve as models for the production of the crowns and bridges. The lab has been having complains and several parts have been returned from the dentists and had to be partially or totally reworked. Besides the potential troubles to patients and the already incurred financial losses there is a reputation loss of which the lab manager is very concerned with. Regardless of all this the acquisition decision has taken more than a year. After installation and in spite all the precautions it has been reported and now demonstrated with some specific trials that the measurements have a high variation which is preventing putting it in operation. Until now the laboratory team has always had full confidence in the equipment supplier and the Lab Manager has even seen the same equipment operating in another laboratory from the group. The supplier has been called on site to check the equipment and having seen no reason for the variability proposes to work with the lab team on identifying the potential causes for the high uncertainty in their measurements. They decided to consider a larger scope than just the equipment and take the full measurement method as described in the laboratory operating procedure. They list different reasons related with they’re work and group them. Listing root causes operators &lt;- c(&quot;Supplier&quot;, &quot;Lab Technician&quot;, &quot;Lab Manager&quot;) materials &lt;- c(&quot;Silicon&quot;, &quot;Alginate&quot;, &quot;Polyethers&quot;) machines &lt;- c(&quot;Brightness&quot;, &quot;Fixture&quot;, &quot;Dimensional algorithm&quot;) methods &lt;- c(&quot;Fixture&quot;, &quot;Holding time&quot;, &quot;Resolution&quot;) measurements &lt;- c(&quot;Recording method&quot;, &quot;Rounding&quot;, &quot;Log&quot;) groups &lt;- c(&quot;Operator&quot;, &quot;Material&quot;, &quot;Machine&quot;, &quot;Method&quot;, &quot;Measurement&quot;) effect &lt;- &quot;Too high uncertainty&quot; One of the team members is using R and he has generating all previous reports on the topic with R markdown. He simply adds to the last report a call to the {qcc} package and quickly obtains a simple diagram that allows for a quick visualization of these influencing factors. Fishbone diagram library(qcc) cause.and.effect( title = &quot;Potential causes for optical measurement variation&quot;, cause = list( Operator = operators, Material = materials, Machine = machines, Method = methods, Measurement = measurements ), effect = effect ) The listed factors can now be addressed either one by one or in combined experiments to evaluate their impact on the measurement method. The lab team has decided to assess the method robustness to the brightness and to the dimensional algorithm and will prepare an experiment design with several combinations of these parameters to evaluate them. Using the diagram they can easily keep track of what has been listed, tested and can be eliminated as root cause. Correlation Case study: perfume distillation experiment A Project Manager in perfume formulation needs to understand in detail the impact of the perfume manufacturing line parameters variation (e.g. temperature, pressure and others) in typical perfume sensorial characteristics such as the floral notes. A correlation matrix is a way to discover relationships between groups of items. Such matrix can also be used to select which output measurement should be done in priority in a design of experiments (DOE). In exploratory phases when the experiments are repeated several time with slightly different configurations, secondary outputs that are strongly correlated to main outputs can be eliminated In an industrial setup the cost of experimenting is often very high. With this approach engineers and scientists can keep the test quantities in control and avoiding measurements until final stages of implementation. We explore in this case study two different techniques, one with a tile plot and another more advanced with a network plot. A DOE consists in a series of trials where several inputs are combined together and important outputs are measured (further details can be seen in the DOE chapter). Commonly DOE analysis results linking inputs to outputs are presented with effects plots and interaction plots but before getting it is important to check the correlation between the outputs. Often there groups of outputs move together even if there is no cause and effect relationship between them. We can see this correlation in a tile plot. A team of experts of a manufacturer of fragrances has listed 23 different output variables of interest for an exploratory perfume distillation experiment. Facing such extensive list the Project Manager decided to put the team together a second time to try to set priorities. The approach was to guess the results of the experiment which allowed to go deeper in the technology and to construct an experiment plan in a meaningful way. The experts inputs have been captured in a a two entry table named perfume_experiement. Matrix perfume_experiment[1:6, 1:7]%&gt;% kable( align = &quot;c&quot;, caption = &quot;perfume DoE correlation matrix of the outputs&quot;, booktabs = T ) Table 2: perfume DoE correlation matrix of the outputs yy pw w pm pe f it pw 0 10 3 3 2 2 w 0 0 3 3 2 2 pm 0 0 0 6 6 0 pe 0 0 0 0 6 0 f 0 0 0 0 0 7 it 0 0 0 0 0 0 In the matrix the experiment output variables are named with coded names made of two letters. They represent the production Line Parameters (e.g. t = temperature, o = opening, pw = power) and the Perfume Attributes (f = flower). We can see in the table what the experts have noted the expected correlation strengths in an unusual way from 1 to 10, with 10 being the highest. In order to prepare a visual representation with a tile plot from {ggplot2} the data is transformed to long format. An additional trick is to convert the values at zero to NA so that they get directly transparent on the plot. Values at zero in the dataset are converted to type NA_real_ to obtain a transparent background in the the tileplot. perfume_long &lt;- perfume_experiment %&gt;% pivot_longer( cols = -yy, values_to = &quot;correlation&quot;, names_to = &quot;xx&quot; ) %&gt;% mutate(correlation = if_else( correlation == 0, NA_real_, correlation)) %&gt;% mutate(correlation = as_factor(correlation)) Tileplot perfume_long %&gt;% ggplot(aes(x = xx, y = yy, fill = correlation)) + scale_fill_viridis_d(direction = -1, name = &quot;Correlation\\nStrength&quot;) + geom_tile() + labs( title = &quot;The Perfume destilation experiment&quot;, subtitle = &quot;Output variables correlation plot &quot;, x = &quot;&quot;, y = &quot;&quot;, caption = &quot;Anonymised data&quot; ) + theme_industRial() The plot shows that many parameters are expected to move together. Looking in detail the flow aroma moves together with other sensory attributes such as hp, o and oc. After this first DoE the real correlations will be established and the team expects to be able to avoid a significant part of the measurements that have a correlation higher than 50% from the second DoE onward. Clustering In this second analysis of the perfume distillation experiment we present a more advanced but more powerful approach using network plots. It explores an automatic way to clustering the variables and a specific way to present such clusters. Technically we’re going to build a weighed non directional network(tbl_graph) object. Several steps of conversion are required for this approach first with functions from various packages from the networks domain. library(igraph) library(tidygraph) library(ggraph) The first step consists in converting the “Perfume” tibble to a matrix format: The perfume_experiment is originaly coded as a tibble object. perfume_matrix &lt;- perfume_experiment %&gt;% column_to_rownames(&quot;yy&quot;) %&gt;% as.matrix() Then using the {igraph} package we convert the matrix into a graph object: perfume_graph &lt;- graph_from_adjacency_matrix( perfume_matrix, mode = &quot;undirected&quot;, weighted = TRUE ) to finally convert it into a tibble graph with {tidygraph} package: perfum_tbl_graph &lt;- as_tbl_graph(perfume_graph, add.rownames = &quot;nodes_names&quot;) As mentioned the experts have provided the correlation strength in the unusual scale from 1 to 10 which was easier for them during discussion. Here we’re here converting it back to the 0 to 1 which is more common in the statistics community. For simplicity, negative correlations were not considered just the strength, enabling the network to be unidirectional. perfum_tbl_graph &lt;- perfum_tbl_graph %&gt;% activate(edges) %&gt;% mutate(weight = weight/10) perfum_tbl_graph # A tbl_graph: 22 nodes and 85 edges # # An undirected simple graph with 7 components # # Edge Data: 85 x 3 (active) from to weight &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 2 1 2 1 3 0.3 3 1 4 0.3 4 1 5 0.2 5 1 6 0.2 6 1 8 0.8 # ... with 79 more rows # # Node Data: 22 x 1 name &lt;chr&gt; 1 pw 2 w 3 pm # ... with 19 more rows In the previous chunk output we see a preview of the tibble graph object with the first few nodes and edges. Now we create a vector with various igraph layouts to allow for easier selection when making the plots: igraph_layouts &lt;- c(&#39;star&#39;, &#39;circle&#39;, &#39;gem&#39;, &#39;dh&#39;, &#39;graphopt&#39;, &#39;grid&#39;, &#39;mds&#39;, &#39;randomly&#39;, &#39;fr&#39;, &#39;kk&#39;, &#39;drl&#39;, &#39;lgl&#39;) and do a first network plot to check data upload: perfum_tbl_graph %&gt;% ggraph::ggraph(layout = &quot;igraph&quot;, algorithm = igraph_layouts[7]) + geom_edge_link(aes(edge_alpha = weight)) + geom_node_label(aes(label = name), repel = TRUE) + # theme_graph() + labs(title = &quot;DOE Perfume Formulation - Inputs&quot;, subtitle = &quot;Most important expected correlations&quot;) Data loading is now confirmed to have been done correctly and we can now move into the clustering analysis. We use different clusters algorithms to generate the groups. Clustering algorithms perfum_tbl_graph &lt;- perfum_tbl_graph %&gt;% activate(nodes) %&gt;% mutate(group_components = group_components(), group_edge_betweenness = group_edge_betweenness(), group_fast_greedy = group_fast_greedy(), group_infomap = group_infomap(), group_label_prop = group_label_prop(), group_leading_eigen = group_leading_eigen(), group_louvain = group_louvain(), group_walktrap = group_walktrap() ) There’s extensive research behind of each of these algorithms and detailed information can be obtained starting simply with the R help system. For example for one selected here type ?group_louvain or ?cluster_louvain on the console. Digging deeper it is possible to find the author names and the papers explaining how and when to use them. To produce the final plot some trial and error is needed to select the algorithm that gives the best clustering results. Now for the final step we also need to load some specific support packages for advanced plotting. library(ggforce) library(ggtext) Network plot perfum_tg_2 &lt;- perfum_tbl_graph %&gt;% activate(edges) %&gt;% mutate(weight2 = if_else(weight &gt;= 0.8, 1, if_else(weight &gt;= 0.5, 0.5, 0.1))) my_palette &lt;- c(viridis(12)[3], viridis(12)[9], &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;) set.seed(48) perfum_tg_2 %&gt;% activate(nodes) %&gt;% mutate(group = group_louvain) %&gt;% filter(group %in% c(1,2)) %&gt;% ggraph(layout = &quot;igraph&quot;, algorithm = igraph_layouts[7]) + geom_mark_hull(mapping = aes(x, y, group = as_factor(group), fill = as_factor(group)), concavity = 0.5, expand = unit(4, &#39;mm&#39;), alpha = 0.25, colour = &#39;white&#39;, show.legend = FALSE) + geom_edge_link(aes(edge_alpha = weight2, edge_width = weight2)) + geom_node_point(size = 3) + geom_node_label(aes(label = name), repel = TRUE) + scale_edge_width(range = c(0.2, 1), name = &quot;Correlation strength&quot;) + scale_edge_alpha(range = c(0.05, 0.2), name = &quot;Correlation strength&quot;) + scale_fill_manual(values = my_palette) + # theme_graph() + labs( title = str_c(&quot;&lt;span style=&#39;color:#433E85FF&#39;&gt;Line Parameters&lt;/span&gt;&quot;, &quot; and &quot;, &quot;&lt;span style=&#39;color:#51C56AFF&#39;&gt;Perfume Attributes&lt;/span&gt;&quot;), subtitle = &quot;Clustering the outputs of Perfume Formulation DOE01&quot;, caption = &quot;Clustering by multi-level modularity optimisation (louvain)&quot;) + theme(plot.title = element_markdown(family = &quot;Helvetica&quot;, size = 14, face = &quot;bold&quot;)) We can see that the algorithm is grouping elements that have a strong correlation. Most stronger correlations are mostly presented within elements of each cluster. This is expected as certain perfume sensorial attributes are strongly correlated and the same for certain Line Parameters.e The code presented can now easily be reused once the DOE is executed to compare with the real correlations measured. Once knowledge is built and confirmed on which outputs are strongly correlated a selection of the key parameters can be done. This strongly simplifies the experiments by reducing the number of outputs to measure and reduces the cost and lead time of new formulations. ## Error in get(genname, envir = envir) : objet &#39;testthat_print&#39; introuvable "],
["MSA.html", "Measurement System Analysis Calibration Precision Uncertainty", " Measurement System Analysis Analyzing and validating measurement methods and tools is the base for ensuring the quality of manufacturing products. For most commercial products it is not simply about satisfying consumer expectations but has regulatory and legal implications. Using measurement tools in industrial setups for high volume production goes naturally beyond buying and installing an equipment. It requires clear operating procedures, trained operators and tested devices for the specific range applications and products. There are many different normalizing bodies in the metrology domain with different approaches and terminology. The cases in this section follow a simplified step by step approach aiming at giving an overview of how data treatment can be done with R. The first case treats the calibration of a recently acquired measurement device by comparing it to a reference device. It provides statistical analysis of the bias of the method compared with the reference for the full measurement range. The following case deals with the estimation of the method precision, namely the measurement repeatability and reproducibility under regular utilization conditions. It provides examples on acceptance criteria typical in industrial context. The final case study presents calculation of the method uncertainty, a more comprehensive indicator taking into account the calculations done in the previous cases. Calibration Case study: juice production plant The Quality Assurance Head has acquired a fast dry matter content measurement device from the supplier DRX. The rational for the acquisition has been the important reduction of the control time. Before it enters operation its performance is being assessed and validated. A first step after a measurement equipment acquisition is the assessment of the response over the entire measurement range. In particular it is important to verify its linearity and variability and determine the average bias throughout the measurement range. In a juice production plant the dry matter content for the top seller is around 13% dry matter content. Typical specifications are the Premium fresh apple juice with 12.4 % and the Austrian beetroot juice with 13.2%. Some other specialties may have a higher content up such as the Organic carrot that has 16.3%. After consulting with the Manufacturing Team Leader, the Quality Assurance Head selects checking the equipment in the range of 10 to 20% dry matter content. For the calibration assessment samples are produced at target values set at round numbers (10%, 15% and so on). This data is captured in the juice_drymatter dataset of which we’re checki juice_drymatter %&gt;% head(5) %&gt;% kable( align = &quot;c&quot;, caption = &quot;juice dry matter data&quot; ) Table 3: juice dry matter data product drymatter_TGT speed particle_size part drymatter_DRX drymatter_REF apple 10 20 250 1 9.80 10.05 apple 10 20 250 2 9.82 10.05 apple 10 20 250 3 9.82 10.05 beetroot 10 20 250 1 9.79 10.03 beetroot 10 20 250 2 9.75 10.03 We see in this raw dataset that it contains the same samples dry matter content measured twice. First with the with the new equipment (DRX) and then with the reference equipment (Ref). The reference equipment is considered as such because it has been validated and accepted by the head quarters quality department. The difference between the two devices for each measurement is calculated below and allocated to a new variable with the name bias. juice_drymatter &lt;- juice_drymatter %&gt;% mutate(bias = drymatter_DRX - drymatter_REF, part = as_factor(part)) A first look at the bias with the skim() function from {skimr} gives already an indication that the bias is not constant along the measurement range. See {skimr} for more details on this R package, an alternative to base::summary() library(skimr) skim(juice_drymatter$bias) %&gt;% yank(&quot;numeric&quot;) Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist data 0 1 -0.3 0.14 -0.63 -0.4 -0.29 -0.19 -0.07 ▂▅▆▇▇ Such results are not encouraging because a non regular bias along the range may require specific correction for different product which may be not practical and prone to error. Often this requires to dig into detail to understand the causes of the bias and determine if they are related with the physical phenomena and if there are clear controllable causes. Ultimately this could result is narrowing the measurement range and validating a specific device and method for a specific product specification target. For the Quality Assurance Manager it is too early to draw conclusions and he establishes a more detailed plot with {ggplot2} to better visualize the data. Bias plot juice_drymatter %&gt;% ggplot(aes(x = drymatter_REF, y = bias)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = T, ) + coord_cartesian( xlim = c(9,21), ylim = c(-.75,0), expand = TRUE) + theme_industRial() + labs(title = &quot;Dry matter method validation&quot;, subtitle = &quot;Gage Linearity&quot;, caption = &quot;Dataset: juice_drymatter233A, Operator: S.Jonathan)&quot;) This type of plot is usually called bias plot and provides a view of how the difference between the measurements obtained with the new device and the reference device compare allong the measuremen range. In the plot generated an additional regression line has been introduced with geom_smooth from {ggplot2}. There are several ways to assess the linearity. In this case we’re going to remain at a visual check only leaving to the Design of Experiments case study a more thourough verification. The linear model appears as well adapted in this case. The first check is the observation that regression line passes close to the averages of each level of the dry matter factor. Nevertheless the slope is rather steep showing a clear increase of the bias (in the negative direction) with the increase in dry matter content. Bias report Using well known {dplyr} function the plot is complemented with statistics of the bias for each level of dry matter target: mean, median, standard deviation. A good practice that took some time to adopt but now is well anchored is to always present the sample size which speaks for the relevance of the statistical indicators. juice_drymatter_bias &lt;- juice_drymatter %&gt;% group_by(drymatter_TGT) %&gt;% summarise(bias_mean = mean(bias, na.rm = TRUE), bias_median = median(bias, na.rm = TRUE), bias_sd = sd(bias, na.rm = TRUE), bias_n = n()) juice_drymatter_bias %&gt;% kable(align = &quot;c&quot;, digits = 2) drymatter_TGT bias_mean bias_median bias_sd bias_n 10 -0.17 -0.15 0.07 36 15 -0.29 -0.31 0.10 36 20 -0.44 -0.44 0.10 36 Mean and median bias are very close which indicates that the data is equally distributed around the mean The standard deviation is also very similar from level to level indicating that the measurement variability is not depending on the range of measurement. A decision now needs to be taken on which systematic offset to apply depending on the operational context. As mentioned most commercial products on the production line where the device is used have a target specification around 13% therefore the Quality Assurance Head decides together with Manufacturing Team Leader to put in the operating procedure of the device a unique offset of 0.3 g. This value is assigned to a new variable called juice_cal_u that will be needed later to calculate the measurement uncertainty. u_cal &lt;- as_vector(juice_drymatter_bias[2,4]) names(u_cal) &lt;- NULL # we&#39;re removing the original name to avoid confusion later. Precision Case study: tablet compaction process Modern pharmaceutical tablet presses reach output volumes of up to 1,700,000 tablets per hour. These huge volumes require frequent in-process quality control for the tablet weight, thickness and hardness. Pharmaceutical production setups combine extreme high volumes with stringent quality demands. In this context many manufacturing plants have inline automatic measurement devices providing automatic data collection to a central database but it is not uncommon to see hand held devices and manual log of measurements in spreadsheets. In an age of machine learning and sophisticated predictive tools this may seem awkward but it is common to see coexisting old and new approaches on the shop floor. A recurring check of measurement devices is the famous gage r&amp;R. r&amp;R stands for reproducibility and Reproductibility which combined give the instrument precision, according to the ISO 5725. In any case automatic or manual the way to assess the measuremen device should follow the same approach. In our case we’re looking into a pharmaceutical company where a tablet compaction process the quality measurement system requires the Production Operator to sample tablets on a regular basis and log the thickness in a spreadsheet on the line. Measurements are done with a micrometer build and acquired specifically for this purpose that has a fixture developed to fit the shape of the tablet. Besides thickness, the quality measurement system requires the operator to collect quite an large variety of parameters including room conditions too. Elaborating on this a Quality Engineer has prepared a specific file for the gage r&amp;R that also included the replicate number. As it is common practice he asked the measurements to be done by several operators. This data has been loaded into R and is available in the dataset tablet_thickness and an extract is presented here in raw: tablet_thickness %&gt;% head(3) %&gt;% kable( align = &quot;c&quot;, caption = &quot;tablet thickness gage r&amp;R data&quot; ) Table 4: tablet thickness gage r&amp;R data Position Size Tablet Replicate Day Date [DD.MM.YYYY] Operator Thickness [micron] Temperature [°C] Relative Humidity [%] Luminescence [lux] Position 1 L L001 1 Day 1 18/11/2020 Paulo 1802.519 22.3 32.7 568.6 Position 1 L L001 2 Day 1 18/11/2020 Paulo 1802.783 22.3 32.8 580.4 Position 1 L L001 3 Day 1 18/11/2020 Paulo 1803.989 22.3 32.8 580.5 It is an excellent practice to look at raw data because it gives an immediate perception of general aspects such as the number of variables, their levels and their datatypes. Although this is irreplaceable it is possible to go further and skim() provides an excellent complement and summary. Below we see that the test requested by the Quality Engineer has required 675 measurements on 11 different variables by 3 different operators. We can see room conditions are stable, rather normally distributed and having small standard deviations and we can even see that thickness appears with 3 groups which seems related with the 3 sizes noted in the Size column. skim(tablet_thickness) The initial idea of the Quality Engineer was to establish a separate gage r&amp;R by tablet size. There is sometimes debate if in the study several different specification should be combined or not. In the last quality weekly meeting this was reason for lively discussions with various logical arguments from the Production Leader and the Engineering Manager. They ended up accepting the proposal of a separate gage per size on the logic that it is important to compare the measurement method variability not only with the process variability but also with the specification itself. Data in excel files to have most of the time human readable formats and the files being open they usually end up with long variable names. Unlike the classical read.csv() function from the base R the read_csv() function from {readr} is not converting character variables to factors. This is a good behavior in our view because it allows for better control and awareness of what is happening. In this case the Quality Engineer is acquainted to the {tidyverse} and is now making the conversion specifically on the desired variables size, tablet and operator. He also makes the filtering for the size L for which he will do the first r&amp;R study. tablet_thickness &lt;- tablet_thickness %&gt;% clean_names() %&gt;% mutate(across(c(size, tablet, operator), as_factor)) tablet_L &lt;- tablet_thickness %&gt;% filter(size == &quot;L&quot;) Now that the dataset is clean and ready he moves forward with the ss.rr() function from the {SixSigma} package. Gage r&amp;R library(SixSigma) # dimensions for chunk output when included: fig.dim=c(8, 10) tablet_L_rr &lt;- ss.rr( data = tablet_L, var = thickness_micron, part = tablet, appr = operator, alphaLim = 1, errorTerm = &quot;repeatability&quot;, main = &quot;Micrometer FTR600\\nr&amp;R for tablet thickness&quot;, sub = &quot;Tablet L&quot;, lsl = 1775, usl = 1825 ) The ss.rr function takes the filtered tablet_L dataset and var, part and the arguments appr to precise what is the measurement variable, the part and the operator in this order. Then to be noted that the alphaLim argument is set to 1 in this first assessment. This is to keep all the model terms including non significant one. In future analysis this can be set to 0.05 the usual significance threshold and those non significant terms are omitted. Another detail important to ensure is to select the repeatability as the errorTerm otherwise we get different results than those obtained with base anova and other software aligned with the Automotive Industry Action Group (2010) guidelines such as Minitab. Finally the function also allows to input the limits he also provides in the arguments the current upper and lower limit of the specification, in this case of 1’800 \\(\\mu m\\) +/- 25 \\(\\mu m\\) for tablet L. The output of this function is a list with several elements inside and an automatically generated report. names(tablet_L_rr) [1] &quot;anovaTable&quot; &quot;anovaRed&quot; &quot;varComp&quot; &quot;studyVar&quot; &quot;ncat&quot; We’re now looking more in detail in some of them. Gage acceptance Measurement system acceptance can be done based on varied criteria and is often done in progressive stages. In Research and Development contexts it is common that the measurement method is developed simultaneously with the end product. There are stages where the teams are conceiving the full industrial setup and there may be an overlap between product sub-assembly, assembly machine and measurement device. These different components of the production or assembly line may not reach maturity all at the same time. In such cases the Quality Assurance may give an approval for the measurement device based on tests done on products that cannot yet be commercialized. This means that the final conditions of usage are not fully tested. In other cases the measurement method is complex but time presses and the teams test the quality of the parts by other means such as the failure rates of the assemblies where the parts go. For all these reasons it is important to clarify at all times the assumptions used in the assessment of the measurement method. Variance components A common way to quickly judge if an equipment variability is high is to look at its variance. In our case the Quality Engineer can look at the variance components of the gage r&amp;R study by calling them from the ss.rr list. tablet_L_rr$varComp %&gt;% kable(digits = 1) VarComp %Contrib Total Gage R&amp;R 1.6 14.8 Repeatability 1.6 14.2 Reproducibility 0.1 0.6 operator 0.1 0.6 tablet:operator 0.0 0.0 Part-To-Part 9.5 85.2 Total Variation 11.1 100.0 Looking at the column %Contrib he sees that the total gage R&amp;R is too high when comparing with the established guidelines for gage acceptance: Less than 1%: the measurement system is acceptable Between 1% and 9%: the measurement system is acceptable depending on the application, the cost of the measurement device, cost of repair, or other factors Greater than 9%: the measurement system is not acceptable and should be improved. Another direct information from this assessment is that this variability comes from the repeatability mostly and not from the operator or from the interaction. This is very useful as a clue to start identifying where the variability comes from and how to try to improve it. Although quick and providing a first impression, variance is not a very intuitive statistic as is not expressed in the measurement units. A much more common and speaking approach is to look into the standard deviation and compare it with the process variation but also with the specification itself. Standard deviation components The standard deviation values from the study can be pulled from the list with the same approach as before. tablet_L_rr$studyVar %&gt;% kable(digits = 1) StdDev StudyVar %StudyVar %Tolerance Total Gage R&amp;R 1.3 7.7 38.5 15.4 Repeatability 1.3 7.5 37.6 15.1 Reproducibility 0.3 1.6 7.9 3.1 operator 0.3 1.6 7.9 3.1 tablet:operator 0.0 0.0 0.0 0.0 Part-To-Part 3.1 18.4 92.3 36.9 Total Variation 3.3 20.0 100.0 40.0 The study variation table is has several columns. The StdDev column contains the square root of each individual variance. The StudyVar column has each StdDev multiplied by 6 which corresponds to the max variability for each component. Then each StudyVar is divided by the Total Variation and expressed in percentage in the %StudyVar column. The last column %Tolerance contains the division of the StudyVar by the specification interval (+/- 25 \\(\\mu m\\) in this case) expressed in percentage. The Quality Engineer is now is a position to progress is assessment. According to the guidelines followed in the company the measurement method variation needs to be less than 10% of the process variation to be considered directly accepted. This is expressed here in the column %StudyVar and is 38.46% which is much above this limit. The guidelines state: Less than 10%: the measurement system is acceptable Between 10% and 30%: the measurement system is acceptable depending on the application, the cost of the measurement device, cost of repair, or other factors Greater that 30%: the measurement system is not acceptable and should be improved. As he already knew, the variability is mostly coming from the repeatability. With this approach he can also compare with the product specification tolerance which is 15.37%. The part to part variation corresponds to the bulk of the variability and this is what is expected. Although the Quality Assurance department is not fully validating a measurement method with these figures there seems to be potential to improve the situation. At this moment we can consider that the micrometer allows to sort good parts from bad because the variability is lower than 30% of specification tolerance but it cannot be used to drive production as the variation is higher than 30% of the production process variation. Gage plots Besides the Variance the Standard Deviation components the ss.rr function generates a full report. We’re going to look into at each of the plots by generating them with the custom function ss.rr.plots. The details of the function itself are presented afterwards. tablet_L_rr_plots &lt;- ss.rr.plots( data = tablet_L, var = thickness_micron, part = tablet, appr = operator, alphaLim = 1, errorTerm = &quot;repeatability&quot;, main = &quot;Micrometer FTR600\\nr&amp;R for tablet thickness&quot;, sub = &quot;Tablet L&quot;, lsl = 1775, usl = 1825 ) plot(tablet_L_rr_plots$plot1) This first bar plot presents in a graphical way the gage results in percentage and we can quickly grasp if we’re on target by looking at the pink bars and comparing them with the dashed bars. We can see that the G.R&amp;R is above 30% and thus is not acceptable from a Study Variation criteria. In green we see that compared with the specification we’re above 10% but below 30% so acceptable but requiring improvement. We’re now looking into the measurements themselves: plot(tablet_L_rr_plots$plot6) plot(tablet_L_rr_plots$plot5) The previous two plots show the measurements for each operator. The first is the Ranges plot showing the differences between the min and max measurement for each part and the second plot is the means plot showing the mean thickness for each part by operator. These help showing that there is a consistency between operators and help as a diagnosis tool to identify if there would be strange patterns appearing where one of the operators would be for instance systematically measuring very high values. The next two plots show average values by part with all operators combined: plot(tablet_L_rr_plots$plot2) plot(tablet_L_rr_plots$plot3) We quickly see the measurements tend to be simetrically distributed around their means and that the means between the different operators are very similar. This confirms the low reproducibility what has been seen in the Variance Components. plot(tablet_L_rr_plots$plot4) This final plot is the so called interaction plot and if there were diverting and strongly crossed lines would indicate that different operators measure the parts in different ways. Again here this confirms the low value obtained for the interaction in the Variance Components table. Tablet thickness measurements obtained with a gage r&amp;R study done on a pharmaceutical tablet compaction process. Red dashed lines corresponds to the thickness specification limits. When the gage report was shared with the Production Leader and the Engineering Manager they raised concerns regarding which is how big is our process variability and how much is the process centered. These are valid points as we often go back and forth between measurement validation, product development and process control. Measurement validation makes us look into details on the measured values and question their validity. Taking conscience that the production specification is not adapted, too large or too narrow. Often we realise that production is systematically slightly off centered. Depending on the diagnostic a new gage r&amp;R plan and sampling may need to be prepared and the process or the specification adjusted. Such topics will be further discussed in the Design of Experiments and in the Statistical Process Control subjects. Negative variance We’ve started the gage assessment by setting the errorTerm to 1. This made that factors that were non significant remained visible, in our case this happened with the tablet:operator interaction. Although the ss.rr function is always showing zero for non significant factors it may happen that in reality the calculated value is negative. We refer to page 557 Montgomery (2012) to get guidance on how to adresses this case: note that the P-value for the interaction term […] is very large, take this as evidence that it really is zero and that there is no interaction effect, and then fit a reduced model of the form that does not include the interaction term. This is a relatively easy approach and one that often works nearly as well as more sophisticated methods. This approach is the one implemented in {SixSigma}. When we leave the argument alphaLim empty the non significant terms are be suppressed from the model, the Anova is recalculated and the remaining tables updated accordingly. We can control this behavior by playing with different values. Usually we consider a p value of 0.05 but we recommend to start with higher values such as 0.1 or 0.2 to avoid suppressing too quickly the factor which would result in a transfer of their variability into the repeatability. Below we run again the ss.rr function with a limit at 0.05 and get the entire data and plot output in one single step. In our case adjusting the p value has had a very limited impact in the total gage r&amp;R which has changed only from 38.46% to 38.38%. tablet_L_rr2 &lt;- ss.rr( data = tablet_L, var = thickness_micron, part = tablet, appr = operator, alphaLim = 0.05, errorTerm = &quot;repeatability&quot;, main = &quot;Micrometer FTR600\\nr&amp;R for tablet thickness&quot;, sub = &quot;Tablet L&quot;, lsl = 1775, usl = 1825 ) Complete model (with interaction): Df Sum Sq Mean Sq F value Pr(&gt;F) tablet 4 1707.1 426.8 271.457 &lt;2e-16 operator 2 13.1 6.6 4.177 0.0166 tablet:operator 8 11.2 1.4 0.892 0.5237 Repeatability 210 330.1 1.6 Total 224 2061.6 alpha for removing interaction: 0.05 Reduced model (without interaction): Df Sum Sq Mean Sq F value Pr(&gt;F) tablet 4 1707.1 426.8 272.533 &lt;2e-16 operator 2 13.1 6.6 4.194 0.0163 Repeatability 218 341.4 1.6 Total 224 2061.6 Gage R&amp;R VarComp %Contrib Total Gage R&amp;R 1.63260451 14.73 Repeatability 1.56591940 14.13 Reproducibility 0.06668511 0.60 operator 0.06668511 0.60 Part-To-Part 9.44885739 85.27 Total Variation 11.08146189 100.00 VarComp StdDev StudyVar %StudyVar %Tolerance Total Gage R&amp;R 1.63260451 1.2777341 7.666405 38.38 15.33 Repeatability 1.56591940 1.2513670 7.508202 37.59 15.02 Reproducibility 0.06668511 0.2582346 1.549408 7.76 3.10 operator 0.06668511 0.2582346 1.549408 7.76 3.10 Part-To-Part 9.44885739 3.0738994 18.443396 92.34 36.89 Total Variation 11.08146189 3.3288830 19.973298 100.00 39.95 Number of Distinct Categories = 3 Further developments on the gage r&amp;R in the excellent book from Springer by the {SixSigma} package author Emilio L. Cano (2012). Custom functions The original report generated by the ss.rrfunction has the inconvenient of being generated as a single plot. Another inconvenient is that there is no option in the function to suppress it in case we just want to look at the data output. To present the individual plots presented in this unit the original function code has had to be modified. This possibility to reuse and modify the code from other authors is one of the great benefits of R. This is possible because are is distributed under a license from the Free Software Foundation. Licenses are long are difficult to read but by simply typing RShowDoc(\"GPL-3\") we can already read in the first few lines you can change the software or use pieces of it in new free programs. The {SixSigma} package itself is also under the same license: SixSigmaDescription &lt;- utils::packageDescription(&quot;SixSigma&quot;) SixSigmaDescription$License [1] &quot;GPL (&gt;= 2)&quot; This being all cleared out the ss.rr function code can the be obtained in RStudio by selecting the package environment in the environment pane and looking for the function. A more direct approach is by simply typing ss.rr on the console. The full code is then revealed and can be copied and modified. For the {industRial} package we’ve copied the code in a new function which we called ss.rr.plots that generates as output a list of plots. Each plot can now be plotted individually. Uncertainty In the Pharmaceutical company described in this case study, the final formal Measurement System Analysis reports are issued with a statement on uncertainty. This is a way to combine this various intermediate assessments described before and to communicate the result in a format that can be interpreted by the persons who read measurement results such as Product Development scientists and the R&amp;D management. Different companies adopt more or less sophisticated norms which provide a detailed way of calculating the combined uncertainty that comes from the different assessments performed. In this case study we’re presenting a simple summation in quadrature equivalento to the one described by Bell (2001) page 14: \\[ u=\\sqrt{u_{man.}^2 + u_{cal.}^2 + u_{repeat.}^2+ u_{reprod.}^2} \\] This formula consists in taking the square root of the sum of the squares of the standard deviations obtained in the different tests. The first term is coming from the micrometer manufacturer which mentions in the equipment guide an accuracy of 0.001 mm which corresponds to 1 \\(\\mu\\). We assign this in R to the u_man variable: u_man &lt;- 1 u_man [1] 1 The calibration uncertainty has been established before in the calibration study : u_cal [1] 0.1024416 The repeatability and reproducibility uncertainties correspond to the standard deviations calculated in the r&amp;R study. In our case we can even obtain them directly from the Variance Components table generated by the ss.rr function of the {SixSigma} package that has bee discussed in details the Gage acceptance unit. We are going to name \\(u_{repeat}^2\\) as u_repeat \\(u_{reprod}^2\\) as u_reprod getting: u_repeat &lt;- tablet_L_rr$studyVar[2,1] u_repeat [1] 1.253844 u_reprod &lt;- tablet_L_rr$studyVar[3,1] u_reprod [1] 0.2624063 Now putting it all together in the uncertainty formula we have: u &lt;- sqrt(u_man^2 + u_cal^2 + u_repeat^2 + u_reprod^2) u [1] 1.628335 Finally what is usually reported is the expanded uncertainty corresponding to 2 standard deviations. To be recalled that \\(\\pm\\) 2 std corresponds to 95% of the values when a repetitive measurement is made. In this case we have \\(U = 2*u\\): U &lt;- 2 * u U [1] 3.256671 For a specific measurement of say 1’800 \\(\\mu m\\) we then say: the tablet thickness is 1’800 \\(\\mu m\\) \\(\\pm\\) 3.3 \\(\\mu m\\), at the 95 percent confidence level. Written in short is: 1’800 \\(\\mu m\\) \\(\\pm\\) 3.3 \\(\\mu m\\), at a level of confidence of 95% Knowing that the specification is [1’775; 1’825] \\(\\mu\\)m we have a specification range of 50. The expanded uncertainty corresponds to 13.03 %. This is another way of looking into the ratio between method variation and specification. The {SixSigma} package gave a similar result of 15.37%. To be noted that the calculation in by the package corresponds to 3 standard deviations and does not comprise the supplier calibration. ## Error in get(genname, envir = envir) : objet &#39;testthat_print&#39; introuvable "],
["DOE.html", "Design of Experiments Direct comparisons Linear regression Anova &amp; Ancova General designs Two level designs Single replicate designs", " Design of Experiments Companies manufacturing goods in industrial quantities have a permanent need to improve the features of their products. This is visible in any such industry be it car parts, watches, electronic components for cell phones, chocolates, clothing, medical devices, medicine, … the list could go on forever. As consumers we expect flawless quality at affordable price and we want to remain free to choose another brand if our confidence has been damaged due to a defective product. Adding to this fortunately the last decades have seen an increasing pressure to develop sustainable products that are responsibly sourced, meet stringent regulations and can last for many years and be properly disposable. Another constraint that can be observed in Research and Development is the growing awareness of the public on ethical issues. There is an increasing expectation that trials generate minimal waste and are done in a way respectful of test subjects human and animal. Experiment design provides ways to meet these important requirements by making us think upfront on what is required and how to best approach a test. Integrated in a complete solid design for quality approach it can provide deep insights on the principles of a system and support decision making based on data. A well prepared test plan minimizes trial and error and reduces the number of prototypes, measurements and time required. There are many well tested approaches, the domain is very large and our textbook can only cover a subset of the many types of DoEs used in the industry. For all these cases statistical notions are key to have a minimal preparation of the test and a valid interpretation of the results. Some statistical concepts every engineer, technician or scientist has to understand go around sampling, sample size, probability, correlation and variability. It is important to be clear about the vocabulary and the mathematics that are behind the constantly used statistics such as the mean, median, variance, standard deviation and so on. We provide a glossary and good bibliography that can be both a good starting point or a refresher. In particular the text and the case studies follow what we consider to be the most important book in this the domain, the Design and Analysis of Experiments by Montgomery (2012). Direct comparisons Winter Sports clothing manufacture All winter sports clothing are virtually made with a mix of natural fibers and synthetic polymers. Upgrading to recyclable polymers while keeping performance requires extensive testing of raw material characteristics such as the tensile strength. We start by exploring simple tests that compare results obtained in two samples. These cases happen all the time as everyone needs one moment or another to compare things. It can be the result of a test before and after an improvement, it can be two different materials applied in the same assembly or still different results obtained by different teams at different moments. In this case, a materials engineer working in the winter sports clothing industry is working with a polymer company to develop a textile raw material based on PET for which the mean tensile strength has to be greater than 69.0 MPa. A first delivery of samples arrives, the materials laboratory measures 28 samples and reports that the test result is not meeting the contract specification. The materials engineer is informed and get hold of the raw data, in the lab system she can see the measurement summary: summary(pet_delivery$A) Min. 1st Qu. Median Mean 3rd Qu. Max. 64.48 68.19 68.78 68.71 69.42 72.04 The mean is in fact slightly lower that the specified contract value of 69 and the materials engineer could think to confirm the rejection the batch right away. She decides nevertheless to observe how do the measurements vary. She plots the raw data on an histogram which is a very common plot showing counts for selected intervals. Histogram pet_spec &lt;- 69 pet_mean &lt;- mean(pet_delivery$A) pet_delivery %&gt;% ggplot(aes(x = A)) + geom_histogram(color = viridis(12)[4], fill = &quot;grey90&quot;) + scale_x_continuous(n.breaks = 10) + geom_vline(xintercept = pet_mean, color = &quot;darkblue&quot;, linetype = 2, size = 1) + geom_vline(xintercept = pet_spec, color = &quot;darkred&quot;, linetype = 2, show.legend = TRUE) + labs(title = &quot;PET raw material delivery&quot;, subtitle = &quot;Histogram of resistance measurements&quot;, y = &quot;Count&quot;, x = &quot;Tensile strength [MPa]&quot;, caption = &quot;Specification min in red, Batch mean in blue¨&quot;) She also observes a certain variability in the batch with many samples with measurements below specification getting close to 64 MPa. She remembers that in this case a t-test could help assessing if the mean that was obtained can be really be considered statistically different from the target value. t-test one sample t.test(x = pet_delivery$A, mu = pet_spec) One Sample t-test data: pet_delivery$A t = -1.0754, df = 27, p-value = 0.2917 alternative hypothesis: true mean is not equal to 69 95 percent confidence interval: 68.15668 69.26332 sample estimates: mean of x 68.71 The basic assumption of the test is that the mean and the reference value are identical and the alternative hypothesis is that their different. The confidence interval selected is 95% as it is common practice in the laboratory. The test result tells us that for a population average of 69, the probability of obtaining a sample with a value as extreme as 68.71 is 29% (p = 0.29). This probability value higher than the limit of 5% that she usually uses to reject the null hypothesis. In fact she cannot conclude that the sample comes from a population with a mean different than 69. She’s not sure what to do of this result and decides asking help to a colleague statistician from R&amp;D: has she applied the right? is the specification correctly defined or should it refer to the minimum sample value? Her colleague confirms that to compare means this is a good approach and as the standard deviation of the production is not available it is reasonable to use the standard deviation from the sample. This is an important detail that was not introduced explicitely as an argument in the R function. As we they are still in the initial steps of the new development they agree that it is a good idea to accept the batch. For the next deliveries the statistic recommends to try to improve the tensile strength average and reduce the variability. For the next delivery she also recommends to agree on a minimum sample size of 30 parts and to redo the t.test but for regular production the team should consider implementing a proper AQL protocol. Improving recyclability while keeping current performance is no easy task. Often novel materials are expensive as their commercial volumes are small and suppliers claim a premium on their own R&amp;D efforts. Consumers of clothing are getting more and more sensitive to waste and to recycling but they don’t always choose products with a higher price to compensate. Following the not fully successful experience with the first delivery of recyclable PET our materials engineer considers a new chemical composition that potentially increases the levels of strength. When the second delivery arrives she establishes a simple plot with the raw data to have a first grasp of the expected improvement. pet_delivery_long &lt;- pet_delivery %&gt;% pivot_longer( cols = everything(), names_to = &quot;sample&quot;, values_to = &quot;tensile_strength&quot; ) pet_delivery_long %&gt;% ggplot(aes(x = sample, y = tensile_strength)) + geom_jitter(width = 0.1, size = 0.8) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;PET clothing case study&quot;, subtitle = &quot;Raw data plot&quot;, x = &quot;Sample&quot;, y = &quot;Tensile strength [MPa]&quot;) Choosing geom_jitter() instead of simply geom_point() avoids overlapping of the dots but has to used with caution as sometimes for precise reading can lead to mistakes. Dot plots also lack information sample statistics and a way to better understanding the bond distributions is to go for a box plot. This type of plot is somehow like the histogram seen before but more compact when several groups are required to be plotted. pet_delivery_long %&gt;% ggplot(aes(x = sample, y = tensile_strength, fill = sample)) + geom_boxplot(width = 0.3) + geom_jitter(width = 0.1, size = 0.8) + scale_fill_viridis_d(begin = 0.5, end = 0.9) + scale_y_continuous(n.breaks = 10) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;PET clothing case study&quot;, subtitle = &quot;Box plot&quot;, x = &quot;Sample&quot;, y = &quot;Tensile strength [MPa]&quot;) In this case she has simply added another layer to the previous plot getting both the dots and the boxes. Now she can see the median and the quantiles. The new sample has clearly higher values and she would like to confirm if the new formulation has a significant effect. While before she was comparing the sample mean with the specification, here she wants to compare the means of the two samples. A direct calculation of this difference gives: PET_meandiff &lt;- mean(pet_delivery$A) - mean(pet_delivery$B) PET_meandiff [1] -0.8628571 To use the t.test it is important to have samples obtained independently and randomly, to check the normality of their distributions and the equality of their variances. To do these checks our materials engineer is using the geom_qq() function from the {ggplot} package and gets directly the normality plots for both samples in the same plot: Normality plot pet_delivery_long %&gt;% ggplot(aes(sample = tensile_strength, color = sample)) + geom_qq() + geom_qq_line() + coord_flip() + scale_color_viridis_d(begin = 0.1, end = 0.7) + labs(title = &quot;PET clothing case study&quot;, subtitle = &quot;Q-Q plot&quot;, x = &quot;Residuals&quot;, y = &quot;Tensile strength [MPa]&quot;) We observe that for both formulation the data is adhering to the straight line thus we consider that it follows a normal distribution. We also see that both lines in the qq plot have equivalent slopes indicating that the assumption of variances is a reasonable one. Visual observations are often better supported by tests such as the variance test. F-test var.test(tensile_strength ~ sample, pet_delivery_long) F test to compare two variances data: tensile_strength by sample F = 1.2755, num df = 27, denom df = 27, p-value = 0.5315 alternative hypothesis: true ratio of variances is not equal to 1 95 percent confidence interval: 0.5902643 2.7563454 sample estimates: ratio of variances 1.275528 The var.test() from the {stats} package us a simple and direct way to compare variances. The F-test is accurate only for normally distributed data. Any small deviation from normality can cause the F-test to be inaccurate, even with large samples. However, if the data conform well to the normal distribution, then the F-test is usually more powerful than Levene’s test. The test null hypothesis is that the variances are equal. Since the p value is much greater than 0.05 we cannot reject the null hypotheses meaning that we can consider them equal. Levene test library(car) leveneTest(tensile_strength ~ sample, data = pet_delivery_long) Levene&#39;s Test for Homogeneity of Variance (center = median) Df F value Pr(&gt;F) group 1 0.0118 0.9139 54 We had considered the samples to be normaly distributed but we can be more conservative and use the leveneTest() function from the {car} package. In this case we get a p &gt; 0.05 thus again we see that there is homogeneity of the variances (they do not differ significantly). Further elaborations on the variance can be found under Minitab (2019a). The clothing sports materials engineer has now a view on the samples distribution and homogeity of variances and can apply t.test to compare the sample means. She takes care to specify the var.equal argument as TRUE (by default it is FALSE). t-test two samples t.test( tensile_strength ~ sample, data = pet_delivery_long, var.equal = TRUE ) Two Sample t-test data: tensile_strength by sample t = -2.3956, df = 54, p-value = 0.02009 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -1.5849965 -0.1407177 sample estimates: mean in group A mean in group B 68.71000 69.57286 She sees that p &lt; 0.05 and confirms the means differ significantly. The test output has also provided a confidence interval for the difference between the means at 95% probability and the mean difference calculated directly of -0.8628571 falls inside this interval (to be noted that zero is obviously not included in this interval). Things look promising in the new recyclable PET formulation. ## Error in get(genname, envir = envir) : objet &#39;testthat_print&#39; introuvable Linear regression e-bike frame hardening A way to go beyond the statistical description of samples and direct comparison between different tests it is to establish a model. Models help us simplify the reality and draw general conclusions. The case studies in this unit introduce linear models and their applications. They also serve as the backbone for statistical inference and forecasting. These are two important techniques because they provide mathematical evidence of such general conclusions in a context where the test quantities are strongly limited as for example in lifecycle testing of expensive mechanical parts. Mountain bikes frames are submitted to many different efforts, namely bending, compression and vibration. Obviously no one expects a bike frame to break in regular usage and it is hard to commercialy claim resistance to failure as a big thing. Nevertheless on the long term a manufacturer reputation is made on performance features such as the number of cycles of effort that the frame resists. An e-bike manufacturing company is looking to increase the duration of its frames by improving the e-bike frame hardening process. A test has been run with five groups of 10 bike frames submitted to four different treatment temperature levels. head(ebike_hardening) # A tibble: 4 x 6 temperature g1 g2 g3 g4 g5 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 160 575000 542000 530000 539000 570000 2 180 565000 593000 590000 579000 610000 3 200 600000 651000 610000 637000 629000 4 220 725000 700000 715000 685000 710000 ebike_narrow &lt;- ebike_hardening %&gt;% pivot_longer( cols = starts_with(&quot;g&quot;), names_to = &quot;observation&quot;, values_to = &quot;cycles&quot; ) %&gt;% group_by(temperature) %&gt;% mutate(cycles_mean = mean(cycles)) %&gt;% ungroup() slice_head(.data = ebike_narrow, n = 5) %&gt;% kable(align = &quot;c&quot;, caption = &quot;e-bike hardening experiment data&quot;) Table 5: e-bike hardening experiment data temperature observation cycles cycles_mean 160 g1 575000 551200 160 g2 542000 551200 160 g3 530000 551200 160 g4 539000 551200 160 g5 570000 551200 ggplot(data = ebike_narrow) + geom_point(aes(x = temperature, y = cycles)) + geom_point(aes(x = temperature, y = cycles_mean), color = &quot;red&quot;) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Raw data plot&quot;, x = &quot;Furnace Temperature [°C]&quot;, y = &quot;Cycles to failure [n]&quot;) Linear model We start by establishing the model, ensuring for now that we leave the variable temperature as a numeric vector. ebike_lm &lt;- lm(cycles ~ temperature, data = ebike_narrow) summary(ebike_lm) Call: lm(formula = cycles ~ temperature, data = ebike_narrow) Residuals: Min 1Q Median 3Q Max -43020 -12325 -1210 16710 33060 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 137620.0 41210.8 3.339 0.00365 ** temperature 2527.0 215.4 11.731 7.26e-10 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 21540 on 18 degrees of freedom Multiple R-squared: 0.8843,\tAdjusted R-squared: 0.8779 F-statistic: 137.6 on 1 and 18 DF, p-value: 7.263e-10 With the summary function we can many different outputs such as the coefficients and the R-squared which we will look into more detail now. As usual, we first inspect the data with a first plot. In this case we’re adding a smoothing geometry with the lm method: ggplot(ebike_narrow) + geom_point(aes(x = temperature, y = cycles)) + geom_smooth(aes(x = temperature, y = cycles), method = &quot;lm&quot;) + geom_point(aes(x = temperature, y = cycles_mean), color = &quot;red&quot;) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Raw data plot&quot;, x = &quot;Furnace Temperature [°C]&quot;, y = &quot;Cycles to failure [n]&quot;) Contrasts treatment In our case the experiementer has selected to control the levels of the temperature variable in what is called a fixed effects model, accepting that conclusions in the comparisons of the levels cannot be extended to levels that were not tested. For this we’re now going to convert the variable to a factor and establish again the model and note that it will give the same R squared but naturally different coefficients. ebike_factor &lt;- ebike_narrow %&gt;% mutate(temperature = as_factor(temperature)) ebike_lm_factor &lt;- lm( cycles ~ temperature, data = ebike_factor, contrasts = list(temperature = &quot;contr.treatment&quot;) ) summary(ebike_lm_factor) Call: lm(formula = cycles ~ temperature, data = ebike_factor, contrasts = list(temperature = &quot;contr.treatment&quot;)) Residuals: Min 1Q Median 3Q Max -25400 -13000 2800 13200 25600 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 551200 8170 67.471 &lt; 2e-16 *** temperature180 36200 11553 3.133 0.00642 ** temperature200 74200 11553 6.422 8.44e-06 *** temperature220 155800 11553 13.485 3.73e-10 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 18270 on 16 degrees of freedom Multiple R-squared: 0.9261,\tAdjusted R-squared: 0.9122 F-statistic: 66.8 on 3 and 16 DF, p-value: 2.883e-09 In order to be precise, we’re making explicit in the lm function that the contrasts argument is “contr.treatment,” although this is the default in R. More on contrasts on the Case Study on \\(2^k\\) designs. The current contrasts settings can be seen as follows: getOption(&quot;contrasts&quot;) unordered ordered &quot;contr.treatment&quot; &quot;contr.poly&quot; Predict Following the residuals analysis and the anova our model is validated. A model is usefull for predictions. In a random effects model where conclusions can applied to the all the population we can predict values at any value of the input variables. In that case reusing the model with temperature as a numeric vector we could have a prediction for various temperature values such as: ebike_new &lt;- tibble(temperature = c(170, 160, 200, 210)) predict(ebike_lm, newdata = ebike_new) 1 2 3 4 567210 541940 643020 668290 We can see that the prediction at the tested levels is slightly different from the measured averages at those levels. This is because the linear interpolation line is not passing exactly by the averages. Anyway this is a fixed effects model and we can only take conclusions at the levels at which the input was tested. We can check that the predictions correspond to the averages we’ve calculated for each level: ebike_new &lt;- data.frame(temperature = as_factor(c(&quot;160&quot;, &quot;200&quot;))) predict(ebike_lm_factor, newdata = ebike_new) 1 2 551200 625400 We’re now ready to assess the validity of the model in order to be ready for our main task which is the comparison of the means using an anova. In order to assess the model performance we’re going to look into the residuals. R provides direct ploting functions with the base and stats packages but in this first example we’re going to break down the analysis and further customise the plots. We are also going to make usage of some additional statistical tests to confirm our observations from the plots. In subsequent chapters we’ll have a more selective approach, where plots and tests are made on a needed basis. We start by loading the package broom which will help us retrieving the data from the lm object into a data frame. Now we build and show below an extract of the “augmented” dataframe Model augment library(broom) ebike_aug &lt;- augment(ebike_lm_factor) %&gt;% mutate(index = row_number()) ebike_aug %&gt;% head() %&gt;% kable(align = &quot;c&quot;) cycles temperature .fitted .resid .std.resid .hat .sigma .cooksd index 575000 160 551200 23800 1.4566455 0.2 17571.09 0.1326135 1 542000 160 551200 -9200 -0.5630730 0.2 18678.69 0.0198157 2 530000 160 551200 -21200 -1.2975161 0.2 17846.38 0.1052218 3 539000 160 551200 -12200 -0.7466838 0.2 18534.92 0.0348460 4 570000 160 551200 18800 1.1506275 0.2 18069.13 0.0827465 5 565000 180 587400 -22400 -1.3709604 0.2 17723.81 0.1174708 6 We can see we’ve obtained detailed model parameters such us fitted values and residuals for each DOE run. Timeseries plot For this plot we need to ensure that the order of plotting in the x axis corresponds exactly to the original data collection order. This plot allows us to assess for strange patterns such as a tendency to have runs of positive of negative results which indicates that the independency assumption does not hold. If patterns emerge then there may be correlation in the residuals. ebike_aug %&gt;% ggplot(aes(x = index, y = .resid)) + geom_point() + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + labs( title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Linear model - Residuals timeseries&quot;, y = &quot;Index&quot;, x = &quot;Fitted values&quot; ) Nothing pattern emerges from the current plot and the design presents itself ^well randomised. Autocorrelation test It is always good to keep in mind that all visual observations can be complemented with a statistical test. In this case we’re going to use the durbinWatson test from the car package (Companion to Applied Regression). library(car) durbinWatsonTest(ebike_lm_factor) lag Autocorrelation D-W Statistic p-value 1 -0.5343347 2.960893 0.104 Alternative hypothesis: rho != 0 Although the output shows Autocorrelation of -0.53 we have to consider that the p value is greater than 0.05 thus there is not enough significance to say that there is autocorrelation. Residuals-Fit plot If the model is correct and the assumptions hold, the residuals should be structureless. In particular they should be unrelated to any other variable including the predicted response. ebike_aug %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE, color = &quot;red&quot;) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + labs( title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Linear model - Residuals vs Fitted values&quot;, y = &quot;Residuals&quot;, x = &quot;Fitted values&quot; ) In this plot we see no variance anomalies such as a higher variance for a certain factor level or other types of skweness. Homocedasticity Equality of variances In the e-bike hardening process, the normality assumption is not in question, so we can apply Bartlett’s test to the etch rate data. bartlett.test(cycles ~ temperature, data = ebike_factor) Bartlett test of homogeneity of variances data: cycles by temperature Bartlett&#39;s K-squared = 0.43349, df = 3, p-value = 0.9332 The P-value is P = 0.934, so we cannot reject the null hypothesis. There is no evidence to counter the claim that all five variances are the same. This is the same conclusion reached by analyzing the plot of residuals versus fitted values. Notes: * the var.test function cannot be used here as it applies to the two levels case only * this test is sensitive to the normality assumption, consequently, when the validity of this assumption is doubtful, the Bartlett test should not be used and replace by the modified Levene test for example Normality plot As the sample size is relatively small we’re going to use a qq plot instead of an histogram to assess the normality of the residuals. ebike_aug %&gt;% ggplot(aes(sample = .resid)) + geom_qq() + geom_qq_line() + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + labs( title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Linear model - qq plot&quot;, y = &quot;Residuals&quot;, x = &quot;Fitted values&quot; ) The plot suggests normal distribution. We see that the error distribution is aproximately normal. In the fixed effects model we give more importance to the center of the values and here we consider acceptable that the extremes of the data tend to bend away from the straight line. The verification can be completed by a test. For populations &lt; 50 use the shapiro-wilk normality test. Normality test shapiro.test(ebike_aug$.resid) Shapiro-Wilk normality test data: ebike_aug$.resid W = 0.93752, p-value = 0.2152 p &gt; 0.05 indicates that the residuals do not differ significantly from a normally distributed population. Standard Residuals-Fit plot This specific Standardized residuals graph also help detecting outliers in the residuals (any residual &gt; 3 standard deviations is a potential outlier). ebike_aug %&gt;% ggplot(aes(x = .fitted, y = .std.resid)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE, color = &quot;red&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Linear model - Standardised Residuals vs Fitted values&quot;, y = &quot;Standardised Residuals&quot;, x = &quot;Fitted values&quot;) The plot shows no outliers to consider in this DOE. Outliers test In a case where we were doubtfull we could go further and make a statistical test to assess if a certain value was an outlier. A usefull test is available in the car package. outlierTest(ebike_lm_factor) No Studentized residuals with Bonferroni p &lt; 0.05 Largest |rstudent|: rstudent unadjusted p-value Bonferroni p 12 1.648813 0.11997 NA In this case, the Bonferroni adjusted p value comes as NA confirming that there is no outlier in the data. Cooks distance ebike_aug %&gt;% ggplot(aes(x = .cooksd, y = .std.resid)) + geom_point() + geom_vline(xintercept = 0.5, color = &quot;red&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Residuals vs Leverage&quot;, y = &quot;Standardised Residuals&quot;, x = &quot;Cooks distance&quot;) Coefficient of determination The R square can be extracted from the linear model that has been used to build the Anova model. summary(ebike_lm_factor)$r.squared [1] 0.9260598 Thus, in the e-bike hardening process, the factor “temperature” explains about 88% percent of the variability in etch rate. Anova fixed effects assumes that: - errors are normally distributed and are independent As the number of residuals is too small we’re not checking the normality via the histogram but rather with a a Q-Q plot. ## Error in get(genname, envir = envir) : objet &#39;testthat_print&#39; introuvable Anova &amp; Ancova We can also compare medians and get a sense of the effect of the treatment levels by looking into the box plot: ggplot(ebike_factor, aes(x = temperature, y = cycles, fill = temperature)) + geom_boxplot() + scale_fill_viridis_d(option = &quot;D&quot;, begin = 0.5) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Raw data plot&quot;, x = &quot;Furnace Temperature [°C]&quot;, y = &quot;Cycles to failure [n]&quot;) 1 factor with severals levels + 1 continuous dependent variable Similar to the t-test but extended - this test allows to compare the means between several levels of treatement for a continuous response variable (the t test is only 2 levels at a time, performing all pair wise t-tests would also not be a solution because its a lot of effort and would increase the type I error) ANOVA principle: the total variability in the data, as measured by the total corrected sum of squares, can be partitioned into a sum of squares of the differences between the treatment averages and the grand average plus a sum of squares of the differences of observations within treatments from the treatment average Aov In R the anova is built by passing the linear model to the anova or aov functions. The output of the anova function is just the anova table as shown here for this first example. The output of the aov function is a list. ebike_aov_factor &lt;- aov(ebike_lm_factor) summary(ebike_aov_factor) Df Sum Sq Mean Sq F value Pr(&gt;F) temperature 3 6.687e+10 2.229e+10 66.8 2.88e-09 *** Residuals 16 5.339e+09 3.337e+08 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that the RF temperature or between-treatment mean square (22,290.18) is many times larger than the within-treatment or error mean square (333.70). This indicates that it is unlikely that the treatment means are equal. Also p &lt; 0.05 thus we can reject the null hypothesis and conclude that the means are significantly different. Anova on plasma etching, modification of the example to achieve a p &gt; 0.05: ebike_narrow2 &lt;- ebike_hardening2 %&gt;% pivot_longer( cols = starts_with(&quot;g&quot;), names_to = &quot;observation&quot;, values_to = &quot;cycles&quot; ) %&gt;% group_by(temperature) %&gt;% mutate(cycles_mean = mean(cycles)) %&gt;% ungroup() ebike_factor2 &lt;- ebike_narrow2 ebike_factor2$temperature &lt;- as.factor(ebike_factor2$temperature) ebike_lm_factor2 &lt;- lm(cycles ~ temperature, data = ebike_factor2) anova(ebike_lm_factor2) Analysis of Variance Table Response: cycles Df Sum Sq Mean Sq F value Pr(&gt;F) temperature 3 1.476e+09 492000000 1.2015 0.341 Residuals 16 6.552e+09 409500000 ggplot(ebike_factor2, aes(x = temperature, y = cycles, fill = temperature)) + geom_boxplot() + scale_y_continuous(n.breaks = 10) + scale_fill_viridis_d(option = &quot;A&quot;, begin = 0.5) + theme(legend.position = &quot;none&quot;) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Boxplot of frame aging resistance&quot;, x = &quot;Furnace Temperature [°C]&quot;, y = &quot;Cycles to failure [n]&quot;) P &gt; 0.05 - there is no significant difference between the means Pairwise comparison The Anova may indicate that the treament means differ but it won’t indicate which ones. In this case we may want to compare pairs of means. ebike_tukey &lt;- TukeyHSD(ebike_aov_factor, ordered = TRUE) head(ebike_tukey$temperature) %&gt;% kable(align = &quot;c&quot;, caption = &quot;tukey test on e-bike frame hardening process&quot;, booktabs = T) Table 6: tukey test on e-bike frame hardening process diff lwr upr p adj 180-160 36200 3145.624 69254.38 0.0294279 200-160 74200 41145.624 107254.38 0.0000455 220-160 155800 122745.624 188854.38 0.0000000 200-180 38000 4945.624 71054.38 0.0215995 220-180 119600 86545.624 152654.38 0.0000001 220-200 81600 48545.624 114654.38 0.0000146 The test provides us a simple direct calculation of the differences between the treatment means and a confidence interval for those. Most importantly it provides us with the p value to help us confirm the significance of the difference and conclude factor level by factor level which differences are significant. Additionally we can obtain the related plot with the confidence intervals plot(ebike_tukey) Least significant difference Fisher’s Least Significant difference is an alternative to Tuckey’s test. library(agricolae) ebike_anova &lt;- anova(ebike_lm_factor) ebike_LSD &lt;- LSD.test(y = ebike_factor$cycles, trt = ebike_factor$temperature, DFerror = ebike_anova$Df[2], MSerror = ebike_anova$`Mean Sq`[2], alpha = 0.05) The Fisher procedure provides us with additional information. A first outcome is the difference between means (of life cycles) that can be considered significant, indicated in the table below by LSD = 24.49. head(ebike_LSD$statistics) %&gt;% kable(align = &quot;c&quot;, caption = &quot;Fisher LSD procedure on e-bike frame hardening: stats&quot;, booktabs = T) Table 7: Fisher LSD procedure on e-bike frame hardening: stats MSerror Df Mean CV t.value LSD 333700000 16 617750 2.957095 2.119905 24492.02 Furthermore it gives us a confidence interval for each treatment level mean: head(ebike_LSD$means) %&gt;% # as_tibble() %&gt;% rename(cycles = `ebike_factor$cycles`) %&gt;% select(-Min, -Max, -Q25, -Q50, -Q75) %&gt;% kable(align = &quot;c&quot;, caption = &quot;Fisher LSD procedure on e-bike frame hardening: means&quot;, booktabs = T) Table 8: Fisher LSD procedure on e-bike frame hardening: means cycles std r LCL UCL 160 551200 20017.49 5 533881.5 568518.5 180 587400 16742.16 5 570081.5 604718.5 200 625400 20525.59 5 608081.5 642718.5 220 707000 15247.95 5 689681.5 724318.5 We can see for example that for temperature 220 °C the etch rate if on average 707.0 with a probability of 95% of being between 689.7 and 724.3 A/min. Another interesting outcome is the grouping of levels for each factor: head(ebike_LSD$groups) %&gt;% kable(align = &quot;c&quot;, caption = &quot;Fisher LSD procedure on e-bike frame hardening: groups&quot;, booktabs = T) Table 9: Fisher LSD procedure on e-bike frame hardening: groups ebike_factor$cycles groups 220 707000 a 200 625400 b 180 587400 c 160 551200 d In this case as all level means are statistically different they all show up in separate groups, each indicated by a specific letter. Finally we can get from this package a plot with the Least significant difference error bars: plot(ebike_LSD) And below we’re exploring a manual execution of this type of plot (in this case with the standard deviations instead). ebike_factor %&gt;% group_by(temperature) %&gt;% summarise(cycles_mean = mean(cycles), cycles_sd = sd(cycles)) %&gt;% ggplot(aes(x = temperature, y = cycles_mean)) + geom_point(size = 2) + geom_line() + geom_errorbar(aes(ymin = cycles_mean - cycles_sd, ymax = cycles_mean + cycles_sd), width = .1) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + # scale_color_viridis_d(option = &quot;C&quot;, begin = 0.1, end = 0.9) + annotate(geom = &quot;text&quot;, x = Inf, y = -Inf, label = &quot;Error bars are +/- 1xSD&quot;, hjust = 1, vjust = -1, colour = &quot;grey30&quot;, size = 3, fontface = &quot;italic&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Boxplot of frame aging resistance&quot;, x = &quot;Furnace Temperature [°C]&quot;, y = &quot;Cycles to failure [n]&quot;) As often with statistical tools, there is debate on the best approach to use. We recommend to combine the Tukey test with the Fisher’s LSD completementary R functions. The Tukey test giving a first indication of the levels that have an effect and calculating the means differences and the Fisher function to provide much more additional information on each level. To be considered in each situation the slight difference between the significance level for difference between means and to decide if required to take the most conservative one. To go further in the Anova F-test we recommend this interesting article from Minitab (2016). Two factors multiple levels The solarcell output test Load and prepare data for analysis: solarcell_factor &lt;- solarcell_output %&gt;% pivot_longer( cols = c(&quot;T-10&quot;, &quot;T20&quot;, &quot;T50&quot;), names_to = &quot;temperature&quot;, values_to = &quot;output&quot; ) %&gt;% mutate(across(c(material, temperature), as_factor)) Model formulae solarcell_formula &lt;- output ~ temperature + material + temperature:material class(solarcell_formula) [1] &quot;formula&quot; solarcell_factor_lm &lt;- lm( formula = solarcell_formula, data = solarcell_factor ) summary(solarcell_factor_lm) Call: lm(formula = solarcell_formula, data = solarcell_factor) Residuals: Min 1Q Median 3Q Max -60.750 -14.625 1.375 17.937 45.250 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 134.75 12.99 10.371 6.46e-11 *** temperatureT20 -77.50 18.37 -4.218 0.000248 *** temperatureT50 -77.25 18.37 -4.204 0.000257 *** materialchristaline 21.00 18.37 1.143 0.263107 materialmultijunction 9.25 18.37 0.503 0.618747 temperatureT20:materialchristaline 41.50 25.98 1.597 0.121886 temperatureT50:materialchristaline -29.00 25.98 -1.116 0.274242 temperatureT20:materialmultijunction 79.25 25.98 3.050 0.005083 ** temperatureT50:materialmultijunction 18.75 25.98 0.722 0.476759 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 25.98 on 27 degrees of freedom Multiple R-squared: 0.7652,\tAdjusted R-squared: 0.6956 F-statistic: 11 on 8 and 27 DF, p-value: 9.426e-07 Looking at the output we see that R-squared is equal to 0.7652. This means about 77 percent of the variability in the battery life is explained by the plate material in the battery, the temperature, and the material type–temperature interaction. We’re going to go more in details now to validate the model and understand the effects and interactions of the different factors. Interaction plot In this experiement instead of just plotting a linear regression we need to go for a more elaborate plot that shows the response as a function of the two factors. Many different approaches are possible in R and here we’re starting with a rather simple one - the interaction plot from the stats package: interaction.plot(x.factor = solarcell_factor$temperature, trace.factor = solarcell_factor$material, fun = mean, response = solarcell_factor$output, trace.label = &quot;Material&quot;, legend = TRUE, main = &quot;Temperature-Material interaction plot&quot;, xlab = &quot;temperature [°C]&quot;, ylab = &quot;output [kWh/yr equivalent]&quot;) Although simple many important learnings can be extracted from this plot. We get the indication of the mean value of battery life for the different data groups at each temperature level for each material. Also we see immediatly that batteries tend to have longer lifes at lower temperature for all material types. We also see that there is certainly an interaction between material and temperature as the lines cross each other. We do now a quick assessment of the residuals, starting by the timeseries of residuals: Simplified timeseries plot(solarcell_factor_lm$residuals) No specific pattern is apparent so now we check all the remaining plots grouped into one single output: Residuals summary par(mfrow = c(2,2)) plot(solarcell_factor_lm) Residuals versus fit presents a rather simetrical distribution around zero indicating equality of variances at all levels and the qq plot presents good adherence to the centel line indicating a normal distributed population of residuals, all ok for these. The scale location plot though, shows a center line that is not horizontal which suggest the presence of outliers. Cooks histogram plot(solarcell_factor_lm, which = 4) We can extract the absolute maximum residual with: solarcell_factor_lm$residuals %&gt;% abs() %&gt;% max() [1] 60.75 Inspecting again the residuals plots we see that this corresponds to the point labeled with 2 for which the standardized value is greater than 2 standard deviations. We’re therefore apply the outlier test from the car package: library(car) outlierTest(solarcell_factor_lm) No Studentized residuals with Bonferroni p &lt; 0.05 Largest |rstudent|: rstudent unadjusted p-value Bonferroni p 4 -3.100368 0.0046065 0.16583 which gives a high Bonferroni p value thus excluding this possibility. As the R-squared was rather high and there were no issues with residuals we considere the model as acceptable and move ahead with the assessment of the significance of the different effects. For that we apply the anova to the linear model: Anova check anova(solarcell_factor_lm) Analysis of Variance Table Response: output Df Sum Sq Mean Sq F value Pr(&gt;F) temperature 2 39119 19559.4 28.9677 1.909e-07 *** material 2 10684 5341.9 7.9114 0.001976 ** temperature:material 4 9614 2403.4 3.5595 0.018611 * Residuals 27 18231 675.2 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see in the output little stars in front of the p value of the different factors. Three stars for temperature corresponding to an extremely low p value indicating that the means of the lifetime at different levels of temperature are significantly different, confirming that temperature has an effect on lifetime. With a lower significance but still clearly impacting lifetime depends on the material. Finally it is confirmed that there is an interaction between both factors has the temperature:material term has a p value of 0.01861 which us lower than the treshold of 0.05. The interaction here corresponds to the fact that increasing temperature from 15 to 70 decreases lifetime for material 2 but increases for material 3. Its interesting to consider what would have been the analysis if the interaction was not put in the model. We can easily assess that by creating a new model in R without the temperature:material term. solarcell_factor_lm_no_int &lt;- lm( output ~ temperature + material, data = solarcell_factor) summary(solarcell_factor_lm_no_int) Call: lm(formula = output ~ temperature + material, data = solarcell_factor) Residuals: Min 1Q Median 3Q Max -54.389 -21.681 2.694 17.215 57.528 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 122.47 11.17 10.965 3.39e-12 *** temperatureT20 -37.25 12.24 -3.044 0.00472 ** temperatureT50 -80.67 12.24 -6.593 2.30e-07 *** materialchristaline 25.17 12.24 2.057 0.04819 * materialmultijunction 41.92 12.24 3.426 0.00175 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 29.97 on 31 degrees of freedom Multiple R-squared: 0.6414,\tAdjusted R-squared: 0.5951 F-statistic: 13.86 on 4 and 31 DF, p-value: 1.367e-06 The model still presents a reasonably high R-square of 0.64. We now apply the anova on this new model: anova(solarcell_factor_lm_no_int) Analysis of Variance Table Response: output Df Sum Sq Mean Sq F value Pr(&gt;F) temperature 2 39119 19559.4 21.7759 1.239e-06 *** material 2 10684 5341.9 5.9472 0.006515 ** Residuals 31 27845 898.2 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The output naturally confirms the significance of the effects of the factors, however, as soon as a residual analysis is performed for these data, it becomes clear that the non-interaction model is inadequate: par(mfrow = c(2,2)) plot(solarcell_factor_lm_no_int) We see in the Residuals vs Fitted a clear pattern with residuals moving from positive to negative and then again to positive along the fitted values axis which indicates that there is an interaction at play. Covariance We assess here the potential utilisation of the analysis of covariance (ancova) in situations where a continuous variable may be influencing the measured value. This technique complements the analysis of variance (anova) allowing for a more accurate assessment of the effects of the categorical variables. Below a description of the approach taken from (Montgomery 2012), pag.655: Suppose that in an experiment with a response variable y there is another variable, say x, and that y is linearly related to x. Furthermore, suppose that x cannot be controlled by the experimenter but can be observed along with y. The variable x is called a covariate or concomitant variable. The analysis of covariance involves adjusting the observed response variable for the effect of the concomitant variable. If such an adjustment is not performed, the concomitant variable could inflate the error mean square and make true differences in the response due to treatments harder to detect. Thus, the analysis of covariance is a method of adjusting for the effects of an uncontrollable nuisance variable. As we will see, the procedure is a combination of analysis of variance and regression analysis. As an example of an experiment in which the analysis of covariance may be employed, consider a study performed to determine if there is a difference in the strength of a monofilament fiber produced by three different machines. The data from this experiment are shown in Table 15.10 (below). Figure 15.3 presents a scatter diagram of strength (y) versus the diameter (or thickness) of the sample. Clearly, the strength of the fiber is also affected by its thickness; consequently, a thicker fiber will generally be stronger than a thinner one. The analysis of covariance could be used to remove the effect of thickness (x) on strength (y) when testing for differences in strength between machines. solarcell_fill %&gt;% kable() material output fillfactor multijunction_A 108 20 multijunction_A 123 25 multijunction_A 117 24 multijunction_A 126 25 multijunction_A 147 32 multijunction_B 120 22 multijunction_B 144 28 multijunction_B 117 22 multijunction_B 135 30 multijunction_B 132 28 multijunction_C 105 21 multijunction_C 111 23 multijunction_C 126 26 multijunction_C 102 21 multijunction_C 96 15 Below a plot of strenght by thickness: solarcell_fill %&gt;% ggplot(aes(x = fillfactor, y = output)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_industRial() + labs( title = &quot;The solarcell output test&quot;, subtitle = &quot;Output vs Fill Factor&quot;, x = &quot;Fill factor [%]&quot;, y = &quot;Output&quot; ) Correlation test And a short test to assess the strenght of the correlation: library(stats) cor.test(solarcell_fill$output, solarcell_fill$fillfactor) Pearson&#39;s product-moment correlation data: solarcell_fill$output and solarcell_fill$fillfactor t = 9.8039, df = 13, p-value = 2.263e-07 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.8209993 0.9797570 sample estimates: cor 0.938542 Going further and using the approach from (Broc 2016) I’m faceting the scatterplots to assess if the coefficient of the linear regression is similar for all the levels of the machine factor: solarcell_fill %&gt;% ggplot(aes(x = fillfactor, y = output)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_wrap(vars(material)) + theme_industRial() + labs( title = &quot;The solarcell output test&quot;, subtitle = &quot;Output vs Fill Factor, by material type&quot;, x = &quot;Fill factor [%]&quot;, y = &quot;Output&quot; ) Visually this is the case, going from one level to the other is not changing the relationship between thickness and strenght - increasing thickness increases stenght. Visually the slopes are similar but the number of points is small. In a real case this verification could be extended with the correlation test for each level or/and a statistical test between slopes. We’re now reproducing in R the ancova case study from the book, still using the aov function. The way to feed the R function arguments is obtained from https://www.datanovia.com/en/lessons/ancova-in-r/ Three different machines produce a monofilament fiber for a textile company. The process engineer is interested in determining if there is a difference in the breaking strength of the fiber produced by the three machines. However, the strength of a fiber is related to its diameter, with thicker fibers being generally stronger than thinner ones. A random sample of five fiber specimens is selected from each machine. ancova solarcell_ancova &lt;- aov( output ~ fillfactor + material, solarcell_fill ) summary(solarcell_ancova) Df Sum Sq Mean Sq F value Pr(&gt;F) fillfactor 1 2746.2 2746.2 119.933 2.96e-07 *** material 2 119.6 59.8 2.611 0.118 Residuals 11 251.9 22.9 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that in the formula the covariate goes first (and there is no interaction)! If you do not do this in order, you will get different results. material in this table corresponds to the adjusted material mean square Conclusions from the book in page 662: Comparing the adjusted treatment means with the unadjusted treatment means (the y i. ), we note that the adjusted means are much closer together, another indication that the covariance analysis was necessary. A basic assumption in the analysis of covariance is that the treatments do not influence the covariate x because the technique removes the effect of variations in the x i. . However, if the variability in the x i. is due in part to the treatments, then analysis of covariance removes part of the treatment effect. Thus, we must be reasonably sure that the treatments do not affect the values x ij. In some experiments this may be obvious from the nature of the covariate, whereas in others it may be more doubtful. In our example, there may be a difference in fiber diameter (x ij ) between the three machines. In such cases, Cochran and Cox (1957) suggest that an analysis of variance on the x ij values may be helpful in determining the validity of this assumption. …there is no reason to believe that machines produce fibers of different diameters. (I did not go further here as it goes beyond the scope of the assessment) Comparison with anova Below the common approach we’ve been using in design of experiments. solarcell_aov &lt;- aov(output ~ material, solarcell_fill) summary(solarcell_aov) Df Sum Sq Mean Sq F value Pr(&gt;F) material 2 1264 631.8 4.089 0.0442 * Residuals 12 1854 154.5 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The anova table obtained also corresponds correctly to the book example. Montgomery final observations: It is interesting to note what would have happened in this experiment if an analysis of covariance had not been performed, that is, if the breaking strength data (y) had been analyzed as a completely randomized single-factor experiment in which the covariate x was ignored. The analysis of variance of the breaking strength data is shown in Table 15.14. We immediately notice that the error estimate is much longer in the CRD analysis (17.17 versus 2.54). This is a reflection of the effectiveness of analysis of covariance in reducing error variability. We would also conclude, based on the CRD analysis, that machines differ significantly in the strength of fiber produced. This is exactly opposite the conclusion reached by the covariance analysis. If we suspected that the machines differed significantly in their effect on fiber strength, then we would try to equalize the strength output of the three machines. However, in this problem the machines do not differ in the strength of fiber produced after the linear effect of fiber diameter is removed. It would be helpful to reduce the within-machine fiber diameter variability because this would probably reduce the strength variability in the fiber. Potential applications In the scope of methods validations this approach could potentially be used in robustness validations when there is suspiction that a continuous variable is disturbing the measurement. Naturally this should not be applied everywhere but only where there would to be logical a physical or chemical reason behind as in the example with thickness and strenght. ## Error in get(genname, envir = envir) : objet &#39;testthat_print&#39; introuvable General designs In a design of experiments we calculate the total number of trials with the expression \\(n^m\\) where n is the number of levels, m the number of factors. A trial represents the number of unique combinations of the factors. To obtain the final number of test runs we have to multiply the number of trials by the number of replicates per trial. In a design with 4 factors of 2 levels we have then \\(2^4 = 16\\) runs and \\(16 \\times 5 = 80\\) replicates. If the design has a combination of factors with different number of levels the number of trials is the multiplication of both such as: \\(n^m \\times n^m\\). For example if we added 2 additional factors with 4 levels each to the previous design we would obtain \\(2^4 \\times 4^2 = 256\\) which we would still need to multiply by the number of replicates to obtain the number of runs \\(256 \\times 5 = 1280\\). In the literature we often see the simbolic notation \\(a^k\\) but we’ve opted for mF-nL (m factors, n levels) in this book for simplification. m factors n levels designs The juice production plant We’re comming back to our Juice Bottling context where a quality team was looking to put in operation a new measurement device for dry matter content in a juices bottling line. After a short brainstorming using the Ishikawa tool presented before the team has identified several potential influcing parameters on the equipment bias when compared with the reference equipement: the product itself, the drymatter level on the product (its target), the speed of the filling line and the poweder particle size. In order to evaluate such impact the team has prepared a mid size experiment design with three products, three levels of drymatter, two line speed levels and two particle size levels. First we load the DoE.base package: library(DoE.base) and then generate the doe with the fac.design function. Factorial design juice_doe &lt;- fac.design( randomize = FALSE, factor.names = list( product = c(&quot;beetroot&quot;, &quot;apple&quot;, &quot;carrot&quot;), drymatter_target = c(10, 15, 20), part = c(1, 2, 3), speed = c(20, 25), particle_size = c(250, 300)) ) Note that the DoE generated is more than just a tibble, it belongs to a specific class called design and has many other attributes just like an lm or aov S3 objects. class(juice_doe) [1] &quot;design&quot; &quot;data.frame&quot; The power and care given by the package authors become visible when we use an R generic function such as summary() with this object and we see it returns a tailor made output, in this case showing the levels of the different factors of our design: summary(juice_doe) Call: fac.design(randomize = FALSE, factor.names = list(product = c(&quot;beetroot&quot;, &quot;apple&quot;, &quot;carrot&quot;), drymatter_target = c(10, 15, 20), part = c(1, 2, 3), speed = c(20, 25), particle_size = c(250, 300))) Experimental design of type full factorial 108 runs Factor settings (scale ends): product drymatter_target part speed particle_size 1 beetroot 10 1 20 250 2 apple 15 2 25 300 3 carrot 20 3 Using this the team has simple copied the experiment plan to an spreadsheet to collect the data: juice_doe %&gt;% write_clip() and after a few day the file completed and ready for analysis looked like: juice_drymatter %&gt;% head() %&gt;% kable() product drymatter_TGT speed particle_size part drymatter_DRX drymatter_REF apple 10 20 250 1 9.80 10.05 apple 10 20 250 2 9.82 10.05 apple 10 20 250 3 9.82 10.05 beetroot 10 20 250 1 9.79 10.03 beetroot 10 20 250 2 9.75 10.03 beetroot 10 20 250 3 9.77 10.03 juice_drymatter &lt;- juice_drymatter %&gt;% mutate(bias = drymatter_DRX - drymatter_REF) Main effects plots As the number of factors and levels of a design increase, more thinking is required to obtain good visualisation of the data. Main effects plots consist usually of a scatterplot representing the experiment output as a function of one of the inputs. In a design like this with three different inputs three plots are required: drymatter_TGT_plot &lt;- juice_drymatter %&gt;% group_by(drymatter_TGT) %&gt;% summarise(bias_m_drymatter = mean(bias)) %&gt;% ggplot(aes(x = drymatter_TGT, y = bias_m_drymatter)) + geom_point() + geom_line() + coord_cartesian( xlim = c(9,21), ylim = c(-1,0), expand = TRUE) + labs( title = &quot;Juice bottling problem&quot;, subtitle = &quot;Main effects plots&quot;, x = &quot;drymatter_TGT [%]&quot;, y = &quot;Average bias [g]&quot; ) particle_size_plot &lt;- juice_drymatter %&gt;% group_by(particle_size) %&gt;% summarise(particle_size_bias_mean = mean(bias)) %&gt;% ggplot(aes(x = particle_size, y = particle_size_bias_mean)) + geom_point() + geom_line() + coord_cartesian( xlim = c(240,310), ylim = c(-1,0), expand = TRUE) + labs( x = &quot;particle_size&quot;, y = &quot;Average bias [g]&quot; ) speed_plot &lt;- juice_drymatter %&gt;% group_by(speed) %&gt;% summarise(speed_bias_mean = mean(bias)) %&gt;% ggplot(aes(x = speed, y = speed_bias_mean)) + geom_point() + geom_line() + coord_cartesian( xlim = c(19, 26), ylim = c(-1,0), expand = TRUE) + labs( x = &quot;Speed&quot;, y = &quot;Average bias [g]&quot; ) drymatter_TGT_plot + particle_size_plot + speed_plot This kind of plots gives already important insights in to the experiement outcome, even before any deeper analysis with a linear model and anova. In our case: higher particle_size and higher speed result in higher bias weight deviation beyond 10.5% drymatter_TGT level the bias weight is always higher than the target Interaction plots (custom) In designs like these with 3 factors we have 3 possible interactions (A-B, A-C, B-C) corresponding the the possible combination between them. This results in three interaction plots that we’re presenting below. The approach here goes beyond the interaction.plot function from the {stats} package presented previously in the two factors multiple levels case. We are developping here the plots with {ggplot2} which provides much more control on the plot attibutes but on the other hand requires that additional code is added to calculate the means by group. drymatter_TGT_particle_size_plot &lt;- juice_drymatter %&gt;% mutate(particle_size = as_factor(particle_size)) %&gt;% group_by(drymatter_TGT, particle_size) %&gt;% summarise(drymatter_bias_mean = mean(bias), drymatter_bias_sd = sd(bias)) %&gt;% ggplot(aes(x = drymatter_TGT, y = drymatter_bias_mean, color = particle_size, linetype = particle_size)) + geom_point(aes(group = particle_size), size = 2) + geom_line(aes(group = particle_size, linetype = particle_size)) + scale_linetype(name = &quot;Particle Size&quot;) + geom_errorbar(aes( ymin = drymatter_bias_mean - 2 * drymatter_bias_sd, ymax = drymatter_bias_mean + 2 * drymatter_bias_sd, width = .5 )) + scale_linetype(guide=FALSE) + scale_color_viridis_d(option = &quot;C&quot;, begin = 0.3, end = 0.7, name = &quot;Particle size&quot;) + coord_cartesian( xlim = c(9,21), ylim = c(-1,0), expand = TRUE) + annotate(geom = &quot;text&quot;, x = Inf, y = 0, label = &quot;Error bars are +/- 2xSD&quot;, hjust = 1, vjust = -1, colour = &quot;grey30&quot;, size = 3, fontface = &quot;italic&quot;) + labs( title = &quot;Juice bottling problem&quot;, subtitle = &quot;Interaction plots&quot;, x = &quot;Drymatter target&quot;, y = &quot;Average bias deviation [g]&quot; ) + theme_industRial() + theme(legend.justification=c(1,0), legend.position=c(1,0)) drymatter_TGT_speed_plot &lt;- juice_drymatter %&gt;% mutate(speed = as_factor(speed)) %&gt;% group_by(drymatter_TGT, speed) %&gt;% summarise(drymatter_bias_mean = mean(bias), drymatter_bias_sd = sd(bias)) %&gt;% ggplot(aes(x = drymatter_TGT, y = drymatter_bias_mean, color = speed)) + geom_point(aes(group = speed), size = 2) + geom_line(aes(group = speed, linetype = speed)) + scale_linetype(guide=FALSE) + scale_color_viridis_d(option = &quot;C&quot;, begin = 0.3, end = 0.7, name = &quot;Speed&quot;) + geom_errorbar(aes( ymin = drymatter_bias_mean - 2 * drymatter_bias_sd, ymax = drymatter_bias_mean + 2 * drymatter_bias_sd, width = .5 )) + coord_cartesian( xlim = c(9, 21), ylim = c(-1,0), expand = TRUE) + annotate(geom = &quot;text&quot;, x = Inf, y = 0, label = &quot;Error bars are +/- 2xSD&quot;, hjust = 1, vjust = -1, colour = &quot;grey30&quot;, size = 3, fontface = &quot;italic&quot;) + labs( x = &quot;Dry matter target&quot;, y = &quot;Average bias deviation [g]&quot; ) + theme_industRial() + theme(legend.justification=c(1,0), legend.position=c(1,0)) speed_particle_size_plot &lt;- juice_drymatter %&gt;% mutate(particle_size = as_factor(particle_size)) %&gt;% group_by(speed, particle_size) %&gt;% summarise(drymatter_bias_mean = mean(bias), drymatter_bias_sd = sd(bias)) %&gt;% ggplot(aes(x = speed, y = drymatter_bias_mean, color = particle_size)) + geom_point(aes(group = particle_size), size = 2) + geom_line(aes(group = particle_size, linetype = particle_size)) + scale_linetype(guide=FALSE) + scale_color_viridis_d(option = &quot;C&quot;, begin = 0.3, end = 0.7, name = &quot;Particle size&quot;) + geom_errorbar(aes( ymin = drymatter_bias_mean - 2 * drymatter_bias_sd, ymax = drymatter_bias_mean + 2 * drymatter_bias_sd, width = .5 )) + coord_cartesian( xlim = c(19, 26), ylim = c(-1,0), expand = TRUE) + annotate(geom = &quot;text&quot;, x = Inf, y = 0, label = &quot;Error bars are +/- 2xSD&quot;, hjust = 1, vjust = -1, colour = &quot;grey30&quot;, size = 3, fontface = &quot;italic&quot;) + labs( x = &quot;Speed&quot;, y = &quot;Average bias deviation [g]&quot; ) + theme_industRial() + theme(legend.justification=c(1,0), legend.position=c(1,0)) drymatter_TGT_particle_size_plot + drymatter_TGT_speed_plot + speed_particle_size_plot The plots indicate no interaction between the different factors as all lines do not intercept and are mostly parallel. In most cases the anova would be performed first and only the plot for the significant interactions would be plotted, if any. Anova with 3rd level interactions The sources of variation for the Anova table for three-factor fixed effects model are: A, B, C, AB, AC, BC, ABC. To be noted that like in the two-factors we must have at least two parts (n&gt;2) to determine the sum of squares due to error if all possible interactions are to be included in the model. We are now fully prepared for an assessment of the effect of the different factors with the anova. To reduce the amount of coding we’re inputing the model directly in the aov function: juice_drymatter_aov &lt;- aov( bias ~ drymatter_TGT * speed * particle_size, data = juice_drymatter) summary(juice_drymatter_aov) Df Sum Sq Mean Sq F value Pr(&gt;F) drymatter_TGT 1 1.3149 1.3149 486.057 &lt;2e-16 *** speed 1 0.0000 0.0000 0.000 0.985 particle_size 1 0.6241 0.6241 230.705 &lt;2e-16 *** drymatter_TGT:speed 1 0.0007 0.0007 0.272 0.603 drymatter_TGT:particle_size 1 0.0028 0.0028 1.040 0.310 speed:particle_size 1 0.0032 0.0032 1.191 0.278 drymatter_TGT:speed:particle_size 1 0.0039 0.0039 1.442 0.233 Residuals 100 0.2705 0.0027 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The observations of the plots are confirmed and completed with statistical input: we see that the percentage of drymatter_TGT and the particle_size significantly affect the bias volume (p &lt; 0.05). The drymatter_TGT-particle_size interactions are non significative. As expected the anova confirms strong influence of the dissolution level on the bias. From the analysis all interactions could be removed from the model in order to establish a predictive model. ## Error in get(genname, envir = envir) : objet &#39;testthat_print&#39; introuvable Two level designs Coding factors 2 factors 2 levels The \\(2^{k}\\) designs are particularly useful in the early stages of experimental work when many factors are likely to be investigated. It provides the smallest number of runs with which k factors can be studied in a complete factorial design. Consequently, these designs are widely used in factor screening experiments. The validity of the analysis depends on the following assumptions: the factors are fixed the designs are completely randomized the usual normality assumptions are satisfied the response is approximately linear over the range of the factor levels chosen Analysis Procedure for a 2 k Design Estimate factor effects Form initial model (full model) If the design is replicated, fit the full model If there is no replication, form the model using a normal probability plot of the effects Perform statistical testing (Anova) Refine model (remove non significant effects) Analyze residuals Interpret results DEF - Sparsity of effects principle: most systems are dominated by some of the main effects and low-order interactions, and most high-order interactions are negligible. In this first Case Study dedicated to \\(2^k\\) designs we’re going to explore the contrasts settings in the linear model functions. The PET clothing improvement plan In this case study factors have only 2 levels. Below we start by preparing our dataset: library(DoE.base) pet_doe &lt;- fac.design( randomize = FALSE, factor.names=list(A=c(&quot;-&quot;,&quot;+&quot;), B=c(&quot;-&quot;,&quot;+&quot;), replicate = c(&quot;I&quot;, &quot;II&quot;, &quot;III&quot;)) ) tensile_strength &lt;- c(64.4,82.8,41.4,71.3,57.5,73.6,43.7,69.0,62.1,73.6,52.9,66.7) pet_doe &lt;- bind_cols( pet_doe, &quot;tensile_strength&quot; = tensile_strength, ) Coding levels Factors as +/- In this first model we’re using a design where the inputs levels have been defined as plus and minus, sometimes also called high and low. The actual naming is not important, what is critical is to ensure that those input parameters are coded as factors. pet_fct &lt;- pet_doe %&gt;% mutate(across(c(A,B), as_factor)) Another detail is to put the higher level as the reference otherwise we will get inverted signs in the lm output: pet_fct$A &lt;- relevel(pet_fct$A, ref=&quot;+&quot;) pet_fct$B &lt;- relevel(pet_fct$B, ref=&quot;+&quot;) and one final step is need which is the setup of the contrasts. As our design is ortogonal and we want the contrasts to add up to zero we have to indicate that on the factor so that the coefficients of the linear model are correctly calculated. The current definition of the contrasts is: contrasts(pet_fct$A) - + 0 - 1 So we change this with: contrasts(pet_fct$A) &lt;- &quot;contr.sum&quot; contrasts(pet_fct$B) &lt;- &quot;contr.sum&quot; contrasts(pet_fct$A) [,1] + 1 - -1 contrasts(pet_fct$A) [,1] + 1 - -1 Now we can run our linear model: pet_ctr_lm &lt;- lm( formula = tensile_strength ~ A * B, data = pet_fct ) summary(pet_ctr_lm) Call: lm.default(formula = tensile_strength ~ A * B, data = pet_fct) Residuals: Min 1Q Median 3Q Max -4.600 -3.067 -1.150 2.492 6.900 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 63.250 1.314 48.135 3.84e-11 *** A1 9.583 1.314 7.293 8.44e-05 *** B1 -5.750 1.314 -4.376 0.00236 ** A1:B1 1.917 1.314 1.459 0.18278 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.552 on 8 degrees of freedom Multiple R-squared: 0.903,\tAdjusted R-squared: 0.8666 F-statistic: 24.82 on 3 and 8 DF, p-value: 0.0002093 We can observe in the output that the p value of the effects is the same in the lm and in the the aov functions. This confirms that the contrasts have been correctly specified with contr.sum Note that we’ve had to adjust the contrasts in the lm function with contr.sum which applies to cases where the sum of the contrasts is zero (the R default is contr.treatment which applies to cases where the levels are coded as 0 and 1). and now going to apply a prediction: predict(pet_ctr_lm, newdata = list(A = &quot;+&quot;, B = &quot;+&quot;)) 1 69 Factors as +/- 1 In this example we convert the levels to factors still using the +/-1 notation. This will also be helpfull to apply what are called the Yates tables. coded &lt;- function(x) { ifelse(x == x[1], -1, 1) } We again convert them to factors and put the upper level as the reference. Regarding the contrasts we show a simpler and more direct approach now by defining them directly in the lm() function. pet_fct &lt;- pet_fct %&gt;% mutate(cA = coded(A), cB = coded(B)) pet_fct2 &lt;- pet_fct %&gt;% mutate(across(c(cA, cB), as_factor)) pet_fct2$cA &lt;- relevel(pet_fct2$cA, ref = &quot;1&quot;) pet_fct2$cB &lt;- relevel(pet_fct2$cB, ref = &quot;1&quot;) pet_ctr2_lm &lt;- lm( formula = tensile_strength ~ cA * cB, data = pet_fct2, contrasts = list(cA = &quot;contr.sum&quot;, cB = &quot;contr.sum&quot;) ) summary(pet_ctr2_lm) Call: lm.default(formula = tensile_strength ~ cA * cB, data = pet_fct2, contrasts = list(cA = &quot;contr.sum&quot;, cB = &quot;contr.sum&quot;)) Residuals: Min 1Q Median 3Q Max -4.600 -3.067 -1.150 2.492 6.900 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 63.250 1.314 48.135 3.84e-11 *** cA1 9.583 1.314 7.293 8.44e-05 *** cB1 -5.750 1.314 -4.376 0.00236 ** cA1:cB1 1.917 1.314 1.459 0.18278 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.552 on 8 degrees of freedom Multiple R-squared: 0.903,\tAdjusted R-squared: 0.8666 F-statistic: 24.82 on 3 and 8 DF, p-value: 0.0002093 Note that a coefficient in a regression equation is the change in the response when the corresponding variable changes by +1. Special attention to the + and - needs to be taken with the R output. As A or B changes from its low level to its high level, the coded variable changes by 1 − (−1) = +2, so the change in the response is twice the regression coefficient. So the effects and interaction(s) from their minumum to their maximum correspond to twice the values in the “Estimate” column. These regression coefficients are often called effects and interactions, even though they differ from the definitions used in the designs themeselves. Checking now with coded factors: predict(pet_ctr2_lm, newdata = list(cA = &quot;1&quot;, cB = &quot;1&quot;)) 1 69 Factors as +/- 1 numeric In this example we’re going to code the levels with +1/-1 but we’re going use the numeric coding: pet_num &lt;- pet_fct %&gt;% mutate(cA = coded(A), cB = coded(B)) pet_num_lm &lt;- lm( formula = tensile_strength ~ cA * cB, data = pet_num ) summary(pet_num_lm) Call: lm.default(formula = tensile_strength ~ cA * cB, data = pet_num) Residuals: Min 1Q Median 3Q Max -4.600 -3.067 -1.150 2.492 6.900 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 63.250 1.314 48.135 3.84e-11 *** cA 9.583 1.314 7.293 8.44e-05 *** cB -5.750 1.314 -4.376 0.00236 ** cA:cB 1.917 1.314 1.459 0.18278 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.552 on 8 degrees of freedom Multiple R-squared: 0.903,\tAdjusted R-squared: 0.8666 F-statistic: 24.82 on 3 and 8 DF, p-value: 0.0002093 In this case we did not define any contrasts. Looking into the lm We can see we’ve obtained exactly the same outputs. predict(pet_num_lm, newdata = list(cA = 1, cB = 1)) 1 69 As the inputs are coded as numeric this behaves just like the first simple linear model we’ve seen in the Case Study on One Factor with Multiple levels. In particular when we feed the predictions function with numeric values. This is very intuitive as it corresponds to the original units of the experiments (also called natural or engineering units). On the other hand coding the design variables provides another advange: generally, the engineering units are not directly comparable while coded variables are very effective for determining the relative size of factor effects. We can see that these three ways of coding the variable levels lead to equivalent results both in lm and prediction. Our preference goes to use numeric values as it is more intuitive and allows for easier prediction between the fixed levels. And now in order to better understand the coding of factors in this unit, we’re going to establish a simple regression plot of our data: pet_num %&gt;% unclass() %&gt;% as_tibble() %&gt;% mutate(cA = coded(A), cB = coded(B)) %&gt;% pivot_longer( cols = c(&quot;cA&quot;, &quot;cB&quot;), names_to = &quot;variable&quot;, values_to = &quot;level&quot;) %&gt;% ggplot() + geom_point(aes(x = level, y = tensile_strength)) + geom_smooth(aes(x = level, y = tensile_strength), method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) + facet_wrap(vars(variable)) Note that we had to extract the data from the S3 doe object, which we’ve done with using unclass() and then as_tibble() The intercept passes at 27.5 as seen on the lm summary. We’re going now to put the B factor at its maximum and replot: pet_num %&gt;% unclass() %&gt;% as_tibble() %&gt;% mutate(cA = coded(A), cB = coded(B)) %&gt;% filter(cB == 1) %&gt;% pivot_longer( cols = c(&quot;cA&quot;, &quot;cB&quot;), names_to = &quot;variable&quot;, values_to = &quot;level&quot;) %&gt;% ggplot() + geom_point(aes(x = level, y = tensile_strength)) + geom_smooth(aes(x = level, y = tensile_strength), method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) + coord_cartesian(xlim = c(-2, 2)) + scale_y_continuous(n.breaks = 10) + facet_wrap(vars(variable)) As seen on the plot the output of our prediction is 69 corresponding the high level of A when B is at 1. To be precise we need to multiply all the coefficients by the levels of the factors as : 63.250 + 9.583x(+1) - 5.750x(+1) + 1.917 Interaction plots with SE Here we’re making a step further in the representation of interaction plots, we’re adding error bars to the means. There are many ways to do this and we’re providing a simple approach with the function plotMeans from the package RcmdrMisc. library(RcmdrMisc) We select standard error as argument for the error.bars argument. par(mfrow = c(1,1), bty = &quot;l&quot;) plotMeans(response = pet_fct$tensile_strength, factor2 = pet_fct$A, factor1 = pet_fct$B, error.bars = &quot;se&quot;, xlab = &quot;A - Reactant&quot;, legend.lab = &quot;B - Catalist\\n(error bars +/-se)&quot;, ylab = &quot;Tensile Strenght&quot;, col = viridis::viridis(12)[4], legend.pos = &quot;bottomright&quot;, main = &quot;The PET clothing improvement plan&quot;) 3 factors 2 levels The lithium-ion battery charging time test A - temperature B - previous cycles (within warranty) C - voltage response - charging time [h] battery_charging %&gt;% head() %&gt;% kable() A B C D Replicate charging_time -1 -1 -1 -1 1 5.50 1 -1 -1 -1 1 6.69 -1 1 -1 -1 1 6.33 1 1 -1 -1 1 6.42 -1 -1 1 -1 1 10.37 1 -1 1 -1 1 7.49 battery_lm &lt;- lm( formula = charging_time ~ A * B * C, data = battery_charging ) summary(battery_lm) Call: lm.default(formula = charging_time ~ A * B * C, data = battery_charging) Residuals: Min 1Q Median 3Q Max -2.0950 -1.0025 -0.5288 0.9287 2.9825 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.41156 0.26542 27.924 &lt; 2e-16 *** A 0.31469 0.26542 1.186 0.247370 B 0.06844 0.26542 0.258 0.798720 C 1.04031 0.26542 3.920 0.000646 *** A:B -0.08719 0.26542 -0.328 0.745387 A:C -0.80906 0.26542 -3.048 0.005532 ** B:C 0.02594 0.26542 0.098 0.922963 A:B:C 0.03281 0.26542 0.124 0.902640 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.501 on 24 degrees of freedom Multiple R-squared: 0.5225,\tAdjusted R-squared: 0.3832 F-statistic: 3.751 on 7 and 24 DF, p-value: 0.006963 battery_aov &lt;- aov(battery_lm) summary(battery_aov) Df Sum Sq Mean Sq F value Pr(&gt;F) A 1 3.17 3.17 1.406 0.247370 B 1 0.15 0.15 0.066 0.798720 C 1 34.63 34.63 15.363 0.000646 *** A:B 1 0.24 0.24 0.108 0.745387 A:C 1 20.95 20.95 9.292 0.005532 ** B:C 1 0.02 0.02 0.010 0.922963 A:B:C 1 0.03 0.03 0.015 0.902640 Residuals 24 54.10 2.25 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The main effects of Gap and Power are highly significant (both have very small P-values). The AC interaction is also highly significant; thus, there is a strong interaction between Gap and Power. Adjusted R-squared The ordinary R^2 is 0.9661 and it measures the proportion of total variability explained by the model. A potential problem with this statistic is that it always increases as factors are added to the model, even if these factors are not significant. The adjusted R^2 is obtained by dividing the Sums of Squares by the degrees of freedom, and is adjusted for the size of the model, that is the number of factors. battery_reduced_lm &lt;- lm( formula = charging_time ~ A + C + A:C, data = battery_charging ) summary(battery_reduced_lm) Call: lm.default(formula = charging_time ~ A + C + A:C, data = battery_charging) Residuals: Min 1Q Median 3Q Max -2.1463 -0.9950 -0.4575 0.8650 2.9050 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.4116 0.2467 30.037 &lt; 2e-16 *** A 0.3147 0.2467 1.275 0.212663 C 1.0403 0.2467 4.216 0.000235 *** A:C -0.8091 0.2467 -3.279 0.002786 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.396 on 28 degrees of freedom Multiple R-squared: 0.5185,\tAdjusted R-squared: 0.4669 F-statistic: 10.05 on 3 and 28 DF, p-value: 0.0001157 Besides the base summary() function, R squared and adjusted R squared can also be easily retrieved with the glance function from the {broom} package. We’re extracting them here for the complete and for reduced model: glance(battery_lm)[1:2] %&gt;% bind_rows(glance(battery_reduced_lm)[1:2], .id = &quot;model&quot;) # A tibble: 2 x 3 model r.squared adj.r.squared &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 0.522 0.383 2 2 0.519 0.467 Adjusted R² has improved. Removing the nonsignificant terms from the full model has produced a final model that is likely to function more effectively as a predictor of new data. Coding inputs Now that we have model often we will want to predict the response at a certainly specific level between the coded factor levels of \\(\\pm\\) 1. To do that we need to convert that specific the natural value into a coded value. Lets calculate the coded value for the factor A (gap) of which the natural value is nA = 0.9, between the natural levels of nA = 0.8 and nA = 1.2. We choose to do this for a fixed level of C of 1, corresponding to its maximum of 325W. natural2coded &lt;- function(xA, lA, hA) {(xA - (lA + hA) / 2) / ((hA - lA) / 2)} # Converting natural value xA into coded value cA: lA &lt;- 0.8 hA &lt;- 1.2 xA &lt;- 0.9 cA &lt;- natural2coded(xA, lA, hA) cA [1] -0.5 To be noted that the opposite conversion looks like: coded2natural &lt;- function(cA, lA, hA) {cA * ((hA - lA) / 2) + ((lA + hA)/2)} # Converting back the coded value cA into its natural value xA lA &lt;- 0.8 hA &lt;- 1.2 cA &lt;- -0.5 nA &lt;- coded2natural(cA, lA, hA) nA [1] 0.9 Coded prediction And now we can feed our linear model and make predictions: battery_new &lt;- tibble(A = cA, C = 1) pA &lt;- predict(battery_reduced_lm, battery_new) pA 1 8.699062 We can visualize this outcome as follows: battery_charging %&gt;% filter(C == 1) %&gt;% ggplot() + geom_point(aes(x = A, y = charging_time, color = as_factor(C))) + geom_smooth(aes(x = A, y = charging_time), method = &quot;lm&quot;) + geom_point(aes(x = cA, y = pA)) + scale_y_continuous(n.breaks = 10) + scale_color_discrete(guide = FALSE) + theme(plot.title = ggtext::element_markdown()) + labs( title = &quot;3^k factorial design&quot;, subtitle = &quot;Prediction with reduced model&quot;) We are introducing here response surface plots which is yet another way to visualize the experiment outputs as a function of the inputs. We’re doing this with the persp() function from the {rsm} package which provides an extremely fast rendering, easy parametrization and a readable output. To be noted that this function is an extension of the base R persp() consisting from the R point of view in an S3 method for the lm class. This allows to simply provide directly the lm object to the function to obtain the response surface. Perspective plot library(rsm) persp( battery_reduced_lm, A ~ C, bounds = list(A = c(-1,1), C = c(-1,1)), col = viridis(12)[8], theta = -40, phi = 20, r = 5, zlab = &quot;Charging Time&quot;, main = &quot;Lithium-ion battery\\ncharging time test&quot; ) Due to the interaction between factors A and C the surface is slightly bent. This is exactly what we observe in the interactions plots of which the one below corresponds to slicing the surface at the min and the max of Power: interaction.plot(x.factor = battery_charging$C, trace.factor = battery_charging$A, fun = mean, response = battery_charging$charging_time, legend = TRUE, xlab = &quot;C&quot;, trace.label = &quot;A&quot;, lwd = 2, col = c(viridis(12)[10], col = viridis(12)[6]), ylab = &quot;Charging Time&quot;, main = &quot;Lithium-ion battery\\ncharging time test&quot;) Just like in the surface plot we can see here in the interaction plot that the response of yield on gap is different depending on the level of power. When power is high it decreases and when power is low it increases. As a reminder this is what is called an interaction between these two factors. Single replicate designs The lithium-ion battery charging time test (cont.) m factors 2 levels Possible approaches: - graphical methods–normal and half-normal probability plots; no formal tests; - assume some high-order interactions are zero, and fit a model that excludes them; degrees of freedom go into error, so testing is possible (not recommended) battery_charging %&gt;% filter((Replicate == 1)) %&gt;% head() # A tibble: 6 x 6 A B C D Replicate charging_time &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 -1 -1 -1 -1 1 5.5 2 1 -1 -1 -1 1 6.69 3 -1 1 -1 -1 1 6.33 4 1 1 -1 -1 1 6.42 5 -1 -1 1 -1 1 10.4 6 1 -1 1 -1 1 7.49 battery_lm3 &lt;- lm( formula = charging_time ~ A * B * C * D, data = battery_charging %&gt;% filter(Replicate == 1)) summary(battery_lm3) Call: lm.default(formula = charging_time ~ A * B * C * D, data = battery_charging %&gt;% filter(Replicate == 1)) Residuals: ALL 16 residuals are 0: no residual degrees of freedom! Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.76062 NA NA NA A -0.50812 NA NA NA B 0.03688 NA NA NA C 1.53063 NA NA NA D 0.15563 NA NA NA A:B -0.12438 NA NA NA A:C -0.76813 NA NA NA B:C -0.01062 NA NA NA A:D 0.12437 NA NA NA B:D -0.05563 NA NA NA C:D 0.16062 NA NA NA A:B:C 0.02812 NA NA NA A:B:D 0.08562 NA NA NA A:C:D 0.18438 NA NA NA B:C:D 0.03688 NA NA NA A:B:C:D -0.03688 NA NA NA Residual standard error: NaN on 0 degrees of freedom Multiple R-squared: 1,\tAdjusted R-squared: NaN F-statistic: NaN on 15 and 0 DF, p-value: NA We can see that being a single replicate design no statistics have been calculated for the effects in the model. A recommended approach in this case is to look into the normal probability plot of the model effects. Here we are going to prepare this plot with the function qqPlot() from the {car} package: Effects normal plot library(car) battery_eff3 &lt;- battery_lm3$coefficients[2:16] battery_eff_names2 &lt;- names((battery_lm3$coefficients)[2:16]) main_effects_plot &lt;- qqPlot( battery_eff3, envelope = 0.70, id = list( method = &quot;y&quot;, n = 5, cex = 1, col = carPalette()[1], location = &quot;lr&quot;), grid = FALSE, col = &quot;black&quot;, col.lines = &quot;black&quot;, main = &quot;Chemical vessel - Normal plot of effects 2&quot; ) In plot we can see that the effects that have the highest influence on the output are the effects A, C and D and their interactions. We can still confirm these observations with a calculation of the percentage contribution of each effect as follows: Effects contribution table battery_lm_tidy3 &lt;- battery_lm3 %&gt;% tidy() %&gt;% filter(term != &quot;(Intercept)&quot;) %&gt;% mutate( effect_estimate = -2 * estimate, effect_estimate_sum = sum(effect_estimate), effect_contribution_perc = abs((effect_estimate/effect_estimate_sum)*100) %&gt;% round(2) ) battery_lm_tidy3 %&gt;% select(term, effect_estimate, effect_contribution_perc) %&gt;% arrange(desc(effect_contribution_perc)) %&gt;% head(8) %&gt;% kable() term effect_estimate effect_contribution_perc C -3.06125 182.35 A:C 1.53625 91.51 A 1.01625 60.54 A:C:D -0.36875 21.97 C:D -0.32125 19.14 D -0.31125 18.54 A:B 0.24875 14.82 A:D -0.24875 14.82 Reduced model Following the previous analysis we are removing the factor B from the model and keeping only the 2nd order interactions assuming the system also respects the sparcity of effects principle. battery_red_lm3 &lt;- lm( formula = charging_time ~ A + C + A:C, data = battery_charging) summary(battery_red_lm3) Call: lm.default(formula = charging_time ~ A + C + A:C, data = battery_charging) Residuals: Min 1Q Median 3Q Max -2.1463 -0.9950 -0.4575 0.8650 2.9050 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.4116 0.2467 30.037 &lt; 2e-16 *** A 0.3147 0.2467 1.275 0.212663 C 1.0403 0.2467 4.216 0.000235 *** A:C -0.8091 0.2467 -3.279 0.002786 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.396 on 28 degrees of freedom Multiple R-squared: 0.5185,\tAdjusted R-squared: 0.4669 F-statistic: 10.05 on 3 and 28 DF, p-value: 0.0001157 We can now see that we’ve regained degrees of freedom and obtained a sort of hidden replication allowing to calculate statistics and error terms on the model. Residuals analysis Checking the residuals we see the significant effect of the remaining interactions. The residuals are not completely normal but the in the standardized residuals the deviations are contained within 1.2 sd. par(mfrow = c(2,2)) plot(battery_red_lm3) We can now establish the main effects and interaction plots and conclude on the optimal settings to maximize the output: A and D should be on the max and C on the min. ## Error in get(genname, envir = envir) : objet &#39;testthat_print&#39; introuvable "],
["SPC.html", "Statistical Process Control xbar-R charts Cpk charts I-MR charts", " Statistical Process Control Keeping the variability of an industrial process under control is one of the most important objectives in manufacturing. Based on expert knowledge or on detailed functional analysis the product and process parameters that are critical to quality are identified and selected for close follow-up. The most common and effective way for such follow-up is the Statistical Process Control which is done by using control charts. The syringe injection molding process xbar-R charts There are many types of control charts and in this case study we’re demonstrating the xbar and R charts. These two charts are often used together and are suited to the control the mean and the variability of a continuous variable. Bamako Lightening is a company that manufactures lamps. The weight of each lamp is critical to the quality of the product. The Production Operator monitors the production process using xbar and R-charts. Samples are taken of six lamps every hour and their means and ranges plotted on control charts. Data is available representing samples taken a period of 25 hours of production. Looking at the first five lines to confirm and assess the quality of our data for further processing. head(syringe_diameter) %&gt;% kable() Hour Sample1 Sample2 Sample3 Sample4 Sample5 Sample6 Hour1 5.331433 5.339867 5.324400 5.336267 5.322833 5.318133 Hour2 5.324033 5.321433 5.314200 5.323733 5.341967 5.339233 Hour3 5.326267 5.340433 5.313567 5.356533 5.338700 5.356967 Hour4 5.355267 5.360000 5.317133 5.331933 5.344600 5.347400 Hour5 5.337867 5.326367 5.314967 5.313433 5.337467 5.340700 Hour6 5.343167 5.335233 5.323833 5.346267 5.333967 5.320533 We see that in this table each line corresponds to a sampling hour and each column corresponds to a sample number. We’re now going to pass this data to the control chart plotting function qcc(). As this function takes a dataset of observations so we’re removing the Hour column with the select function from tidyverse: syringe_clean &lt;- syringe_diameter %&gt;% select(-Hour) %&gt;% mutate(across(starts_with(&quot;S&quot;), round, 2)) Now we load the qcc package that has the required quality control tools: Calibration run In order to establish a control chart it is recommended to run a “calibration run.” The calibration run is used to calculate the control limits before entering “regular production.” Using the first 10 samples we call the qcc() function to make the required calculations. Mean chart library(qcc) syringe_xbar &lt;- qcc( syringe_clean[1:10, ], type = &quot;xbar&quot;, title = &quot;Lamp weight \\n xbar chart&quot;, xlab = &quot;Sample group&quot;, plot = FALSE ) Before we step ahead and simply plot the SPC chart and interpret the results lets look a bit in detail in the calculations done to established the Control Chart. To do this we’re going to go in the details of what we’ve obtained in the previous chunk. A first step is to read the begining of the qcc() help file typing ?qcc in the console. It says \"Create an object of class ‘qcc’ to perform statistical process control’ (in R technical terms function is a helper that generates an S3 R object). The key point here is that this means we can inspect the calculations separately from the plot itself. We can start by confirming the class and the type of the qcc object: class(syringe_xbar) [1] &quot;qcc&quot; typeof(syringe_xbar) [1] &quot;list&quot; It is confirmed it is an object of class qcc with the R type list. Looking into the structure of the list: str(syringe_xbar) List of 11 $ call : language qcc(data = syringe_clean[1:10, ], type = &quot;xbar&quot;, plot = FALSE, title = &quot;Lamp weight \\n xbar chart&quot;, xlab = &quot;Sample group&quot;) $ type : chr &quot;xbar&quot; $ data.name : chr &quot;syringe_clean[1:10, ]&quot; $ data : num [1:10, 1:6] 5.33 5.32 5.33 5.36 5.34 5.34 5.3 5.32 5.34 5.36 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ Group : chr [1:10] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... .. ..$ Samples: chr [1:6] &quot;Sample1&quot; &quot;Sample2&quot; &quot;Sample3&quot; &quot;Sample4&quot; ... $ statistics: Named num [1:10] 5.33 5.33 5.34 5.34 5.33 ... ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... $ sizes : int [1:10] 6 6 6 6 6 6 6 6 6 6 $ center : num 5.33 $ std.dev : num 0.0142 $ nsigmas : num 3 $ limits : num [1, 1:2] 5.32 5.35 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr &quot;&quot; .. ..$ : chr [1:2] &quot;LCL&quot; &quot;UCL&quot; $ violations:List of 2 ..$ beyond.limits : int(0) ..$ violating.runs: num(0) - attr(*, &quot;class&quot;)= chr &quot;qcc&quot; The output is not easy to read but we present it here just to show that inside the list there are several tables with the statistical analysis required for our plot. If we want like to see for instance the standard deviation we can extract it separately: syringe_xbar$std.dev [1] 0.01420679 And if we want like to see a summary of all the data stored in the object we could apply the summary method: summary(syringe_xbar) Call: qcc(data = syringe_clean[1:10, ], type = &quot;xbar&quot;, plot = FALSE, title = &quot;Lamp weight \\n xbar chart&quot;, xlab = &quot;Sample group&quot;) xbar chart for syringe_clean[1:10, ] Summary of group statistics: Min. 1st Qu. Median Mean 3rd Qu. Max. 5.325000 5.328333 5.333333 5.333333 5.337500 5.343333 Group sample size: 6 Number of groups: 10 Center of group statistics: 5.333333 Standard deviation: 0.01420679 Control limits: LCL UCL 5.315934 5.350733 We are now ready to finally we can see this all together in a plot: plot(syringe_xbar) Range chart Using the same 10 first samples we also obtain the corresponding R chart: syringe_R &lt;- qcc( syringe_clean[1:10, ], type = &quot;R&quot;, title = &quot;Lamp weight \\n R chart&quot;, xlab = &quot;Sample group&quot; ) Regular production Now that the calibration data has been plotted we can consider that the control limits are defined. They can become fixed and reused in new plots for the future production runs. Samples from those future runs can then be assessed against this limits and the control chart rules can be verified (in this example the shewhart rules are used). We now add the remaining data points to our chart by specifying which lines we’re refering too in our dataframe in the ‘newdata’ argument: syringe_xbar &lt;- qcc( data = syringe_clean[1:10, ], newdata = syringe_clean[11:25,], type = &quot;xbar&quot;, title = &quot;Lamp weight \\n xbar chart&quot;, xlab = &quot;Sample group&quot; ) We can see that the data point corresponding to the average of the measurements of the samplegroup 17 is plotted in red because it is outside of the control limits. Now we plot the R chart to assess the variability: syringe_R &lt;- qcc( data = syringe_clean[1:10, ], newdata = syringe_clean[11:25,], type = &quot;R&quot;, title = &quot;Lamp weight \\n R chart&quot;, xlab = &quot;Sample group&quot; ) In this case all the points are within the previously defined control limits. Warnings and specification limits More tight controls can be put in place by clearly identifying warning limits in a narrower range than the control limits. These measures need to be accompaigned by clear decision criteria and proper training to avoid the typical problem of overeacting and destabilizing the process by introducing unintented special causes of variation. Control limits We add warning limits in the plot with as follows: warn.limits &lt;- limits.xbar( syringe_xbar$center, syringe_xbar$std.dev, syringe_xbar$sizes, 2 ) plot( syringe_xbar, restore.par = FALSE, title = &quot;Lamp weight \\n xbar chart&quot;, xlab = &quot;Sample group&quot;) abline(h = warn.limits, lty = 3, col = &quot;chocolate&quot;) A manufacturing process under control has a variation that is lower than the product specifications and ideally it is centered. Therefore it is usually good practice to follow the control chart rules refering to the process control limits. In some cases nevertheless there may be desired or interesting to add the specification limits. This can be done as follows, first we establish the specifications: spec_max &lt;- 5.6 spec_min &lt;- 5.3 spec_tgt &lt;- (spec_max - spec_min) / 2 + spec_min specs &lt;- c(spec_min, spec_tgt, spec_max) and replot the control chart with visible specification limits and targets: plot( syringe_xbar, restore.par = FALSE, title = &quot;Lamp weight \\n xbar chart&quot;, xlab = &quot;Sample group&quot;, ylim = c(specs[1], specs[3]) ) abline(h = specs, lty = 3, col = &quot;red&quot;) In the previous example we see a situation that happens in practice and that requires action: the data plotted is still within the min max specification limits for this relativelly small number of data points. Furthermore the variation is overall well contained within the process limits. Nevertheless we see it is extremelly off centered when compared with the product specification. A process capability study should help determining the causes for this offcentering and help correcting it. Adapted from Bass (2007) In this chapter we’re going to go more in depth in the study of the manufacturing process variability. We’re going to make a comparison between the product specifications and the process variability. We’re looking for opportunities to tigthen the product specifications. Tightening a product specification without increasing the cost of a manufacturing cost can be a source of competitive advantage. Cpk charts Off specification syringe_long &lt;- syringe_diameter %&gt;% pivot_longer(cols = starts_with(&quot;Sample&quot;), names_to = &quot;sample&quot;, values_to = &quot;value&quot;) variables syringe_mean = syringe_long %&gt;% pull(value) %&gt;% mean() syringe_sd = syringe_long %&gt;% pull(value) %&gt;% sd() syringe_n &lt;- length(syringe_long) theor_n = 1000000 calculation: probability of being between the limits off_spec &lt;- function(UCL, LCL, mean, sd) { round(100 - ((stats::pnorm(UCL, mean, sd) - stats::pnorm(LCL, mean, sd))*100), 2) } syringe_off_spec &lt;- off_spec(spec_max, spec_min, syringe_mean, syringe_sd) syringe_theor &lt;- rnorm(n = theor_n, mean = syringe_mean, sd = syringe_sd) %&gt;% as_tibble() plot_subtitle &lt;- paste( &quot;Spec: [&quot;, spec_min, &quot;;&quot;, spec_max, &quot;], Proportion off-spec = &quot;, signif(syringe_off_spec, digits = 2), &quot;%&quot; ) Note that we deliberately twick the plot colors to make it look like the plots from minitab and from the qcc package. We provide this theme in the book companion package industRial with the name theme_qcc. syringe_long %&gt;% ggplot(aes(x = value, y = ..density..)) + geom_histogram( bins = 30, fill = &quot;white&quot;, color = &quot;grey20&quot;) + geom_density(data = syringe_theor, linetype = 2) + geom_vline(xintercept = {spec_min}, color = &quot;red&quot;, linetype = 3) + geom_vline(xintercept = {spec_max}, color = &quot;red&quot;, linetype = 3) + geom_vline(xintercept = {spec_tgt}, color = &quot;red&quot;, linetype = 2) + scale_x_continuous(n.breaks = 10) + theme_qcc() + labs( title = &quot;Out of specification (Expected)&quot;, subtitle = {plot_subtitle}) By looking at the histogram of the Bamako lightning dataset we confirm the extreme offcentering of the production. We also see that although there are no measurements beyond the lower specification limit (LSL) it is very likely this will happen soon. We can also calculate the Cpk Process Capability process_Cpk &lt;- function(UCL, LCL, mean, sd) { pmin( (abs(mean - abs(LCL)) / (3 * sd)), (abs((abs(UCL) - mean)) / (3 * sd)) ) } process_Cpk(spec_max, spec_min, syringe_mean, syringe_sd) [1] 0.7158694 And convert the percentage out of spec in parts per million. We’re not considering the 1.5 shift that sometimes is presented in the literature but rather making a simple direct conversion of the proportion out of spec found before: formatC(((syringe_off_spec) * 10000), format = &quot;d&quot;, big.mark = &quot;&#39;&quot;) [1] &quot;15&#39;900&quot; The expected population below the LSL is 1,3% which is very high for industry standards. In fact this corresponds to 15’900 parts per million (ppm) whereas a common target would be 1 ppm. Naturally these figures are indicative and they depend of the context criteria such as severity of the problem, cost, difficulty to eliminate the problem and so on. We can now establish a simple table using the functions created before, to present the expected percentage that falls within certain limits. To make it useful as a reference table we’re putting this limits from \\(\\pm\\) 1 to \\(\\pm\\) 6 standard deviations Sigma conversion table sigma_limits &lt;- tibble( sigma_plus = c(1, 2, 3, 4, 5, 6), sigma_minus = -sigma_plus, mean = 0, sd = 1 ) sigma_limits %&gt;% mutate( off_spec_perc = off_spec(sigma_plus, sigma_minus, mean, sd), in_spec_perc = 100 - off_spec_perc, Cpk = process_Cpk(sigma_plus, sigma_minus, mean, sd), ppm_defects = formatC( off_spec(sigma_plus, sigma_minus, mean, sd) * 10000, format = &quot;d&quot;, big.mark = &quot;&#39;&quot;)) %&gt;% select(sigma_minus, sigma_plus, off_spec_perc, in_spec_perc, Cpk, ppm_defects) %&gt;% kable(align = &quot;c&quot;, digits = 3) sigma_minus sigma_plus off_spec_perc in_spec_perc Cpk ppm_defects -1 1 31.73 68.27 0.333 317’300 -2 2 4.55 95.45 0.667 45’500 -3 3 0.27 99.73 1.000 2’700 -4 4 0.01 99.99 1.333 100 -5 5 0.00 100.00 1.667 0 -6 6 0.00 100.00 2.000 0 Capability chart syringe_cpk &lt;- process.capability( syringe_xbar, breaks = 10, spec.limits = c(spec_min, spec_max), target = spec_tgt, digits = 2, print = FALSE, std.dev = syringe_sd ) A fine tuning of the forecast of the number of expected parts out of specification can be done with the parameter std.dev. The input value will be used in the probability distribution function. Different approaches can be considered: calculating the sandard deviation within each subgroup or the standard deviation of the entire population and also correcting the standard deviation dividing by n or by n - 1. In this example we re-use the standard deviation calculated on the entire set of datapoints as the group is small but for a case with more data it would be interesting to used the subgroups that tend to give smaller standard deviations. I-MR charts In this final chapter we’re exploring the development of custom functions for summary statistics and timeseries plotting. All these functions are available on the book companion package {industRial} for exploration and further development. They don’t pretend to be used as such for real life applications. For that we recommend the functions from the package {QCC} presented before. The objective here is to show a workflow and demonstrate some possibilities that the {tidyverse} offers to make completely customized functions. To encourage this exploration we’re not presenting here the complete code for each function but propose to check it with the R functionality for function code exploration. We see often the recommendation to read R source code and we can only support it as an excellent way to develop our skilset. Lets start with the simple function that calculates the percentage of parts out of specification given the specification limits, the process mean and standard deviation. This function was presented in the previous case study and since it is loaded in memory we can read its content with the R function body(): body(off_spec) { round(100 - ((stats::pnorm(UCL, mean, sd) - stats::pnorm(LCL, mean, sd)) * 100), 2) } we can see that it uses simple functions from the package {stats}. We can also explicitly request to see the arguments it takes with formals(): dput(formals(off_spec)) as.pairlist(alist(UCL = , LCL = , mean = , sd = )) and for a complete review we can open the function help page with: ?off_spec Lets give some data and use the function: off_spec(0.981, 0.819, 0.943, 0.019) [1] 2.28 we get 2.28% parts out of spec. We’ll see this calculation in action in a moment. Process statistics The tablet weight control procedure tablet_weight &lt;- tablet_weight %&gt;% janitor::clean_names(case = &quot;snake&quot;) We’re now going to use the function process stats to calculate several statistical data for this dataset. As mentionned we encourage the reader to explore the code with body(process_stats) and dput(formals(process_stats)) as there is a wealth of details in how to calculate process control limits, moving ranges and the like. weight_statistics_data &lt;- process_stats(tablet_weight, 10) this being done we can now convert this data into an easy readable format for reporting of for a future integration in a shiny app for example. We’re exploring the package {gt} that has a specific very neat look rather different from the {kable} package used in most of the book. process_stats_table(weight_statistics_data) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #gxrumaxcvz .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #gxrumaxcvz .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gxrumaxcvz .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #gxrumaxcvz .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #gxrumaxcvz .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gxrumaxcvz .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gxrumaxcvz .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #gxrumaxcvz .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #gxrumaxcvz .gt_column_spanner_outer:first-child { padding-left: 0; } #gxrumaxcvz .gt_column_spanner_outer:last-child { padding-right: 0; } #gxrumaxcvz .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #gxrumaxcvz .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #gxrumaxcvz .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #gxrumaxcvz .gt_from_md > :first-child { margin-top: 0; } #gxrumaxcvz .gt_from_md > :last-child { margin-bottom: 0; } #gxrumaxcvz .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #gxrumaxcvz .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #gxrumaxcvz .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gxrumaxcvz .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #gxrumaxcvz .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gxrumaxcvz .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #gxrumaxcvz .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #gxrumaxcvz .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gxrumaxcvz .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gxrumaxcvz .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #gxrumaxcvz .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gxrumaxcvz .gt_sourcenote { font-size: 90%; padding: 4px; } #gxrumaxcvz .gt_left { text-align: left; } #gxrumaxcvz .gt_center { text-align: center; } #gxrumaxcvz .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #gxrumaxcvz .gt_font_normal { font-weight: normal; } #gxrumaxcvz .gt_font_bold { font-weight: bold; } #gxrumaxcvz .gt_font_italic { font-style: italic; } #gxrumaxcvz .gt_super { font-size: 65%; } #gxrumaxcvz .gt_footnote_marks { font-style: italic; font-size: 65%; } Process Summary Statistics Variable Value Unit Weight mean 0.9400000 g Spec target 0.9000000 g Spec min 0.8100000 g Spec max 0.9900000 g Out of spec 0.7100000 % Cpk 0.8178525 Sample size 137 parts Individual chart The data set being available we’re feeding it into the chart_I() function: chart_I(weight_statistics_data) Moving range chart The companion of the I chart is the MR chart, where MR stands for moving range. This chart can be called with: chart_IMR(weight_statistics_data) Capability chart (custom) And a final chart for this session the capability chart: chart_Cpk(weight_statistics_data) "],
["contents.html", "Index", " Index Subject Unit Functions Datasets DFSS Pareto Pareto chart qicharts2::paretochart dial_control Ishikawa Fishbone diagram qcc::cause.and.effect Correlation Matrix perfume_experiment Tileplot ggplot2::geom_tile Clustering Network plot ggraph::ggraph perfume_experiment MSA Calibration Bias plot ggplot2::geom_smooth juice_drymatter Bias report Precision Gage r&amp;R SixSigma::ss.rr tablet_thickness Gage acceptance Uncertainty Uncertainty tablet_thickness DOE Direct comparisons Histogram ggplot2::geom_histogram pet_delivery t-test one sample stats::t.test Normality plot ggplot2::geom_qq F test stats::var.test Levene test car::leveneTest Linear regression Linear model stats::lm ebike_hardening Contrasts treatment Predict stats::predict Model augment broom::augment Timeseries plot Autocorrelation test car::durbinWatsonTest Residuals-Fit plot Homocedasticity stats::bartlett.test Normality test stats::shapiro.test Standard Residuals-Fit plot Outliers test car::outlierTest Cooks distance Coeficient of determination base::summary()$r.squared Anova &amp; Ancova Analysis of variance stats::aov Pairwise comparison stats::TukeyHSD Least significant difference agricolae::LSD.test Model formulae stats::formula solarcell_output Interaction plot stats:interaction.plot Simplified timeseries plot(model$residuals) Residuals summary plot(model) Cooks histogram plot(model, which = 4) Anova check stats::anova Correlation test stats::cor.test solarcell_fill Analysis of covariance stats::aov General designs Factorial design DoE.base::fac.design juice_drymatter Main effects plots Interactions plots (custom) ggplot2::geom_errorbar Anova 3rd level interactions Two level designs Coding levels stats::relevel pet_doe stats::contrasts Interaction plots with SE RcmdrMisc::plotMeans Adjusted R-square broom::glance battery_charging Coding inputs Coding prediction Perpective plot graphics::persp Single replicate designs Effects normal plot car::qqPlot Effects contribution table boom::tidy SPC Xbar-R charts Mean chart qcc::qcc syringe_diameter Range chart qcc::qcc Control limits qcc::limits.xbar Cpk charts Off specification industRial::off_spec syringe_diameter Process capability industRial::process_Cpk Sigma conversion table Capability chart qcc::process.capability I-MR charts Process statistics tablet_weight Individual chart industRial::chart_I Moving range chart industRial::chart_IMR Capability chart (custom) industRial::chart_Cpk "],
["glossary.html", "Glossary Statistics DFSS MSA DOE SPC", " Glossary Statistics Statistic concepts are picked up and applied throught the Cases Studies on a needed basis. To get a better understanding of how they fit together we are reminding below some definitions coming from Yakir (2011). For a deep and comprehensive course on statistics we recommend the free online kahn academy courses. Notation conventions The arithmetic mean of a series of values x1, x2, …, xn is often denoted by placing an “overbar” over the symbol, e.g. \\(\\bar{x}\\) , pronounced “x bar.” Some commonly used symbols for sample statistics are: sample mean \\(\\bar{x}\\), sample standard deviation s. Some commonly used symbols for population parameters: population mean μ, population standard deviation σ. Random variables are usually written in upper case roman letters: \\(X, Y\\), etc. Particular realizations of a random variable are written in corresponding lower case letters. For example, x1, x2, …, xn could be a sample corresponding to the random variable \\(X\\). A cumulative probability is formally written P(\\(X\\)≤x) to differentiate the random variable from its realization. Greek letters (e.g. θ, β) are commonly used to denote unknown parameters (population parameters). Placing a hat, or caret, over a true parameter denotes an estimator of it, e.g., \\(\\hat{θ}\\) is an estimator for θ. Descriptive statistics Statistic: A numerical characteristic of the data. A statistic estimates the corresponding population parameter. Population: The collection, or set, of all individuals, objects, or measurements whose properties are being studied. Sample: A portion of the population understudy. A sample is representative if it characterizes the population being studied. Frequency: The number of times a value occurs in the data. Relative Frequency: The ratio between the frequency and the size of data. \\(f / n\\) Median: A number that separates ordered data into halves. Mean: A number that measures the central tendency. A common name for mean is ‘average.’ Sample size: \\(n\\) Sample mean: \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\sum_{x}(x\\times f_x / n)\\) Population size: \\(N\\) Population mean: \\(\\bar{\\mu} = \\frac{\\sum_{i=1}^{N}x_i}{N}\\) Variance: Mean of the squared deviations from the mean. Sample variance: \\(s^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})^2 = \\frac{n}{n-1}\\sum_{x}((x-\\bar{x})^2\\times(f_x/n))\\) Standard Deviation: A number that is equal to the square root of the variance and measures how far data values are from their mean. Sample standard deviation: \\(\\sqrt[]{s^{2}}\\) Probability Random Variable: The probabilistic model for the value of a measurement, before the measurement is taken (e.g. Binomial, Poisson, Uniform, Exponential, Normal). It is denoted with latin capitals \\(X, Y\\) and \\(Z\\) Expectation: The central value for a random variable. The expectation of the random variable X is marked by E(\\(X\\)). Variance: The (squared) spread of a random variable. The variance of the random variable X is marked by Var(\\(X\\)). Normal Random Variable: A bell-shaped distribution that is frequently used to model a measurement. The distribution is marked with Normal(\\(\\mu,\\sigma^2\\)). Standard Normal Distribution: The Normal(0,1). The distribution of stan- dardized Normal measurement. Percentile: Given a percent p · 100% (or a probability p), the value x is the percentile of a random variable X if it satisfies the equation P\\((X ≤ x) = p\\). Sampling distribution Random Sample: The probabilistic model for the values of a measurements in the sample, before the measurement is taken. Sampling Distribution: The distribution of a random sample. Sampling Distribution of a Statistic: A statistic is a function of the data; i.e. a formula applied to the data. The statistic becomes a random variable when the formula is applied to a random sample. The distribution of this random variable, which is inherited from the distribution of the sample, is its sampling distribution. Sampling Distribution of the Sample Average: The distribution of the sample average, considered as a random variable. The Law of Large Numbers: A mathematical result regarding the sampling distribution of the sample average. States that the distribution of the av- erage of measurements is highly concentrated in the vicinity of the expec- tation of a measurement when the sample size is large. The Central Limit Theorem: A mathematical result regarding the sampling distribution of the sample average. States that the distribution of the average is approximately Normal when the sample size is large. (note: the central limit theorem is a key notion for understanding industrial measurement and its consequences will be applied in most case studies) Expectation of the sample average: the expectation of the sample mean is equal to the theoretical expectation of its components E\\((\\bar{X})\\) = E(\\(X\\)) Variance of the sample average: the variance of the sample average is equal to the variance of each of the components, divided by the sample size Var(\\(X\\)) = Var\\((X)/n\\) Statistical Inference Statistical Inference: Methods for gaining insight regarding the population parameters from the observed data. Point Estimation: An attempt to obtain the best guess of the value of a population parameter. An estimator is a statistic that produces such a guess. The estimate is the observed value of the estimator. Confidence Interval: An interval that is most likely to contain the population parameter. The confidence level of the interval is the sampling probability that the confidence interval contains the parameter value. Hypothesis Testing: A method for determining between two hypothesis, with one of the two being the currently accepted hypothesis. A determination is based on the value of the test statistic. The probability of falsely rejecting the currently accepted hypothesis is the significance level of the test. Comparing Samples: Samples emerge from different populations or under different experimental conditions. Statistical inference may be used to compare the distributions of the samples to each other. Regression: Relates different variables that are measured on the same sample. Regression models are used to describe the effect of one of the variables on the distribution of the other one. The former is called the explanatory variable and the later is called the response. Bias: The difference between the expectation of the estimator and the value of the parameter. An estimator is unbiased if the bias is equal to zero. Otherwise, it is biased. Mean Square Error (MSE): A measure of the concentration of the distribu- tion of the estimator about the value of the parameter. The mean square error of an estimator is equal to the sum of the variance and the square of the bias. If the estimator is unbiased then the mean square error is equal to the variance. Confidence Level: The sampling probability that random confidence intervals contain the parameter value. The confidence level of an observed interval indicates that it was constructed using a formula that produces, when applied to random samples, such random intervals. Null Hypothesis (\\(H0\\)): A sub-collection that emerges in response to the sit- uation when the phenomena is absent. The established scientific theory that is being challenged. The hypothesis which is worse to erroneously reject. Alternative Hypothesis (\\(H1\\)): A sub-collection that emerges in response to the presence of the investigated phenomena. The new scientific theory that challenges the currently established theory. Test Statistic: A statistic that summarizes the data in the sample in order to decide between the two alternative. Rejection Region: A set of values that the test statistic may obtain. If the observed value of the test statistic belongs to the rejection region then the null hypothesis is rejected. Otherwise, the null hypothesis is not rejected. Type I Error: The null hypothesis is correct but it is rejected by the test. Type II Error: The alternative hypothesis holds but the null hypothesis is not rejected by the test. Significance Level: The probability of a Type I error. The probability, com- puted under the null hypothesis, of rejecting the null hypothesis. The test is constructed to have a given significance level. A commonly used significance level is 5%. Statistical Power: The probability, computed under the alternative hypoth- esis, of rejecting the null hypothesis. The statistical power is equal to 1 minus the probability of a Type II error. \\(p\\)-value: A form of a test statistic. It is associated with a specific test statistic and a structure of the rejection region. The p-value is equal to the signif- icance level of the test in which the observed value of the statistic serves as the threshold. DFSS One way of summarising the Six Sigma framework is presented below in a step by step approach with definitions. Each steps consists of an analyis of the product development and production process that progressively refines the final product specifications. For a more detailed description we recommend reviewing the comprehensive Six Sigma certification reference book by Roderik A.Munro and J.Zrymiak (2015). Voice of Customer 1. Product brief List of the product features expected by the customer (internal or external), including qualitative indication of the acceptance limits. 2. Functional analysis Translation of the product attributes into lower level functions including interactions between product components and requirements induced by each component on the others. 3. Failure modes and effects analysis (FMEA) List of critical product features with failure causes, effects, detection and action plans, rated and sorted by criticality. 4. Product specifications and parts drawings Implementation of the product components into unique formulations and drawings including detailed values and tolerances of it characteristics (physical, chemical or electric or others). Voice of Process 1. Process mapping A visual diagram of the production process with inputs and outputs for each step. 2. Process FMEA List of critical production process steps with failure causes, effects, detection and action plans, rated and sorted by criticality. 3. Quality Control plan List of control points including measurement method, sample size, frequency and acceptance criteria. When needed, critical control points are handled by Statistical Process Control (SPC) 4. Measurement system analysis A thorough assessment of a measurement process, and typically includes a specially designed experiment that seeks to identify the components of variation in that measurement process. 5. Process capability analysis Comparison of the variability of a production process with its engineered specifications. MSA It is a fact that different communities utilize different methodologies and terminologies on the domain of measurement uncertainty. Unfortunately these differences are still too often overlapping, see J E Muelaner (2015) for detailed comparison. In our text we opt for the industry terminology, in particular to the norm ISO 5725, the practical application guides from Automotive Industry Action Group (2010) and some articles on Minitab (2019b) which itself is based on the AIAG guidelines. Variance components assess the amount of variation contributed by each source of measurement error, plus the contribution of part-to-part variability. The sum of the individual variance components equals the total variation. total gage r&amp;R: the sum of the repeatability and the reproducibility variance components. part: The variation that comes from the parts, with 5 levels in this case. operator: The variation that comes from the operators, with 3 levels in this case. replicants, n: number of replications corresponding to the number of times each part is measured by each operator. repeatability (or error, or residuals): The variation that is not explained by part, operator, or the operator and part interaction. It represents how much variability is caused by the measurement device (the same operator measures the same part many times, using the same gage). The repeatability can be measured directly from the Anova table from the residual mean squares. reproducibility: how much variation is caused by the differences between operators (different operators measure the same part many times, using the same gage). operators: the operators part of the reproducibility is the operators variation minus the interaction divided by the number of different parts times the replicants (zero if negative). parts:operators: The variation that comes from the operator and part interaction. An interaction exists when an operator measures different parts differently. The interaction part of of the reproducibility is the interaction minus the repeatability divided by the number of replicants (zero if negative). part-to-part: the variability due to different parts. Ideally, very little should be due to repeatability and reproducibility. Differences between parts should account for most of the variability (when the %Contribution from part-to-part variation is high, the measurement system can reliably distinguish between parts). The sum of the individual variance components equals the total variation. Accuracy/Uncertainty: Combination of precision and trueness. In the ISO 5725 both terms are equivalent. Precision: Combination of repeatability and reproducibility Trueness: difference between the mean of many measurements and the reference value. In ISO 5725 the term bias has been replaced by trueness. DOE Below key definitions from Montgomery (2012), complemented with Wikipedia article details on the same topics. Randomization: both the allocation of the experimental material and the order in which the individual runs of the experiment are to be performed are randomly determined. Run: unique combination of the input factors. Replicate: independent repeat of a run. Experiment: series of runs. Factorial design: in each complete trial or replicate of the experiment all possible combinations of the levels of the factors are investigated. Crossed factors: factors arranged in a factorial design. Coded variable: the \\(\\pm\\) 1 coding for the low and high levels of the factors. Coded variables are very effective for determining the relative size of factor effects. In almost all situations, the coded unit analysis is preferable. Contrast: a linear combination of parameters in the form \\(\\tau=\\sum_{i=1}^{a}c_i\\mu_i\\) where the contrast constants \\(c_1,c_2, ..., c_a\\) sum to zero; that is, \\(\\sum_{i=1}^{a}c_i=0\\). Orthogonal contrasts: two contrasts with coefficients \\({c_i}\\) and \\({d_i}\\) are orthogonal if: \\(\\sum_{i=1}^{a}c_id_i\\). In a balanced one-way analysis of variance, using orthogonal contrasts has the advantage of completely partitioning the treatment sum of squares into non-overlapping additive components that represent the variation due to each contrast. Contrasts then allow for the comparison between the different means. Sparsity of effects principle: states that most systems are dominated by some of the main effects and low-order interactions, and most high-order interactions are negligible. SPC Process capability: is the ability of a manufacturing process to produce an output within the product specification limits. Process Capability Index: a statistical measure of the process capability. Different indexes have been defined: Cp, Cpk, Cpm, Cpkm. "],
["references.html", "References", " References A good mastership of the vast domain of Industrial Data Science can take several years and can only be obtained by a strong combination of theory and practice. As mentionned in the introduction chapter, our book is focused on the practice and in this bibliography we find some the necessary supporting theory. The list below collects websites, books and articles referenced throughout this book. It is a curated set of some of the most relevant works available today in Six Sigma, Statistics, Data Science and programming with R. Automotive Industry Action Group, AIAG -, ed. 2010. https://www.aiag.org/store/publications/details?ProductCode=MSA-4. Bass, Issa. 2007. Six Sigma Statistics with Excel and Minitab. 1st ed. McGraw-Hill. Bell, Stéphanie. 2001. The Beginers Guide to Uncertainty of Measurement. 2nd ed. Teddington, Middlesex, United Kingdom: National Phisical Laboratory. https://www.npl.co.uk/special-pages/guides/gpg11_uncertainty. Broc, Guillaume. 2016. Stats Faciles Avec r. 1th ed. deBoeck. Emilio L. Cano, Andrés Redchuk, Javier M. Moguerza. 2012. Six Sigma with r. 1th ed. Vol. 36. Use r! Springer New York Heidelberg Dordrecht London: Springer. Emilio L. Cano • Javier M. Moguerza, Mariano Prieto Corcoba. 2015. Quality Control with r. 1th ed. Use r! Springer New York Heidelberg Dordrecht London: Springer. Fox, John, and Sanford Weisberg. 2019. An r Companion to Applied Regression. 3rd edition. Thousand Oaks CA: SAGE publishing. J E Muelaner, M Chappell, A Francis. 2015. “A Hybrid Measurement Systems Analysis and Uncertainty of Measurement Approach for Industrial Measurement in the Light Controlled Factory.” Laboratory for Integrated Metrology Applications, Dep. Of Mechanical Engineering, University of Bath, Bath, UK. Minitab. 2016. “Understanding Analysis of Variance (ANOVA) and the f-Test.” https://blog.minitab.com/en/adventures-in-statistics-2/understanding-analysis-of-variance-anova-and-the-f-test. ———. 2019a. “Interpret the Key Results for 2 Variances.” https://support.minitab.com/en-us/minitab/18/help-and-how-to/statistics/basic-statistics/how-to/2-variances/interpret-the-results/key-results/. ———. 2019b. “What Is a Gage Repeatability and Reproducibility (r&amp;r) Study?” https://support.minitab.com/en-us/minitab/18/help-and-how-to/quality-and-process-improvement/measurement-system-analysis/supporting-topics/gage-r-r-analyses/what-is-a-gage-r-r-study/. Montgomery, Douglas C. 2012. Design and Analysis of Experiments. 8th ed. Wiley. Roderik A.Munro, Govindarajan Ramu, and Daniel J.Zrymiak. 2015. The Certified Six Sigma Green Belt Handbook. 2nd ed. Milwaukee, Winsconsin: ASQ Quality Press. Scrucca, Luca. 2004. “Qcc: An r Package for Quality Control Charting and Statistical Process Control.” R News 4/1: 11–17. Yakir, Benjamin. 2011. Introduction to Statistical Thinking (with r, Without Calculus). Department of Statistics, The Hebrew University of Jerusalem. "],
["imprint.html", "Imprint", " Imprint Many packages are available for editing documentation, from notes to blogs up to complete websites. In this book we’ve opted to use the R package {Bookdown} from Yihui Xie further customized with a layout developed by Matthew J. C. Crump. An important aspect to ensure reproducibility of the examples along the time and between users is to have the same programming setup. We’re showing below our setup at the time of rendering the book. devtools::session_info()[[1]] ## Error in get(genname, envir = envir) : objet &#39;testthat_print&#39; introuvable ## setting value ## version R version 4.0.2 (2020-06-22) ## os Windows 10 x64 ## system x86_64, mingw32 ## ui RTerm ## language (EN) ## collate French_Switzerland.1252 ## ctype French_Switzerland.1252 ## tz Europe/Berlin ## date 2021-05-31 Disclaimer This book presents a variety of software tools and recommended approaches for industrial data analysis. It is incumbent upon the user to execute judgment in their use. The author does not provide any guarantee, expressed or implied, with regard to the general or specific applicability of the software, the range of errors that may be associated with it, or the appropriateness of using them in any subsequent calculation, design, or decision process. The author accepts no responsibility for damages, if any, suffered by any reader or user of this handbook as a result of decisions made or actions taken on information contained therein. Licence This book and its companion package are made available under a GPLv3 licence granting end users the freedom to run, study, share, and modify the software. "]
]
