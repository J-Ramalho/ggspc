[
["index.html", "industRial data science Case studies in product development and manufacturing Welcome", " industRial data science Case studies in product development and manufacturing João Ramalho 2021-05-21 Welcome This is the online version of industRial data science, a book on Data Science tools and techniques for Product Development and Manufacturing. It is organized around Case Studies in a “cookbook” approach, making it easier to directly adopt the tools. The examples come from varied manufacturing industries, mostly where repetitive production in massive quantities is involved, including: pharmaceuticals, food, electronics, watch making and automotive. Product Development and Manufacturing are very important activities in society because bringing innovative products to the market has an immense potential to improve the quality of life of everyone. Additionally Data Science brings new powerful approaches to the engineering and manufacturing of consumer goods, helping minimizing environmental impact, improving quality and keeping costs under control. How to use this book We assume the reader is familiar with product development and manufacturing quality methodologies such as dmaic and six sigma and the associated statistical concepts. Furthermore it is considered that he brings at least a beginner knowledge on R. The book focus is on putting these areas together. Being a collection of case studies, the book is better used as a reference book. The navigation bar on the left is organized by domain and a full list of the Case Studies, Datasets and R functions is available in the section Table of contents. Complementing the text, a series of tutorials can be accessed either on line or locally to practice dynamically key statistical concepts. For the online option no specific software installation is required. A list of web links and detailed instructions on local installation can be seen in the section Tutorials. To go even further all the case studies data sets, selected example functions and the textbook original files can be downloaded as a package called {industRial}. For guidelines on how to use them refer to the sections Datasets and Functions Finally we refer to several good quality books on both Data Science and Product Development that have served to provide the required theoretical background. These cover key disciplines such as six sigma, statistics and computer programming. This book aims complementing them and showcase how to benefit from recent software in this area. A full list can be found in References. Content overview The case studies are organized according the a logical product development flow. The text starts with case studies in the domain of Design for Six Sigma. These are some practical tools in that help prioritizing problems and get focus on how to tackle them. The next group of case studies covers the domain of Measurement System Analysis, an initial important step when developing a product or manufacturing process. Here is discussed how to analyze the response of a measurement device in terms of its bias and its uncertainty. The next large group of case studies is the Design of Experiments. This corresponds to the core of the R&amp;D activities and provides approaches to minimize the quantity of trials and time to reach to a sufficient knowledge of how the product or system works. Also on how to obtain the right balance on its features and properties to obtain the desired output. A final group of case studies presents ways to get the manufacturing process in control according to what was defined in the product development phase. These are the well known Statistical Process Control and Capability studies. Acknowledgements I would like to express my gratitude to the instructors and colleagues who have spent time sharing their knowledge, answering my questions and giving me inputs: Enrico Chavez, Iegor Rudnytskyi, Giulia Ruggeri, Harry Handerson and Bobby Stuijfzand from the EPFL ADSCV team; Jean-Vincent Le Bé, Jasmine Petry, Yvan Bouza, James Clulow and Akos Spiegel from the Nestlé STC team; Frank Paris from DOQS; Théophile Emmanouilidis and Sélim Ach from Thoth. To report any issue or make suggestions please open an issue on the book repository: industRialds/issues About the authors João Ramalho is a Senior Industrial Data Scientist with more than 20 years of experience in the manufacturing industry. He’s been in varied positions in R&amp;D, Operations and IT at Philip Morris, Rolex and Nestlé. He holds a Master in Mechanical Engineering from the IST of Lisbon, a PMP certification from the Project Management Institute and a Data Science certification from DataCamp. He’s currently specializing in Data Visualization at the Swiss technical university EPFL. See full profile at j-ramalho.github.io "],
["contents.html", "Table of Contents", " Table of Contents Chapter Unit Dataset Functions DFSS Pareto dial_control qicharts2::paretochart Ishikawa qcc::cause.and.effect Correlation matrix perfume_experiment ggplot2::geom_tile Clustering perfume_experiment MSA Linearity juice_drymatter ggplot2::geom_smooth Precision tablet_thickness SixSigma::ss.rr Uncertainty tablet_thickness DOE Simple experiments pet_delivery stats::t.test stats::var.test car::leveneTest Regression and anova ebike_hardening stats::lm broom::augment car::durbinWatsonTest stats::bartlett.test stats::shapiro.test stats::aov stats::TukeyHSD agricolae::LSD.test stats::predict Interactions solarcell_output car::outlierTest stats:interaction.plot Covariance solarcell_fill stats::cor.test stats::aov – ancova General factorial designs juice_drymatter DoE.base::fac.design Two level designs pet_doe stats::relevel stats::contrasts RcmdrMisc::plotMeans battery_charging broom::glance graphics::persp car::qqPlot SPC Xbar-R charts syringe_diameter qcc::qcc – xbar qcc::qcc – R qcc::limits.xbar Cpk charts syringe_diameter qcc::process.capability I-MR charts tablet_weight industRial::off_spec "],
["r-packages.html", "R Packages six sigma qcc qicharts2 DoE.base car", " R Packages Why R? Many tools exist to do Data Analysis and Statistics with different degrees of power and difficulty such as: Spreadsheets: Excel, Libreoffice, Numbers Proprietary software: Minitab, Mathlab Programming languages: Visual Basic, R, Python, Julia Databases: sqlite, postgre, mysql, mongodb Choosing the right set of tools for Data Science is often not a very scientific task. Mostly is a matter of what is available and what our colleagues, customers or suppliers use. As with everything it is important to remain open to evaluate new tools and approaches and even to be able to combine them. In this book we’ve chosen to provide all examples in R which is a free software environment for statistical computing and graphics https://www.r-project.org/ Besides taste and personnal preference R brings a significant number of specific advantages in the field of Industrial Data Science: R allows for reproducible research This because the algorithms and functions defined to make the calculations can be inspected and all results can be fully reproduced and audited. This is known as reproducible research and is a critical aspect in all areas where a proof is needed such as in equipment validation and product quality reporting. R functions, tools and programs can be adapted and improved Being an open source language, all R libraries and packages added to the basic environment can be not only audited but adapted and improved This is very important as when we enter into details every industry has a slight different way of doing things, different naming conventions, different coeficients and so on. R is extensible R is compatible with most other software on the market and is an excellent “glue” tool allowing for example for data loading from excel files, producing reports in pdf and even building complete dashboards in the form of web pages. All tools applied throughout this book are available in the form of packages of the programming language R. As with all open source code, they’re all available for download with freedom to modification and at no cost. The amount of packages available is extremely large and growing very fast. When selecting new packages it is recommended to check the latest package update. Packages that have had no improvements since more than a couple of years should be questionned. The field evolves rapidly and compatibility and other issues can become painfull. A way to obtain statistics on package history is on metacran or RStudio package manager. Below a comprehensive list of the different packages used in the book: Package Area Version Depends License rcartocolor colors 2.0.0 R (&gt;= 3.4) MIT + file LICENSE viridis colors 0.5.1 R (&gt;= 2.10), viridisLite (&gt;= 0.3.0) MIT + file LICENSE datasets data 4.0.2 NA Part of R 4.0.2 readr data loading 1.3.1 R (&gt;= 3.1) GPL (&gt;= 2) | file LICENSE readxl data loading 1.3.1 NA GPL-3 dplyr data science 1.0.2 R (&gt;= 3.2.0) MIT + file LICENSE forcats data science 0.5.0 R (&gt;= 3.2) GPL-3 hms data science 0.5.3 NA GPL-3 janitor data science 2.0.1 R (&gt;= 3.1.2) MIT + file LICENSE lubridate data science 1.7.9 methods, R (&gt;= 3.2) GPL (&gt;= 2) magrittr data science 1.5 NA MIT + file LICENSE tibble data science 3.0.3 R (&gt;= 3.1.0) MIT + file LICENSE tidyr data science 1.1.1 R (&gt;= 3.1) MIT + file LICENSE broom datascience 0.7.0 R (&gt;= 3.1) MIT + file LICENSE tidyverse datascience 1.3.0 R (&gt;= 3.2) GPL-3 | file LICENSE purrr functional programming 0.3.4 R (&gt;= 3.2) GPL-3 | file LICENSE qicharts2 industrial stats 0.7.1 R (&gt;= 3.0.0) GPL-3 qcc industrial stats 2.7 R (&gt;= 3.0) GPL (&gt;= 2) SixSigma industrial stats 0.9-52 R (&gt;= 2.14.0) GPL (&gt;= 2) DoE.base industrial stats 1.1-6 R (&gt;= 2.10), grid, conf.design GPL (&gt;= 2) rsm industrial stats 2.10.2 NA GPL (&gt;= 2) patchwork plotting 1.0.1 NA MIT + file LICENSE ggplot2 plotting 3.3.2 R (&gt;= 3.2) GPL-2 | file LICENSE scales plotting 1.1.1 R (&gt;= 3.2) MIT + file LICENSE bookdown publishing 0.20 NA GPL-3 knitr publishing 1.29 R (&gt;= 3.2.3) GPL stats statistics 4.0.2 NA Part of R 4.0.2 tidymodels statistics NA NA NA agricolae statistics 1.3-3 R (&gt;= 2.10) GPL car statistics 3.0-9 R (&gt;= 3.5.0), carData (&gt;= 3.0-0) GPL (&gt;= 2) RcmdrMisc statistics 2.7-1 R (&gt;= 3.5.0), utils, car (&gt;= 3.0-0), sandwich GPL (&gt;= 2) lsr statistics 0.5 NA GPL-3 fs sysadmin 1.5.0 R (&gt;= 3.1) GPL-3 glue text 1.4.1 R (&gt;= 3.1) MIT + file LICENSE stringr text 1.4.0 R (&gt;= 3.1) GPL-2 | file LICENSE Theses packages below are loaded implicitly and not visible in the book text. Be sure to load at minimum the packages below before trying any example: ds_pkgs &lt;- c(&quot;tidyverse&quot;, &quot;scales&quot;, &quot;janitor&quot;, &quot;knitr&quot;, &quot;stats&quot;, &quot;industRial&quot;, &quot;viridis&quot;, &quot;broom&quot;, &quot;patchwork&quot;) purrr::map(ds_pkgs, library, character.only = TRUE) A common issue in R when compared to python for instance is function masking. As we tend to load all the sets of functions from each package we end up with conflicting function names. In the scope of this text it is mostly the function filter() from {dplyr} which conflicts with the function with the same name from {stats}. We use the most simple technique which is to add filter &lt;- dplyr::filter in the beginning of our script to precise which function we want to give priority and we pre-append the package name to all calls of the other function such as stats::filter. For more sophisticated ways to handle this issue we suggest the package {import}. We’re highlighting now some specific packages that are used in the book and that bring powerful features in analysis of data from R&amp;D and Operations. Wherever they are required in the book they are loaded explicitly so that we know where the functions come from. six sigma SixSigma is a very complete and robust package by Emilio L.Cano (Emilio L. Cano • Javier M. Moguerza 2015). It provides well many well tested functions in the area of quality management. qcc qcc is another extremely complete and solid package. It was developped and is maintained by Luca Scrucca and offers a very large range of statistical process control charts and capability analysis. Short examples in its vignette: qcc vignette qicharts2 I recommend qichart2 specifically for the nice pareto plots. As many niche packages we need to be awere that the number of contributers is small meaning that it cannot be as thouroughly tested as community packages. DoE.base This package is one of the most complete and vast packages in Design of Experiements. It is a first of a large suite of packages on the topic, it has vast functionality and is extremely well documented. DoE.base car The Companion for Applied Regression is also used extensively as it contains many usefull functions to assess the performance of linear models and anova. "],
["datasets.html", "Datasets", " Datasets An original package with the name {industRial} has been developed as a companion package for this book. It can be downloaded from github with the following command: devtools::install_github(&quot;J-Ramalho/industRial&quot;) The primary package goal is to make easily available all the data sets from all case studies. All dataset presented throughout the book are published either fully anonymized. Once the package is correctly installed it can be loaded in the R session as usual with the library() function: library(industRial) Once loaded the data sets become immediately available in memory and can directly be used in the examples presented or for further exploration. Lets confirm this invoking the first data set: dial_control %&gt;% head() %&gt;% kable() Operator Date Defect Location Jane 2018.01.31 Indent 3h Jane 2018.02.02 Indent 3h Jane 2018.02.02 Indent 4h Peter 2018.02.02 Indent 10h Jane 2018.02.03 Scratch 3h Jane 2018.02.03 Indent 3h The dateset can be used and manipulated like any other dataset created in the session or loaded otherwise. For example it can be filtered and assigned to a new variable name: dial_peter &lt;- dial_control %&gt;% filter(Operator == &quot;Peter&quot;) dial_peter %&gt;% head(2) %&gt;% kable() Operator Date Defect Location Peter 2018.02.02 Indent 10h Peter 2018.02.03 Scratch 4h "],
["functions.html", "Functions", " Functions Besides the data sets the {industRial} package also contains toy functions to plot Statistical Process Control (SPC) charts. The objective here is to showcase how to build such functions and their scope of application is limited to the book case studies. For complete and robust SPC functions we recommend using the {QCC} package also described below. Additionally the package contains theme functions to print and customize the aesthetics of spc charts and other charts. These themes are built on top of the {ggplot2} by H.Wickham and {cowplot} package by Claus O.Wilke. The main objective is to give the reader a starting point for customization of charts in this domain. The complete list of datasets, themes and functions can be seen by listing all objects from the package with ls(): ls(&quot;package:industRial&quot;) [1] &quot;%&gt;%&quot; &quot;battery_charging&quot; &quot;chart_Cpk&quot; [4] &quot;chart_I&quot; &quot;chart_IMR&quot; &quot;dial_control&quot; [7] &quot;ebike_hardening&quot; &quot;ebike_hardening2&quot; &quot;juice_drymatter&quot; [10] &quot;off_spec&quot; &quot;perfume_experiment&quot; &quot;pet_delivery&quot; [13] &quot;pet_doe&quot; &quot;process_Cpk&quot; &quot;process_stats&quot; [16] &quot;process_stats_table&quot; &quot;solarcell_fill&quot; &quot;solarcell_output&quot; [19] &quot;syringe_diameter&quot; &quot;tablet_thickness&quot; &quot;tablet_weight&quot; [22] &quot;theme_industRial&quot; &quot;theme_qcc&quot; or conveniently on the console with industRial:: and then tab. For each function a help page is available and can be obtained the same way as any other R data sets, themes and functions with ?&lt;object&gt; (e.g. ?chart_xbar) To go even deeper the text book and get access to all the code, the original Rmd files are also bundled in the package and can be seen in the book folder. A way to get the exact path is with: paste0(.libPaths()[1], &quot;/industRial/book&quot;) [1] &quot;C:/R/R-4.0.2/jr_library/industRial/book&quot; "],
["tutorials.html", "Tutorials", " Tutorials A set of practical exercises on key concepts presented throughout this book is available either on the web or locally, instructions follow. Run tutorials on the web the exercises are published on the shinyapp.io server and can be freely accessed with the links below: Pareto charts This tutorial builds on the The dial polishing workshop case study from the Design for Six Sigma chapter, train building pareto charts using the {qichart2} package and explore how playing with different variables gives new insights into apparently simple data collections. Anova This tutorial explores how the p value is calculated by playing with a dynamic anova chart. This exercise is based on the The e-bike frame hardening process of the DOE Interactions chapter. Run tutorials locally The tutorials can also be run locally which can be convenient as they load faster. This also allows for further exploration as the original tutorial code becomes available. Install locally the {industRial} and {learnr} packages, with: devtools::install_github(&quot;J-Ramalho/industRial&quot;) install.packages(&quot;learnr) load the packages: library(industRial) library(learnr) and list the tutorials with: learnr::available_tutorials(package = &quot;industRial&quot;) Available tutorials: * industRial - anova : &quot;industRial practice&quot; - capability : &quot;industRial practice&quot; - pareto : &quot;industRial practice&quot; choose a tutorial and run it as follows: learnr::run_tutorial(package = &quot;industRial&quot;, &quot;anova&quot;) The original files are available in the package tutorials folder. Their names correspond to the tutorial names listed before so there is a simple way to open the desired file, e.g.: rstudioapi::navigateToFile( paste0(.libPaths()[1], &quot;/industRial/tutorials/anova/anova.Rmd&quot;) ) "],
["design-for-six-sigma.html", "Design for Six Sigma Pareto Ishikawa Correlation matrix Clustering", " Design for Six Sigma Pareto The pareto chart has always proven an effective way of defining priorities and keeping workload under control. It is known for helping focusing on the few important elements that account for most problems. It builds on the well known insight that a few reasons explain or allow to control most of the outcome. This applies particularly well in the technological and industrial context. The dial polishing workshop The example here comes from a dial polishing workshop in the watchmaking industry. Dials are received from the stamping process and polished before being sent to the final assembly. As part of the authonomous quality control performed by the polishing operators a count of the defects observed on the dials each day is kept in a file. Figure 1: watch dial inspection Loading packages and data: getting the dial dataset: dial_control &lt;- industRial::dial_control A first look at the dataset now: head(dial_control) %&gt;% kable(align = &quot;c&quot;, caption = &quot;dial control data&quot;, booktabs = T) Table 1: dial control data Operator Date Defect Location Jane 2018.01.31 Indent 3h Jane 2018.02.02 Indent 3h Jane 2018.02.02 Indent 4h Peter 2018.02.02 Indent 10h Jane 2018.02.03 Scratch 3h Jane 2018.02.03 Indent 3h We can see that the count includes both the deffect type and the location (the hour in the dial) and that it is traced to the day and operator. The team leader promotes a culture of fact based assessment of the quality measurements. Every week the team looks back and observes the weekly counts. This is important because it helps moving away from perception into a more solid assessment. The volume of data is higher like this enabling trends to start becoming apparent. The team can discuss potential actions and prepare reporting to the supplier of the parts (the stamping workshop). It also helps calibrating between operators and agreeing on acceptance criteria and what is and what is not a defect. A first example of the pareto of the types of defects: library(qicharts2) d_type &lt;- dial_control %&gt;% pull(Defect) %&gt;% as.character() d_type_p &lt;- paretochart(d_type, title = &quot;Watch Dial polishing&quot;, subtitle = &quot;Pareto chart&quot;, ylab = &quot;Percentage of deffects&quot;, xlab = &quot;Deffect type&quot;, caption = &quot;Source: Dial Production Team&quot;) d_type_p + theme_industRial() As often happens we can see that the first two deffects account for more than 80% of the problems. Identation and scratching are the things to tackle here. From the available data presented before in table we can go deeper and establish a pareto of the defect location: d_location &lt;- dial_control %&gt;% pull(Location) %&gt;% as.character() d_location_p &lt;- paretochart(d_location, title = &quot;Watch Dial polishing&quot;, subtitle = &quot;Pareto chart&quot;, ylab = &quot;Percentage of deffects&quot;, xlab = &quot;Deffect location (hour)&quot;, caption = &quot;Source: Dial Production Team&quot;) d_location_p + theme_industRial() Here a third bucket could be included in the priorities: to reach 80% of the count we consider the defects that appear at 4 o’clock, 3 o’clock and 5 o’clock. During the reviews the team can also identify other types of data for follow up such as the dial model or the material type. With a simple excel file and an upload in R this can be finetuned from week to week according to the progress of the improvement measures. Ishikawa Usually called Fishbone or Ishikawa diagrams this simple tool has proven to be extremely practical and helpful in structuring team discussions. With it we can easily identify and list the expected influencing factors for example to design an experiment. Such selection and grouping of parameters can be useful for among others in defining the right mix of ingredients in a new product or material, in selecting the machine parameters in a manufacturing line or in the definition of a draft operating procedure for a measurement. In each of these situations it helps seeing the big picture and not fall into the trap of relying only in the data and findings obtained by statistical analysis. Below we’re showing an example building on the qcc package. The package author describes in his book Emilio L. Cano (2012) the advantages of using R for the design of these diagrams. In our view it strongly complements the data analysis making the full case easilly reproducible and easily updatable. The dental prosthesis laboratory An optical measurement device has just been installed in a large Dental Prosthesis Manufacturing Laboratory. This is a very expensive device based on laser technology installed in a dedicated stabilized workbench. Despite all the precautions it has been reported and now demonstrated with some specific trials that the measurements have a higher variation which makes it unsuitable to be used for what is was used for: the precise measurement of dental impressions that serve as models for the production of the crowns and bridges. Figure 2: dental impression measurement So far the Lab Team has full confidence in the equipement supplier and the Lab Manager has seen a similar equipment from the same supplier operating in another laboratory he has visited. The supplier checked the equipment and having seen no reason for the variability proposes to work with the lab team on identifying the potential causes for the high uncertainty in their measurements. They decided to consider a larger scope that the equipment and take the full measurement method as described in the laboratory operating procedure. They list different reasons related with they’re work and group them: operators &lt;- c(&quot;Supplier&quot;, &quot;Lab Technician&quot;, &quot;Lab Manager&quot;) materials &lt;- c(&quot;Silicon&quot;, &quot;Alginate&quot;, &quot;Polyethers&quot;) machines &lt;- c(&quot;Brightness&quot;, &quot;Fixture&quot;, &quot;Dimensional algorithm&quot;) methods &lt;- c(&quot;Fixture&quot;, &quot;Holding time&quot;, &quot;Resolution&quot;) measurements &lt;- c(&quot;Recording method&quot;, &quot;Rounding&quot;, &quot;Resolution&quot;) groups &lt;- c(&quot;Operator&quot;, &quot;Material&quot;, &quot;Machine&quot;, &quot;Method&quot;, &quot;Measurement&quot;) effect &lt;- &quot;Too high uncertainty&quot; And then load the qcc package and quickly obtain a simple diagram that allows for a quick visualisation of these influencing factors. library(qcc) cause.and.effect( title = &quot;Potential causes for uncertainty increase during a measurement&quot;, cause = list( Operator = operators, Material = materials, Machine = machines, Method = methods, Measurement = measurements ), effect = effect ) Figure 3: ishikawa diagram aplication The listed factors can then be adressed one by one or in combined experiments to evaluate their impact on the measurement method. Correlation matrix A matrix diagram is a way to discover relationships between groups of items as described by described in the Six Sigma book by Roderik A.Munro and J.Zrymiak (2015). These matrix can be used to select which measurement to do in a design of experiments. In exploratory phases when the experiments are repeated several time with slightly different configurations, secondary outputs that are strongly correlated to main outputs can be eliminated In an industrial setup the cost of experimenting is often very high. With this approach engineers can keep the quantities test quantities in control by avoiding measurements until final stages of implementation. We provide here two different techniques, one with a tile plot and another with a network plot. The Perfume destilation experiment Figure 4: Perfume destillation line DOEs consist in a series of trials where several inputs are combined in specific levels and important outputs are measured (further details can be seen in the DOE chapter). The case below refers to a DOE on Perfume Formulation Development. The Product Development team would like to understand the impact of the perfum manufacturing line parameters variation (e.g. temperature, pressure and others) in typical perfume sensorial characteristics such as the floral notes. The typical DOE analysis results linking inputs to outputs are presented with effects plots and interaction plots. Here though it is another analysis that interests us: the correlation between the outputs, where there is not always necessary a cause effect but where it is interesting to see if groups of outputs move together. This type of analysis is most commonly presented in a tile plot. In our present case note that the first DOE has not yet been executed, only a preparatory session has taken place with the project manager to review the potential outputs and anticipate what will be the DOE results. This allowed to go deeper in the technology understanding and to confirm that the plan was constructed in a meaning full way. The experts input has been captured in a 1/2 of a two entry table named “Perfume.” As the industRial package was already loaded in the Pareto section, we can then access directly to the Perfume dataset of which we’re showing a subset: perfume_experiment[1:8, 1:8] %&gt;% kable( caption = &quot;perfume DoE output variables&quot;, booktabs = T ) Table 2: perfume DoE output variables yy pw w pm pe f it ew pw 0 10 3 3 2 2 0 w 0 0 3 3 2 2 0 pm 0 0 0 6 6 0 0 pe 0 0 0 0 6 0 0 f 0 0 0 0 0 7 0 it 0 0 0 0 0 0 0 ew 0 0 0 0 0 0 0 c 0 0 0 0 0 0 0 Variables are named with coded names made of two letter. They represent the production Line Parameters and the Perfume Attributes (e.g. t = temperature, o = opening, pw = power). We can see in the table what the team has noted as expected correlation strenght, with 10 being the highest. Now we filter correlations higher or equal to 7 which have been considered as the potential targets for the simplification in future designs: perfume_long &lt;- perfume_experiment %&gt;% pivot_longer( cols = -yy, values_to = &quot;correlation&quot;, names_to = &quot;xx&quot; ) %&gt;% filter(correlation &gt;= 7) %&gt;% mutate(correlation = as_factor(correlation)) perfume_long %&gt;% ggplot(aes(x = xx, y = yy, fill = correlation)) + scale_fill_viridis_d(direction = -1, name = &quot;Correlation\\nStrength&quot;) + geom_tile() + labs( title = &quot;The Perfume destilation experiment&quot;, subtitle = &quot;Input variables correlation plot &quot;, x = &quot;&quot;, y = &quot;&quot;, caption = &quot;Data variables anonymized&quot; ) + theme_industRial() The plot shows that many parameters are expected to move together, for example with maximum correlation we have hp moving with d, oc moving with o and so on. After this first DoE the real correlations will be established and the team expects to be able to avoid a significant part of the measurements that have a correlation higher than 50% from the second DoE onwards. Clustering In this variant we explore a more advanced but more powerfull approach using network plots. It provides an automatic clustering of the factors and a specific way to read such clusters. We’re going to build a weighed non directional network(tbl_graph) object. Several steps of conversion are required for this approach first with igraph and then to tidygraph. We start by loading the required packages: library(igraph) library(tidygraph) library(ggraph) library(ggforce) library(ggtext) The first step consists in converting the “Perfume” tibble to a matrix format: perfume_matrix &lt;- perfume_experiment %&gt;% column_to_rownames(&quot;yy&quot;) %&gt;% as.matrix() Then using the {igraph} package we convert the matrix into a graph object: perfume_graph &lt;- graph_from_adjacency_matrix( perfume_matrix, mode = &quot;undirected&quot;, weighted = TRUE ) to finaly convert it into a tibble graph with tidygraph package: perfum_tbl_graph &lt;- as_tbl_graph(perfume_graph, add.rownames = &quot;nodes_names&quot;) The users have provided the correlation strength in a simple scale from 1 to 10 which was easier for the discussion. We’re here converting it back to the 0 to 1 which is more common in the statistics community. For simplicity, negative correlations were not considered just the strength, enabling the network to be unidirectional. perfum_tbl_graph &lt;- perfum_tbl_graph %&gt;% activate(edges) %&gt;% mutate(weight = weight/10) perfum_tbl_graph # A tbl_graph: 22 nodes and 85 edges # # An undirected simple graph with 7 components # # Edge Data: 85 x 3 (active) from to weight &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 2 1 2 1 3 0.3 3 1 4 0.3 4 1 5 0.2 5 1 6 0.2 6 1 8 0.8 # ... with 79 more rows # # Node Data: 22 x 1 name &lt;chr&gt; 1 pw 2 w 3 pm # ... with 19 more rows In the previous chunk output we see a preview of the tibble graph object with the first few nodes and edges. Now we create a vector with various igraph layouts to allow for easier selection when making the plots: igraph_layouts &lt;- c(&#39;star&#39;, &#39;circle&#39;, &#39;gem&#39;, &#39;dh&#39;, &#39;graphopt&#39;, &#39;grid&#39;, &#39;mds&#39;, &#39;randomly&#39;, &#39;fr&#39;, &#39;kk&#39;, &#39;drl&#39;, &#39;lgl&#39;) and do a first network plot to check data upload: perfum_tbl_graph %&gt;% ggraph(layout = &quot;igraph&quot;, algorithm = igraph_layouts[7]) + geom_edge_link(aes(edge_alpha = weight)) + geom_node_label(aes(label = name), repel = TRUE) + theme_graph() + labs(title = &quot;DOE Perfume Formulation - Inputs&quot;, subtitle = &quot;Most important expected correlations&quot;) Data loading is confirmed to have been done correctly, we can now move into the clustering and analysis. We use different clusters algorithms like in part 3 to generate the groups: perfum_tbl_graph &lt;- perfum_tbl_graph %&gt;% activate(nodes) %&gt;% mutate(group_components = group_components(), group_edge_betweenness = group_edge_betweenness(), group_fast_greedy = group_fast_greedy(), group_infomap = group_infomap(), group_label_prop = group_label_prop(), group_leading_eigen = group_leading_eigen(), group_louvain = group_louvain(), group_walktrap = group_walktrap() ) and produce a final plot, selecting group optimal that with some testing has proven to be the algorithm that gives the best clustering results. The correlations strengths are here represented by the edges width for optimal visualization. perfum_tg_2 &lt;- perfum_tbl_graph %&gt;% activate(edges) %&gt;% mutate(weight2 = if_else(weight &gt;= 0.8, 1, if_else(weight &gt;= 0.5, 0.5, 0.1))) my_palette &lt;- c(viridis(12)[3], viridis(12)[9], &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;) set.seed(48) perfum_tg_2 %&gt;% activate(nodes) %&gt;% mutate(group = group_louvain) %&gt;% filter(group %in% c(1,2)) %&gt;% ggraph(layout = &quot;igraph&quot;, algorithm = igraph_layouts[7]) + geom_mark_hull(mapping = aes(x, y, group = as_factor(group), fill = as_factor(group)), concavity = 0.5, expand = unit(4, &#39;mm&#39;), alpha = 0.25, colour = &#39;white&#39;, show.legend = FALSE) + geom_edge_link(aes(edge_alpha = weight2, edge_width = weight2)) + geom_node_point(size = 3) + geom_node_label(aes(label = name), repel = TRUE) + scale_edge_width(range = c(0.2, 1), name = &quot;Correlation strength&quot;) + scale_edge_alpha(range = c(0.05, 0.2), name = &quot;Correlation strength&quot;) + scale_fill_manual(values = my_palette) + theme_graph() + labs( title = str_c(&quot;&lt;span style=&#39;color:#433E85FF&#39;&gt;Line Parameters&lt;/span&gt;&quot;, &quot; and &quot;, &quot;&lt;span style=&#39;color:#51C56AFF&#39;&gt;Perfume Attributes&lt;/span&gt;&quot;), subtitle = &quot;Clustering the outputs of Perfume Formulation DOE01&quot;, caption = &quot;Clustering by multi-level modularity optimisation (louvain)&quot;) + theme(plot.title = element_markdown(family = &quot;Helvetica&quot;, size = 14, face = &quot;bold&quot;)) We can see that the algorithm is grouping elements that have a strong correlation. Most stronger correlations are present within elements of each cluster with some exceptions such as oc with pw and y with pe. The code presented can now easily be reused once the DOE is executed to compare with the real correlations measured. "],
["MSA.html", "Measurement System Analysis Trueness Precision Uncertainty", " Measurement System Analysis Validation of a measurement device Trueness The juice production plant The Quality Control Manager of a Juice producing plant acquired a faster dry matter content measurement device from the supplier DRX. An important reduction of the control time was the rational for the acquisition and now before finally putting it into operation its performance is being assessed and validated. Figure 1: juice bottling line In this case study we will look into the assessment of the linearity which is the difference in average bias throughout the measurement range. Dry matter content for the company top seller juice_bottling have around 12% dry matter as for example Premium Fresh Apple juice: 12.4 % and Austrian Beetroot: 13.2% and some other specialities may have a higher content up such as Organic Carrot with 16.3%. It has been decided to start by checking the equipement in the range of 10 to 20% dry matter content. Lets look into the measurements obtained on the juice bottling process: summary(juice_drymatter) product drymatter_TGT speed particle_size part Length:108 Min. :10 Min. :20.0 Min. :250 Min. :1 Class :character 1st Qu.:10 1st Qu.:20.0 1st Qu.:250 1st Qu.:1 Mode :character Median :15 Median :22.5 Median :275 Median :2 Mean :15 Mean :22.5 Mean :275 Mean :2 3rd Qu.:20 3rd Qu.:25.0 3rd Qu.:300 3rd Qu.:3 Max. :20 Max. :25.0 Max. :300 Max. :3 drymatter_DRX drymatter_REF Min. : 9.720 Min. : 9.97 1st Qu.: 9.898 1st Qu.:10.02 Median :14.700 Median :15.01 Mean :14.706 Mean :15.00 3rd Qu.:19.503 3rd Qu.:20.00 Max. :19.710 Max. :20.03 we see raw data for the DRX and Ref equipment allowing us to calculate the difference between the two devices for each measurement: juice_drymatter &lt;- juice_drymatter %&gt;% mutate(bias = drymatter_DRX - drymatter_REF, part = as_factor(part)) summary(juice_drymatter$bias) Min. 1st Qu. Median Mean 3rd Qu. Max. -0.6300 -0.4025 -0.2850 -0.2979 -0.1875 -0.0700 We immediatly see a median bias of -0.2 Lets explore further. Bias plot juice_drymatter %&gt;% ggplot(aes(x = drymatter_REF, y = bias)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = T, ) + coord_cartesian( xlim = c(9,21), ylim = c(-.75,0), expand = TRUE) + theme_industRial() + labs(title = &quot;Dry matter method validation&quot;, subtitle = &quot;Gage Linearity&quot;, caption = &quot;Dataset: juice_drymatter233A, Operator: S.Jonathan)&quot;) The linear model is well adapted in this case, this by seing the position of the slope close to the averages of each level of the factor. Nevertheless the slope is rather steep showing a clear increase of the bias (in the negative direction) with the increase in dry matter content. Bias report juice_drymatter %&gt;% group_by(drymatter_TGT) %&gt;% summarise(bias_mean = mean(bias, na.rm = TRUE), bias_median = median(bias, na.rm = TRUE)) %&gt;% select(drymatter_TGT, bias_mean, bias_median) %&gt;% kable(align = &quot;c&quot;, digits = 2) drymatter_TGT bias_mean bias_median 10 -0.17 -0.15 15 -0.29 -0.31 20 -0.44 -0.44 Mean and median bias are very close. A decision now needs to be taken on which systematic offset to apply depending on the operational context. If most products on the line where the device is used a simplified operational procedure with a unique offset of 0.2 can be sufficient. Precision The tablet compaction process Modern pharmaceutical tablet presses reach output volumes of up to 1,700,000 tablets per hour. These huge volumes require frequent in-process quality control for the tablet weight, thickness and hardness. In our case study we’re going to measurement the precision of the measurement method used to determine the thickness of the tablet. Figure 5: Tablet thickness micrometer According to the ISO 5725 the Precision of a measurement method is the combination of the Reproductibility &amp; Reproducibility. Data loading We use here the data from the tablet thickness method validation: str(tablet_thickness) tibble [675 x 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame) $ Position : chr [1:675] &quot;Position 1&quot; &quot;Position 1&quot; &quot;Position 1&quot; &quot;Position 1&quot; ... $ Size : chr [1:675] &quot;L&quot; &quot;L&quot; &quot;L&quot; &quot;L&quot; ... $ Tablet : chr [1:675] &quot;L001&quot; &quot;L001&quot; &quot;L001&quot; &quot;L001&quot; ... $ Replicate : num [1:675] 1 2 3 4 5 1 2 3 4 5 ... $ Day : chr [1:675] &quot;Day 1&quot; &quot;Day 1&quot; &quot;Day 1&quot; &quot;Day 1&quot; ... $ Date [DD.MM.YYYY] : chr [1:675] &quot;18/11/2020&quot; &quot;18/11/2020&quot; &quot;18/11/2020&quot; &quot;18/11/2020&quot; ... $ Operator : chr [1:675] &quot;Paulo&quot; &quot;Paulo&quot; &quot;Paulo&quot; &quot;Paulo&quot; ... $ Thickness [micron] : num [1:675] 1803 1803 1804 1804 1803 ... $ Temperature [°C] : num [1:675] 22.3 22.3 22.3 22.4 22.6 22.9 22.8 22.8 22.7 22.7 ... $ Relative Humidity [%]: num [1:675] 32.7 32.8 32.8 33.4 33.4 36 36.3 36.4 36.6 36.3 ... $ Luminescence [lux] : num [1:675] 569 580 580 584 594 ... - attr(*, &quot;spec&quot;)= .. cols( .. Position = col_character(), .. Size = col_character(), .. Tablet = col_character(), .. Replicate = col_double(), .. Day = col_character(), .. `Date [DD.MM.YYYY]` = col_character(), .. Operator = col_character(), .. `Thickness [micron]` = col_double(), .. `Temperature [°C]` = col_double(), .. `Relative Humidity [%]` = col_double(), .. `Luminescence [lux]` = col_double() .. ) We see that quite an extensive variaty of parameters was collected here including room conditions. We are mostly interested in the tablet size, thickness and operator who has performed the measurement. As these parameters are coded as characters we are going to convert them to factors: tablet_thickness &lt;- tablet_thickness %&gt;% clean_names() %&gt;% mutate(across(c(size, tablet, operator), as_factor)) We want to establish an independed r&amp;R by specification size (S, M or L) and so we first filter only for the first size the L. tablet_L &lt;- tablet_thickness %&gt;% filter(size == &quot;L&quot;) Base Anova We’re feeding the aov function from the stats package with operator and tablet factors. tablet_L_lm &lt;- lm( thickness_micron ~ ( # main effects tablet + operator + # 2nd order interactions operator:tablet ), data = tablet_L ) tablet_L_aov &lt;- aov(tablet_L_lm) summary(tablet_L_aov) Df Sum Sq Mean Sq F value Pr(&gt;F) tablet 4 1707.1 426.8 271.457 &lt;2e-16 *** operator 2 13.1 6.6 4.177 0.0166 * tablet:operator 8 11.2 1.4 0.892 0.5237 Residuals 210 330.1 1.6 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Below we’re recreating the same analysis with the ss.rr function from the Six Sigma package. As the function allows to input the limits we’re also providing in the function arguments the current upper and lower limit of the specification. tablet L 18’000mm3 +/- 250mm3 (18.0ml +/- 0.25ml) gage r&amp;R library(SixSigma) Error in get(genname, envir = envir) : objet &#39;testthat_print&#39; introuvable tablet_L_rr &lt;- ss.rr( data = tablet_L, var = thickness_micron, part = tablet, appr = operator, alphaLim = 1, errorTerm = &quot;repeatability&quot;, # very important otherwise F test not identical to base aov main = &quot;Micrometer FTR600\\nr&amp;R for tablet thickness&quot;, sub = &quot;Tablet L&quot;, lsl = 1775, usl = 1825 ) Complete model (with interaction): Df Sum Sq Mean Sq F value Pr(&gt;F) tablet 4 1707.1 426.8 271.457 &lt;2e-16 operator 2 13.1 6.6 4.177 0.0166 tablet:operator 8 11.2 1.4 0.892 0.5237 Repeatability 210 330.1 1.6 Total 224 2061.6 alpha for removing interaction: 1 Gage R&amp;R VarComp %Contrib Total Gage R&amp;R 1.64098201 14.79 Repeatability 1.57212495 14.17 Reproducibility 0.06885705 0.62 operator 0.06885705 0.62 tablet:operator 0.00000000 0.00 Part-To-Part 9.45247729 85.21 Total Variation 11.09345930 100.00 VarComp StdDev StudyVar %StudyVar %Tolerance Total Gage R&amp;R 1.64098201 1.2810082 7.686049 38.46 15.37 Repeatability 1.57212495 1.2538441 7.523064 37.65 15.05 Reproducibility 0.06885705 0.2624063 1.574438 7.88 3.15 operator 0.06885705 0.2624063 1.574438 7.88 3.15 tablet:operator 0.00000000 0.0000000 0.000000 0.00 0.00 Part-To-Part 9.45247729 3.0744881 18.446929 92.31 36.89 Total Variation 11.09345930 3.3306845 19.984107 100.00 39.97 Number of Distinct Categories = 3 We can observe that the SixSigma package recreates exactly the same anova table, just calling Repeatability to the Residuals and adding an additional line with the total degrees of freedom and the total sum of squares. Note that the argument alphaLim has been set to 1 to avoid suppressing the interaction which is in this case non significative. Acceptance on Variance Criteria for measurement system acceptance: To evaluate your process variation, compare the Total Gage R&amp;R contribution in the %Contrib column with the values in the list below: Less than 1%: the measurement system is acceptable Between 1% and 9%: the measurement system is acceptable depending on the application, the cost of the measurement device, cost of repair, or other factors Greater than 9%: the measurement system is not acceptable and should be improved. Acceptance on SD The study variation table is established by calculating the square root of each variance (the standard deviation) and by multiplying it by 6 (the six sigma) and then again by comparing each variation with the total variation. Standard deviations are usualy more speaking to the industry professionals. This table also provides a comparison with the specification. Criteria for measurement system acceptance: According to the guidelines from the Automotive Industry Action Group (2010), if your system variation is less than 10% of the process variation, then it is acceptable. To evaluate your process variation, compare the Total Gage R&amp;R contribution in the %StudyVar column with the values in the list below: Less than 10%: the measurement system is acceptable Between 10% and 30%: the measurement system is acceptable depending on the application, the cost of the measurement device, cost of repair, or other factors Greater that 30%: the measurement system is not acceptable and should be improved. If the p-value for the operator and part interaction is 0.05 or higher, the system removes the interaction because it is not significant and generates a second ANOVA table without the interaction. The AIAG also states that the number of distinct categories into which the measurement system divides process output should be greater or equal to 5. The part to part variation is high which is what is expected in a study like this. In our specific example we observe that the device cannot be accepted as the Study variation for the Total Gage r&amp;R is 38.46% thus much higher than 30%. Furhtermore the number of distinc categories is only of 3. Finaly to be noted that the total variation rather low when compared with the specification. In a nutshell two questions are answered: can the method be used to assess process performance: no the measurement system variation equals 38.4% of the process variation, can the method be used to sort good parts from bad: yes but can be improved, the measurement system variation equals 15.3% of the tolerance. Interaction plots The Six Sigma package plots are similar to the interaction plots provided by other DoE packages but don’t have error bars. These can nevertheless easily be established on a needed basis as in the example below where we’re recreating the tablet:operator interaction plot with a +/- 1 standard deviation error bars. tablet_L %&gt;% group_by(tablet, operator) %&gt;% summarise(vol_mean = mean(thickness_micron), vol_sd = sd(thickness_micron)) %&gt;% ggplot(aes(x = tablet, y = vol_mean, color = operator)) + geom_point(aes(group = operator), size = 2) + geom_line(aes(group = operator, linetype = operator)) + geom_errorbar(aes(ymin = vol_mean - vol_sd, ymax = vol_mean + vol_sd), width = .1) + scale_y_continuous(labels = label_number(big.mark = &quot;&#39;&quot;)) + scale_color_viridis_d(option = &quot;C&quot;, begin = 0.1, end = 0.9) + # coord_cartesian(ylim = c(17950, 18150)) + annotate(geom = &quot;text&quot;, x = Inf, y = -Inf, label = &quot;Error bars are +/- 1xSD&quot;, hjust = 1, vjust = -1, colour = &quot;grey30&quot;, size = 3, fontface = &quot;italic&quot;) + theme_industRial() + labs(title = &quot;Tablet thickness method validation&quot;, subtitle = &quot;Interaction plot - L tablet x Operator&quot;, x = &quot;&quot;, y = &quot;thickness [mm]&quot;, caption = &quot;Data source: QA Lab&quot;) Negative Variations Two important limitations exist in the current approach: when the operators reproducibility is negative it is converted to zero. (Montgomery 2012) in page 557 adresses this case in the following way: Notice that the estimate of one of the variance components,is negative. This is certainly not reasonable because by definition variances are nonnegative. Unfortunately, negative estimates of variance components can result when we use the analysis of variance method of estimation (this is considered one of its drawbacks). We can deal with this negative result in a variety of ways: 1) one possibility is to assume that the negative estimate means that the variance component is really zero and just set it to zero, leaving the other nonnegative estimates unchanged. 2) Another approach is to estimate the variance components with a method that assures nonnegative estimates (this can be done with the maximum likelihood approach). 3) Finally, we could note that the P-value for the interaction term … is very large, take this as evidence that really is zero and that there is no interaction effect, and then fit a reduced model of the form that does not include the interaction term. This is a relatively easy approach and one that often works nearly as well as more sophisticated methods. This final approach is also what the SixSigma package creators have foreseen and if we leave the argument alphaLim empty the non significant terms will be suppressed from the model, the Anova recalculated and the remaining tables updated accordingly. This can be finetuned with the argument alphaLim. Usually we consider a p value of 0.05 but we recommend to start with higher values such as 0.1 or 0.2 to avoid suppressing too quickly the factor which would result in a transfer of their variability into the repeatability. tablet_L_rr2 &lt;- ss.rr( data = tablet_L, var = thickness_micron, part = tablet, appr = operator, alphaLim = 0.2, # instead of 0.05 it is recommended to start higher errorTerm = &quot;repeatability&quot;, main = &quot;Micrometer FTR600\\nr&amp;R for tablet thickness&quot;, sub = &quot;Tablet L&quot;, lsl = 1775, usl = 1825 ) Complete model (with interaction): Df Sum Sq Mean Sq F value Pr(&gt;F) tablet 4 1707.1 426.8 271.457 &lt;2e-16 operator 2 13.1 6.6 4.177 0.0166 tablet:operator 8 11.2 1.4 0.892 0.5237 Repeatability 210 330.1 1.6 Total 224 2061.6 alpha for removing interaction: 0.2 Reduced model (without interaction): Df Sum Sq Mean Sq F value Pr(&gt;F) tablet 4 1707.1 426.8 272.533 &lt;2e-16 operator 2 13.1 6.6 4.194 0.0163 Repeatability 218 341.4 1.6 Total 224 2061.6 Gage R&amp;R VarComp %Contrib Total Gage R&amp;R 1.63260451 14.73 Repeatability 1.56591940 14.13 Reproducibility 0.06668511 0.60 operator 0.06668511 0.60 Part-To-Part 9.44885739 85.27 Total Variation 11.08146189 100.00 VarComp StdDev StudyVar %StudyVar %Tolerance Total Gage R&amp;R 1.63260451 1.2777341 7.666405 38.38 15.33 Repeatability 1.56591940 1.2513670 7.508202 37.59 15.02 Reproducibility 0.06668511 0.2582346 1.549408 7.76 3.10 operator 0.06668511 0.2582346 1.549408 7.76 3.10 Part-To-Part 9.44885739 3.0738994 18.443396 92.34 36.89 Total Variation 11.08146189 3.3288830 19.973298 100.00 39.95 Number of Distinct Categories = 3 In our case when comparing the total gage r&amp;R with and without the interaction we see it changing from 38.46% to 38.38%. Beyond two factors The ss.rr function only accepts 2 factors so it is not possible to obtain all the tables and plots with for example the day as a factor. This significantly limits the calculation of the total uncertainty for some measurement methods. Next steps in our study will be to prepare an R function to deal with more than 2 factors. Conclusions The Six Sigma package r&amp;R approach can be applied with no issue to simple cases with 2 factors (e.g. operators and parts) where the Variance Component of the Reproducibility is not negative. Uncertainty A final step in the validation of our measurement device is now the calculation of the total measurement uncertainty. In some reports the terminology uncertainty is prefered instead of gage r&amp;R. In this case the formula usually used to evaluate the measurement uncertainty is: \\[ u^2=u_{repeat.}^2+ u_{reprod.}^2+ u_{cal.}^2 \\] where the repeatability and reproducibility members can be obtained from the variances calculated in the r&amp;R study \\[ u_{repeat}^2 = σ_{repeat}^2\\\\ u_{reprod}^2 = σ_{reprod}^2 \\] These variance components can be directly obtained from the object generated by the function ss.rr of the {SixSigma} package. If we name our variable \\(σ_{repeat}^2\\) as repeat_var in R we have: repeat_var &lt;- tablet_L_rr$varComp[2,1] repeat_var [1] 1.572125 and for the reproducibility we have: reprod_var &lt;- tablet_L_rr$varComp[3,1] reprod_var [1] 0.06885705 The equipment manual mentions an accuracy of 0.001 mm. If we take this as the calibration uncertainty expressed as a standard deviation, this means we have: \\[ u_{cal} = 0.001 mm \\Leftrightarrow 1 \\mu m\\\\ u_{cal}^2 = 1^2 = 1 \\] that we can assign in R to the variable calibration_var. calibration_var &lt;- 1 We thus we have a uncertainty of: u &lt;- sqrt(reprod_var + repeat_var + calibration_var) u [1] 1.62511 Finally what is usually reported is the expanded uncertainty corresponding to 2 standard deviations. To be recalled that \\(\\pm\\) 2 std corresponds to 95% of the values when a repeative measurement is done. In this case we have \\(U = 2*u\\): U &lt;- 2 * u U [1] 3.25022 For a specific measurement of say 1’800 \\(\\mu m\\) we then say: the tablet thickness is 1’800 \\(\\mu m\\) \\(\\pm\\) 3.3 \\(\\mu m\\), at the 95 percent confidence level. Or written in short: 1’800 \\(\\mu m\\) \\(\\pm\\) 3.3 \\(\\mu m\\), at a level of confidence of 95% Knowing that the specification is [1’775; 1’825] \\(\\mu\\)m we have a specification range of 500. The expanded uncertainty corresponds to 13 %. This is another way of looking into the ratio between method variation and specification. This {SixSigma} package gave a similar result of 15.37%. To be noted that the calculation in by the package corresponds to 3 standard deviations and does not comprise the supplier calibration. For further reading on the topic we recommend the booklet from Bell (2001). "],
["DOE.html", "Design of Experiments Simple experiments Regression and anova Interactions Covariance General factorial designs Two level designs", " Design of Experiments In a design of experiements we calculate the total number of trials with the expression \\(n^m\\) where n is the number of levels, m the number of factors. A trial represents the number of unique combinations of the factors. To obtain the final number of test runs we have to multiply the number of trials by the number of replicates per trial. In a design with 4 factors of 2 levels we have then \\(2^4 = 16\\) runs and \\(16 \\times 5 = 80\\) replicates. If the design has a combination of factors with different number of levels the number of trials is the multiplication of both such as: \\(n^m \\times n^m\\). For example if we added 2 additional factors with 4 levels each to the previous design we would obtain \\(2^4 \\times 4^2 = 256\\) which we would still need to multiply by the number of replicates to obtain the number of runs \\(256 \\times 5 = 1280\\). In the literature we often see the simbolic notation \\(a^k\\) but we’ve opted for mF-nL (m factors, n levels) in this book for simplification. Simple experiments 1 factor 2 levels In this chapter we cover introductory designs of experiments and take it progressively until the general 2^k factorial designs. In any case this pretends to be an introduction to the topic with only a subset of the many types of DoEs used in the industry. Means comparison t-test one sample The Winter Sports clothing manufacturer Comparing mean to specification An engineer working the winter sports clothing industry has established a contract for PET textile raw material supply based on the following specification: the average tensile strength has to be greater than 69.0 \\(Mpa\\) for each delivery. In the contract is also specified that the test protocol which is based on a 30 samples. Figure 1: PET tensile test A first delivery is submited and the customer wants to know if the lot average tensile strength exceeds the agreed level and if so, she wants to accept the lot. The Quality Control department specialist at the reception starts by calculating the average, a first criteria to reject the batch: pet_spec &lt;- 69 pet_mean &lt;- mean(pet_delivery$A) pet_mean [1] 68.71 The average is itself below the spec and the engineer could think to reject the batch right away. She decides nevertheless to observe the variability and for this she decides to plot the raw data on an histogram. An histogram is a very common plot showing counts for selected intervals. pet_delivery %&gt;% ggplot(aes(x = A)) + geom_histogram(color = viridis(12)[4], fill = &quot;grey90&quot;) + geom_vline(xintercept = pet_mean, color = &quot;darkblue&quot;, linetype = 3) + geom_vline(xintercept = pet_spec, color = &quot;darkgreen&quot;, linetype = 2, show.legend = TRUE) + labs(title = &quot;PET clothing case study&quot;, subtitle = &quot;Raw data plot&quot;, x = &quot;Treatment&quot;, y = &quot;Tensile strength [MPa]&quot;) The mean is just slightly below the target mean defined for acceptance and she also observes a certain variability in the batch. She decides then to perform a t-test to assess if the average calculated can be really be considered statistically different than the target value. t.test(x = pet_delivery$A, mu = pet_spec) One Sample t-test data: pet_delivery$A t = -1.0754, df = 27, p-value = 0.2917 alternative hypothesis: true mean is not equal to 69 95 percent confidence interval: 68.15668 69.26332 sample estimates: mean of x 68.71 The basic assumption of the test is that the means are equal and the alternative hypothesis is that the sample mean is different than the spec. The confidence interval selected is 95%. The test result tells us that for a population average of 69, the probability of obtaining a sample with a value as extreme as 68.71 is of 29.17% (p = 0.2917). This probability value higher than the limit of 5% that she had defined to reject the null hypothesis. She cannot conclude that the sample commes from a population with a mean different than 69 and thus decides to accept the batch. t-test two samples Comparing means In order to avoid similar situations in the future the development engineer considers a new chemical compositions of pet that potentially increases the levels of strenght. Data loading pet_delivery_long &lt;- pet_delivery %&gt;% pivot_longer( cols = everything(), names_to = &quot;sample&quot;, values_to = &quot;tensile_strength&quot; ) Raw data plot In data analysis it is good practice to start by plotting the raw data and have a first open look at what the first plots tell us. pet_delivery_long %&gt;% ggplot(aes(x = sample, y = tensile_strength)) + geom_point() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;PET clothing case study&quot;, subtitle = &quot;Raw data plot&quot;, x = &quot;Sample&quot;, y = &quot;Tensile strength [MPa]&quot;) Another way to better understanding the bond distributions is to plot a box plot. This type of plot is somehow like the histogram seen before but more compact when several groups are required to be plotted. pet_delivery_long %&gt;% ggplot(aes(x = sample, y = tensile_strength, fill = sample)) + geom_boxplot(width = 0.3) + scale_fill_viridis_d(begin = 0.5, end = 0.8) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;PET clothing case study&quot;, subtitle = &quot;Box plot&quot;, x = &quot;Sample&quot;, y = &quot;Tensile strength [MPa]&quot;) We would like to understand if the treatment has an effect. Thus we want to compare the two population means. For that we use a t test using samples obtained independently and randomly. Before running the test we also have to check the normality of the samples distributions and equality of their variances. To do these checks we’re using the stat_qq functions from the ggplot package and plotting the qq plots for both levels in the same plot: pet_delivery_long %&gt;% ggplot(aes(sample = tensile_strength, color = sample)) + geom_qq() + geom_qq_line() + coord_flip() + scale_color_viridis_d(begin = 0.1, end = 0.7) + labs(title = &quot;PET clothing case study&quot;, subtitle = &quot;Q-Q plot&quot;, x = &quot;Residuals&quot;, y = &quot;Tensile strength [MPa]&quot;) We observe that for both levels of treatment the data is adhering to the straight line thus we can assume they follow a normal distribution. Also both lines in the qq plot have equivalent slopes indicating that the assumption of variances is a reasonable one. These verifications are summary ones. We review in subsequent sessions other deeper verifications of such as the shapiro-wilk normality test. We’re now going to apply the t-test: library(stats) t.test(tensile_strength ~ sample, data = pet_delivery_long, var.equal = TRUE) Two Sample t-test data: tensile_strength by sample t = -2.3956, df = 54, p-value = 0.02009 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -1.5849965 -0.1407177 sample estimates: mean in group A mean in group B 68.71000 69.57286 We see that p &lt; 0.05 thus the means differ significantly. Furthemore the mean difference is estimated with 95% confidence, to be between -0.55 and -0.01 (to be noted that zero is obviously not included in this interval). There is an effect in our treatment that explains the difference in means between the two samples. Variances comparison F-test We’re now confirming this with a variance test from the stats package. var.test(tensile_strength ~ sample, pet_delivery_long) F test to compare two variances data: tensile_strength by sample F = 1.2755, num df = 27, denom df = 27, p-value = 0.5315 alternative hypothesis: true ratio of variances is not equal to 1 95 percent confidence interval: 0.5902643 2.7563454 sample estimates: ratio of variances 1.275528 The test null hypothesis is that the variances are equal. Since the p value is much greater than 0.05 we cannot reject the null hypotheses meaning that we can consider them equal. The F-test is accurate only for normally distributed data. Any small deviation from normality can cause the F-test to be inaccurate, even with large samples. However, if the data conform well to the normal distribution, then the F-test is usually more powerful than Levene’s test. Levene test This test is assessing the homogeneity of variances (homoscedasticity). library(car) leveneTest(tensile_strength ~ sample, data = pet_delivery_long) Levene&#39;s Test for Homogeneity of Variance (center = median) Df F value Pr(&gt;F) group 1 0.0118 0.9139 54 Pr &gt; 0.05 thus there is homogeneity of the variances (they do not differ significantly). Further elaborations on the variance can be found under Minitab (2019a). Regression and anova One factor multiple levels The e-bike frame hardening process Mountain bikes frames are submitted to many different efforts, namely bending, compression and vibration. Although no one expects the frame to break in regular usage, manufacturers reputation is made on less visible performance features. One of them is the duration of the bike or in more technical terms in the number of cycles of such efforts that the frame resists. Figure 1: e-bike frames entering hardening treatment Linear regression We will present here a first example of the utilisation of linear regression techniques and establish a linear model. These models are going to be used extensively in the upcoming cases. Data loading ebike_narrow &lt;- ebike_hardening %&gt;% pivot_longer( cols = starts_with(&quot;g&quot;), names_to = &quot;observation&quot;, values_to = &quot;cycles&quot; ) %&gt;% group_by(temperature) %&gt;% mutate(cycles_mean = mean(cycles)) %&gt;% ungroup() slice_head(.data = ebike_narrow, n = 5) %&gt;% kable(align = &quot;c&quot;, caption = &quot;e-bike hardening experiment data&quot;) Table 1: e-bike hardening experiment data temperature observation cycles cycles_mean 160 g1 575000 551200 160 g2 542000 551200 160 g3 530000 551200 160 g4 539000 551200 160 g5 570000 551200 Raw data plot ggplot(data = ebike_narrow) + geom_point(aes(x = temperature, y = cycles)) + geom_point(aes(x = temperature, y = cycles_mean), color = &quot;red&quot;) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Raw data plot&quot;, x = &quot;Furnace Temperature [°C]&quot;, y = &quot;Cycles to failure [n]&quot;) Linear model We start by establishing the model, ensuring for now that we leave the variable temperature as a numeric vector. ebike_lm &lt;- lm(cycles ~ temperature, data = ebike_narrow) summary(ebike_lm) Call: lm(formula = cycles ~ temperature, data = ebike_narrow) Residuals: Min 1Q Median 3Q Max -43020 -12325 -1210 16710 33060 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 137620.0 41210.8 3.339 0.00365 ** temperature 2527.0 215.4 11.731 7.26e-10 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 21540 on 18 degrees of freedom Multiple R-squared: 0.8843,\tAdjusted R-squared: 0.8779 F-statistic: 137.6 on 1 and 18 DF, p-value: 7.263e-10 With the summary function we can many different outputs such as the coefficients and the R-squared which we will look into more detail now. As usual, we first inspect the data with a first plot. In this case we’re adding a smoothing geometry with the lm method: Linear model plot ggplot(ebike_narrow) + geom_point(aes(x = temperature, y = cycles)) + geom_smooth(aes(x = temperature, y = cycles), method = &quot;lm&quot;) + geom_point(aes(x = temperature, y = cycles_mean), color = &quot;red&quot;) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Raw data plot&quot;, x = &quot;Furnace Temperature [°C]&quot;, y = &quot;Cycles to failure [n]&quot;) Linear model fixed effects In our case the experiementer has selected to control the levels of the temperature variable in what is called a fixed effects model, accepting that conclusions in the comparisons of the levels cannot be extended to levels that were not tested. For this we’re now going to convert the variable to a factor and establish again the model and note that it will give the same R squared but naturally different coefficients. ebike_factor &lt;- ebike_narrow %&gt;% mutate(temperature = as_factor(temperature)) ebike_lm_factor &lt;- lm( cycles ~ temperature, data = ebike_factor, contrasts = list(temperature = &quot;contr.treatment&quot;) ) summary(ebike_lm_factor) Call: lm(formula = cycles ~ temperature, data = ebike_factor, contrasts = list(temperature = &quot;contr.treatment&quot;)) Residuals: Min 1Q Median 3Q Max -25400 -13000 2800 13200 25600 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 551200 8170 67.471 &lt; 2e-16 *** temperature180 36200 11553 3.133 0.00642 ** temperature200 74200 11553 6.422 8.44e-06 *** temperature220 155800 11553 13.485 3.73e-10 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 18270 on 16 degrees of freedom Multiple R-squared: 0.9261,\tAdjusted R-squared: 0.9122 F-statistic: 66.8 on 3 and 16 DF, p-value: 2.883e-09 In order to be precise, we’re making explicit in the lm function that the contrasts argument is “contr.treatment,” although this is the default in R. More on contrasts on the Case Study on \\(2^k\\) designs. The current contrasts settings can be seen as follows: getOption(&quot;contrasts&quot;) unordered ordered &quot;contr.treatment&quot; &quot;contr.poly&quot; We’re now ready to assess the validity of the model in order to be ready for our main task which is the comparison of the means using an anova. Residuals &amp; model check In order to assess the model performance we’re going to look into the residuals. R provides direct ploting functions with the base and stats packages but in this first example we’re going to break down the analysis and further customise the plots. We are also going to make usage of some additional statistical tests to confirm our observations from the plots. In subsequent chapters we’ll have a more selective approach, where plots and tests are made on a needed basis. We start by loading the package broom which will help us retrieving the data from the lm object into a data frame. Now we build and show below an extract of the “augmented” dataframe library(broom) ebike_aug &lt;- augment(ebike_lm_factor) %&gt;% mutate(index = row_number()) ebike_aug %&gt;% head() %&gt;% kable(align = &quot;c&quot;) cycles temperature .fitted .resid .std.resid .hat .sigma .cooksd index 575000 160 551200 23800 1.4566455 0.2 17571.09 0.1326135 1 542000 160 551200 -9200 -0.5630730 0.2 18678.69 0.0198157 2 530000 160 551200 -21200 -1.2975161 0.2 17846.38 0.1052218 3 539000 160 551200 -12200 -0.7466838 0.2 18534.92 0.0348460 4 570000 160 551200 18800 1.1506275 0.2 18069.13 0.0827465 5 565000 180 587400 -22400 -1.3709604 0.2 17723.81 0.1174708 6 We can see we’ve obtained detailed model parameters such us fitted values and residuals for each DOE run. Time sequence plot For this plot we need to ensure that the order of plotting in the x axis corresponds exactly to the original data collection order. This plot allows us to assess for strange patterns such as a tendency to have runs of positive of negative results which indicates that the independency assumption does not hold. If patterns emerge then there may be correlation in the residuals. ebike_aug %&gt;% ggplot(aes(x = index, y = .resid)) + geom_point() + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + labs( title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Linear model - Residuals timeseries&quot;, y = &quot;Index&quot;, x = &quot;Fitted values&quot; ) Nothing pattern emerges from the current plot and the design presents itself ^well randomised. Autocorrelation test It is always good to keep in mind that all visual observations can be complemented with a statistical test. In this case we’re going to use the durbinWatson test from the car package (Companion to Applied Regression). library(car) durbinWatsonTest(ebike_lm_factor) lag Autocorrelation D-W Statistic p-value 1 -0.5343347 2.960893 0.092 Alternative hypothesis: rho != 0 Although the output shows Autocorrelation of -0.53 we have to consider that the p value is greater than 0.05 thus there is not enough significance to say that there is autocorrelation. Residuals vs fit plot If the model is correct and the assumptions hold, the residuals should be structureless. In particular they should be unrelated to any other variable including the predicted response. ebike_aug %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE, color = &quot;red&quot;) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + labs( title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Linear model - Residuals vs Fitted values&quot;, y = &quot;Residuals&quot;, x = &quot;Fitted values&quot; ) In this plot we see no variance anomalies such as a higher variance for a certain factor level or other types of skweness. Equality of variance test In the e-bike hardening process, the normality assumption is not in question, so we can apply Bartlett’s test to the etch rate data. bartlett.test(cycles ~ temperature, data = ebike_factor) Bartlett test of homogeneity of variances data: cycles by temperature Bartlett&#39;s K-squared = 0.43349, df = 3, p-value = 0.9332 The P-value is P = 0.934, so we cannot reject the null hypothesis. There is no evidence to counter the claim that all five variances are the same. This is the same conclusion reached by analyzing the plot of residuals versus fitted values. Notes: * the var.test function cannot be used here as it applies to the two levels case only * this test is sensitive to the normality assumption, consequently, when the validity of this assumption is doubtful, the Bartlett test should not be used and replace by the modified Levene test for example Normality plot As the sample size is relatively small we’re going to use a qq plot instead of an histogram to assess the normality of the residuals. ebike_aug %&gt;% ggplot(aes(sample = .resid)) + geom_qq() + geom_qq_line() + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + labs( title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Linear model - qq plot&quot;, y = &quot;Residuals&quot;, x = &quot;Fitted values&quot; ) The plot suggests normal distribution. We see that the error distribution is aproximately normal. In the fixed effects model we give more importance to the center of the values and here we consider acceptable that the extremes of the data tend to bend away from the straight line. The verification can be completed by a test. For populations &lt; 50 use the shapiro-wilk normality test. Shapiro test shapiro.test(ebike_aug$.resid) Shapiro-Wilk normality test data: ebike_aug$.resid W = 0.93752, p-value = 0.2152 p &gt; 0.05 indicates that the residuals do not differ significantly from a normally distributed population. Std residuals vs fit plot This specific Standardized residuals graph also help detecting outliers in the residuals (any residual &gt; 3 standard deviations is a potential outlier). ebike_aug %&gt;% ggplot(aes(x = .fitted, y = .std.resid)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE, color = &quot;red&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Linear model - Standardised Residuals vs Fitted values&quot;, y = &quot;Standardised Residuals&quot;, x = &quot;Fitted values&quot;) The plot shows no outliers to consider in this DOE. Outlier test In a case where we were doubtfull we could go further and make a statistical test to assess if a certain value was an outlier. A usefull test is available in the car package. outlierTest(ebike_lm_factor) No Studentized residuals with Bonferroni p &lt; 0.05 Largest |rstudent|: rstudent unadjusted p-value Bonferroni p 12 1.648813 0.11997 NA In this case, the Bonferroni adjusted p value comes as NA confirming that there is no outlier in the data. Cooks distance plot ebike_aug %&gt;% ggplot(aes(x = .cooksd, y = .std.resid)) + geom_point() + geom_vline(xintercept = 0.5, color = &quot;red&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Residuals vs Leverage&quot;, y = &quot;Standardised Residuals&quot;, x = &quot;Cooks distance&quot;) R squared R² the coefficient of determination The R square can be extracted from the linear model that has been used to build the Anova model. summary(ebike_lm_factor)$r.squared [1] 0.9260598 Thus, in the e-bike hardening process, the factor “temperature” explains about 88% percent of the variability in etch rate. Anova fixed effects assumes that: - errors are normally distributed and are independent As the number of residuals is too small we’re not checking the normality via the histogram but rather with a a Q-Q plot. Multiple means comparison Box plot of raw data We can also compare medians and get a sense of the effect of the treatment levels by looking into the box plot: ggplot(ebike_factor, aes(x = temperature, y = cycles, fill = temperature)) + geom_boxplot() + scale_fill_viridis_d(option = &quot;D&quot;, begin = 0.5) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Raw data plot&quot;, x = &quot;Furnace Temperature [°C]&quot;, y = &quot;Cycles to failure [n]&quot;) 1 factor with severals levels + 1 continuous dependent variable Similar to the t-test but extended - this test allows to compare the means between several levels of treatement for a continuous response variable (the t test is only 2 levels at a time, performing all pair wise t-tests would also not be a solution because its a lot of effort and would increase the type I error) ANOVA principle: the total variability in the data, as measured by the total corrected sum of squares, can be partitioned into a sum of squares of the differences between the treatment averages and the grand average plus a sum of squares of the differences of observations within treatments from the treatment average Anova fixed effects In R the anova is built by passing the linear model to the anova or aov functions. The output of the anova function is just the anova table as shown here for this first example. The output of the aov function is a list. ebike_aov_factor &lt;- aov(ebike_lm_factor) summary(ebike_aov_factor) Df Sum Sq Mean Sq F value Pr(&gt;F) temperature 3 6.687e+10 2.229e+10 66.8 2.88e-09 *** Residuals 16 5.339e+09 3.337e+08 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that the RF temperature or between-treatment mean square (22,290.18) is many times larger than the within-treatment or error mean square (333.70). This indicates that it is unlikely that the treatment means are equal. Also p &lt; 0.05 thus we can reject the null hypothesis and conclude that the means are significantly different. Anova (no significance) Anova on plasma etching, modification of the example to achieve a p &gt; 0.05: ebike_narrow2 &lt;- ebike_hardening2 %&gt;% pivot_longer( cols = starts_with(&quot;g&quot;), names_to = &quot;observation&quot;, values_to = &quot;cycles&quot; ) %&gt;% group_by(temperature) %&gt;% mutate(cycles_mean = mean(cycles)) %&gt;% ungroup() ebike_factor2 &lt;- ebike_narrow2 ebike_factor2$temperature &lt;- as.factor(ebike_factor2$temperature) ebike_lm_factor2 &lt;- lm(cycles ~ temperature, data = ebike_factor2) anova(ebike_lm_factor2) Analysis of Variance Table Response: cycles Df Sum Sq Mean Sq F value Pr(&gt;F) temperature 3 1.476e+09 492000000 1.2015 0.341 Residuals 16 6.552e+09 409500000 ggplot(ebike_factor2, aes(x = temperature, y = cycles, fill = temperature)) + geom_boxplot() + scale_y_continuous(n.breaks = 10) + scale_fill_viridis_d(option = &quot;A&quot;, begin = 0.5) + theme(legend.position = &quot;none&quot;) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Boxplot of frame aging resistance&quot;, x = &quot;Furnace Temperature [°C]&quot;, y = &quot;Cycles to failure [n]&quot;) P &gt; 0.05 - there is no significant difference between the means Pairwise comparisons Tukey’s test The Anova may indicate that the treament means differ but it won’t indicate which ones. In this case we may want to compare pairs of means. ebike_tukey &lt;- TukeyHSD(ebike_aov_factor, ordered = TRUE) head(ebike_tukey$temperature) %&gt;% kable(align = &quot;c&quot;, caption = &quot;tukey test on e-bike frame hardening process&quot;, booktabs = T) Table 3: tukey test on e-bike frame hardening process diff lwr upr p adj 180-160 36200 3145.624 69254.38 0.0294279 200-160 74200 41145.624 107254.38 0.0000455 220-160 155800 122745.624 188854.38 0.0000000 200-180 38000 4945.624 71054.38 0.0215995 220-180 119600 86545.624 152654.38 0.0000001 220-200 81600 48545.624 114654.38 0.0000146 The test provides us a simple direct calculation of the differences between the treatment means and a confidence interval for those. Most importantly it provides us with the p value to help us confirm the significance of the difference and conclude factor level by factor level which differences are significant. Additionally we can obtain the related plot with the confidence intervals plot(ebike_tukey) Fisher’s LSD Fisher’s Least Significant difference is an alternative to Tuckey’s test. library(agricolae) ebike_anova &lt;- anova(ebike_lm_factor) ebike_LSD &lt;- LSD.test(y = ebike_factor$cycles, trt = ebike_factor$temperature, DFerror = ebike_anova$Df[2], MSerror = ebike_anova$`Mean Sq`[2], alpha = 0.05) The Fisher procedure provides us with many additional information. A first outcome is the difference between means (of life cycles) that can be considered significant, indicated in the table below by LSD = 24.49. head(ebike_LSD$statistics) %&gt;% kable(align = &quot;c&quot;, caption = &quot;Fisher LSD procedure on e-bike frame hardening: stats&quot;, booktabs = T) Table 4: Fisher LSD procedure on e-bike frame hardening: stats MSerror Df Mean CV t.value LSD 333700000 16 617750 2.957095 2.119905 24492.02 Furthermore it gives us a confidence interval for each treatment level mean: head(ebike_LSD$means) %&gt;% # as_tibble() %&gt;% rename(cycles = `ebike_factor$cycles`) %&gt;% select(-Min, -Max, -Q25, -Q50, -Q75) %&gt;% kable(align = &quot;c&quot;, caption = &quot;Fisher LSD procedure on e-bike frame hardening: means&quot;, booktabs = T) Table 5: Fisher LSD procedure on e-bike frame hardening: means cycles std r LCL UCL 160 551200 20017.49 5 533881.5 568518.5 180 587400 16742.16 5 570081.5 604718.5 200 625400 20525.59 5 608081.5 642718.5 220 707000 15247.95 5 689681.5 724318.5 We can see for example that for temperature 220 °C the etch rate if on average 707.0 with a probability of 95% of being between 689.7 and 724.3 A/min. Another interesting outcome is the grouping of levels for each factor: head(ebike_LSD$groups) %&gt;% kable(align = &quot;c&quot;, caption = &quot;Fisher LSD procedure on e-bike frame hardening: groups&quot;, booktabs = T) Table 6: Fisher LSD procedure on e-bike frame hardening: groups ebike_factor$cycles groups 220 707000 a 200 625400 b 180 587400 c 160 551200 d In this case as all level means are statistically different they all show up in separate groups, each indicated by a specific letter. Finally we can get from this package a plot with the Least significant difference error bars: plot(ebike_LSD) And below we’re exploring a manual execution of this type of plot (in this case with the standard deviations instead). ebike_factor %&gt;% group_by(temperature) %&gt;% summarise(cycles_mean = mean(cycles), cycles_sd = sd(cycles)) %&gt;% ggplot(aes(x = temperature, y = cycles_mean)) + geom_point(size = 2) + geom_line() + geom_errorbar(aes(ymin = cycles_mean - cycles_sd, ymax = cycles_mean + cycles_sd), width = .1) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + # scale_color_viridis_d(option = &quot;C&quot;, begin = 0.1, end = 0.9) + annotate(geom = &quot;text&quot;, x = Inf, y = -Inf, label = &quot;Error bars are +/- 1xSD&quot;, hjust = 1, vjust = -1, colour = &quot;grey30&quot;, size = 3, fontface = &quot;italic&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Boxplot of frame aging resistance&quot;, x = &quot;Furnace Temperature [°C]&quot;, y = &quot;Cycles to failure [n]&quot;) As often with statistical tools, there is debate on the best approach to use. We recommend to combine the Tukey test with the Fisher’s LSD completementary R functions. The Tukey test giving a first indication of the levels that have an effect and calculating the means differences and the Fisher function to provide much more additional information on each level. To be considered in each situation the slight difference between the significance level for difference between means and to decide if required to take the most conservative one. To go further in the Anova F-test we recommend this interesting article from Minitab (2016). Prediction Following the residuals analysis and the anova our model is validated. A model is usefull for predictions. In a random effects model where conclusions can applied to the all the population we can predict values at any value of the input variables. In that case reusing the model with temperature as a numeric vector we could have a prediction for various temperature values such as: ebike_new &lt;- tibble(temperature = c(170, 160, 200, 210)) predict(ebike_lm, newdata = ebike_new) 1 2 3 4 567210 541940 643020 668290 We can see that the prediction at the tested levels is slightly different from the measured averages at those levels. This is because the linear interpolation line is not passing exactly by the averages. Anyway this is a fixed effects model and we can only take conclusions at the levels at which the input was tested. We can check that the predictions correspond to the averages we’ve calculated for each level: ebike_new &lt;- data.frame(temperature = as_factor(c(&quot;160&quot;, &quot;200&quot;))) predict(ebike_lm_factor, newdata = ebike_new) 1 2 551200 625400 library(tidyverse) library(readxl) library(stats) library(knitr) library(industRial) filter &lt;- dplyr::filter select &lt;- dplyr::select Interactions Two factors multiple levels The solarcell output test Figure 6: solar panel test chamber Load and prepare data for analysis: solarcell_factor &lt;- solarcell_output %&gt;% pivot_longer( cols = c(&quot;T-10&quot;, &quot;T20&quot;, &quot;T50&quot;), names_to = &quot;temperature&quot;, values_to = &quot;output&quot; ) %&gt;% mutate(across(c(material, temperature), as_factor)) lm with interactions solarcell_factor_lm &lt;- lm( output ~ temperature + material + temperature:material, data = solarcell_factor ) summary(solarcell_factor_lm) Call: lm(formula = output ~ temperature + material + temperature:material, data = solarcell_factor) Residuals: Min 1Q Median 3Q Max -60.750 -14.625 1.375 17.937 45.250 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 134.75 12.99 10.371 6.46e-11 *** temperatureT20 -77.50 18.37 -4.218 0.000248 *** temperatureT50 -77.25 18.37 -4.204 0.000257 *** materialchristaline 21.00 18.37 1.143 0.263107 materialmultijunction 9.25 18.37 0.503 0.618747 temperatureT20:materialchristaline 41.50 25.98 1.597 0.121886 temperatureT50:materialchristaline -29.00 25.98 -1.116 0.274242 temperatureT20:materialmultijunction 79.25 25.98 3.050 0.005083 ** temperatureT50:materialmultijunction 18.75 25.98 0.722 0.476759 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 25.98 on 27 degrees of freedom Multiple R-squared: 0.7652,\tAdjusted R-squared: 0.6956 F-statistic: 11 on 8 and 27 DF, p-value: 9.426e-07 Looking at the output we see that R-squared is equal to 0.7652. This means about 77 percent of the variability in the battery life is explained by the plate material in the battery, the temperature, and the material type–temperature interaction. We’re going to go more in details now to validate the model and understand the effects and interactions of the different factors. Outliers and model check We start by an assessment of the residuals, starting by the timeseries of residuals: plot(solarcell_factor_lm$residuals) No specific pattern is apparent so now we check all the remaining plots grouped into one single output: par(mfrow = c(2,2)) plot(solarcell_factor_lm) Residuals versus fit presents a rather simetrical distribution around zero indicating equality of variances at all levels and the qq plot presents good adherence to the centel line indicating a normal distributed population of residuals, all ok for these. The scale location plot though, shows a center line that is not horizontal which suggest the presence of outliers. We can extract the absolute maximum residual with: solarcell_factor_lm$residuals %&gt;% abs() %&gt;% max() [1] 60.75 Inspecting again the residuals plots we see that this corresponds to the point labeled with 2 for which the standardized value is greater than 2 standard deviations. We’re therefore apply the outlier test from the car package: library(car) outlierTest(solarcell_factor_lm) No Studentized residuals with Bonferroni p &lt; 0.05 Largest |rstudent|: rstudent unadjusted p-value Bonferroni p 4 -3.100368 0.0046065 0.16583 which gives a high Bonferroni p value thus excluding this possibility. Interaction plot In this experiement instead of just plotting a linear regression we need to go for a more elaborate plot that shows the response as a function of the two factors. Many different approaches are possible in R and here we’re starting with a rather simple one - the interaction plot from the stats package: interaction.plot(x.factor = solarcell_factor$temperature, trace.factor = solarcell_factor$material, fun = mean, response = solarcell_factor$output, trace.label = &quot;Material&quot;, legend = TRUE, main = &quot;Temperature-Material interaction plot&quot;, xlab = &quot;temperature [°C]&quot;, ylab = &quot;output [kWh/yr equivalent]&quot;) Although simple many important learnings can be extracted from this plot. We get the indication of the mean value of battery life for the different data groups at each temperature level for each material. Also we see immediatly that batteries tend to have longer lifes at lower temperature for all material types. We also see that there is certainly an interaction between material and temperature as the lines cross each other. Effects significance As the R-squared was rather high and there were no issues with residuals we considere the model as acceptable and move ahead with the assessment of the significance of the different effects. For that we apply the anova to the linear model: solarcell_factor_aov &lt;- aov(solarcell_factor_lm) summary(solarcell_factor_aov) Df Sum Sq Mean Sq F value Pr(&gt;F) temperature 2 39119 19559 28.968 1.91e-07 *** material 2 10684 5342 7.911 0.00198 ** temperature:material 4 9614 2403 3.560 0.01861 * Residuals 27 18231 675 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see in the output little stars in front of the p value of the different factors. Three stars for temperature corresponding to an extremely low p value indicating that the means of the lifetime at different levels of temperature are significantly different, confirming that temperature has an effect on lifetime. With a lower significance but still clearly impacting lifetime depends on the material. Finally it is confirmed that there is an interaction between both factors has the temperature:material term has a p value of 0.01861 which us lower than the treshold of 0.05. The interaction here corresponds to the fact that increasing temperature from 15 to 70 decreases lifetime for material 2 but increases for material 3. Removing interaction Its interesting to consider what would have been the analysis if the interaction was not put in the model. We can easily assess that by creating a new model in R without the temperature:material term. solarcell_factor_lm_no_int &lt;- lm( output ~ temperature + material, data = solarcell_factor) summary(solarcell_factor_lm_no_int) Call: lm(formula = output ~ temperature + material, data = solarcell_factor) Residuals: Min 1Q Median 3Q Max -54.389 -21.681 2.694 17.215 57.528 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 122.47 11.17 10.965 3.39e-12 *** temperatureT20 -37.25 12.24 -3.044 0.00472 ** temperatureT50 -80.67 12.24 -6.593 2.30e-07 *** materialchristaline 25.17 12.24 2.057 0.04819 * materialmultijunction 41.92 12.24 3.426 0.00175 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 29.97 on 31 degrees of freedom Multiple R-squared: 0.6414,\tAdjusted R-squared: 0.5951 F-statistic: 13.86 on 4 and 31 DF, p-value: 1.367e-06 The model still presents a reasonably high R-square of 0.64. We now apply the anova on this new model: battery_aov_no_int &lt;- aov(solarcell_factor_lm_no_int) summary(battery_aov_no_int) Df Sum Sq Mean Sq F value Pr(&gt;F) temperature 2 39119 19559 21.776 1.24e-06 *** material 2 10684 5342 5.947 0.00651 ** Residuals 31 27845 898 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The output naturally confirms the significance of the effects of the factors, however, as soon as a residual analysis is performed for these data, it becomes clear that the non-interaction model is inadequate: par(mfrow = c(2,2)) plot(solarcell_factor_lm_no_int) We see in the Residuals vs Fitted a clear pattern with residuals moving from positive to negative and then again to positive along the fitted values axis which indicates that there is an interaction at play. Covariance We assess here the potential utilisation of the analysis of covariance (ancova) in situations where a continuous variable may be influencing the measured value. This technique complements the analysis of variance (anova) allowing for a more accurate assessment of the effects of the categorical variables. Below a description of the approach taken from (Montgomery 2012), pag.655: Suppose that in an experiment with a response variable y there is another variable, say x, and that y is linearly related to x. Furthermore, suppose that x cannot be controlled by the experimenter but can be observed along with y. The variable x is called a covariate or concomitant variable. The analysis of covariance involves adjusting the observed response variable for the effect of the concomitant variable. If such an adjustment is not performed, the concomitant variable could inflate the error mean square and make true differences in the response due to treatments harder to detect. Thus, the analysis of covariance is a method of adjusting for the effects of an uncontrollable nuisance variable. As we will see, the procedure is a combination of analysis of variance and regression analysis. As an example of an experiment in which the analysis of covariance may be employed, consider a study performed to determine if there is a difference in the strength of a monofilament fiber produced by three different machines. The data from this experiment are shown in Table 15.10 (below). Figure 15.3 presents a scatter diagram of strength (y) versus the diameter (or thickness) of the sample. Clearly, the strength of the fiber is also affected by its thickness; consequently, a thicker fiber will generally be stronger than a thinner one. The analysis of covariance could be used to remove the effect of thickness (x) on strength (y) when testing for differences in strength between machines. solarcell_fill %&gt;% kable() material output fillfactor multijunction_A 108 20 multijunction_A 123 25 multijunction_A 117 24 multijunction_A 126 25 multijunction_A 147 32 multijunction_B 120 22 multijunction_B 144 28 multijunction_B 117 22 multijunction_B 135 30 multijunction_B 132 28 multijunction_C 105 21 multijunction_C 111 23 multijunction_C 126 26 multijunction_C 102 21 multijunction_C 96 15 Below a plot of strenght by thickness: solarcell_fill %&gt;% ggplot(aes(x = fillfactor, y = output)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_industRial() + labs( title = &quot;The solarcell output test&quot;, subtitle = &quot;Output vs Fill Factor&quot;, x = &quot;Fill factor [%]&quot;, y = &quot;Output&quot; ) Correlation strenght And a short test to assess the strenght of the correlation: library(stats) cor.test(solarcell_fill$output, solarcell_fill$fillfactor) Pearson&#39;s product-moment correlation data: solarcell_fill$output and solarcell_fill$fillfactor t = 9.8039, df = 13, p-value = 2.263e-07 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.8209993 0.9797570 sample estimates: cor 0.938542 Going further and using the approach from (Broc 2016) I’m faceting the scatterplots to assess if the coefficient of the linear regression is similar for all the levels of the machine factor: solarcell_fill %&gt;% ggplot(aes(x = fillfactor, y = output)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_wrap(vars(material)) + theme_industRial() + labs( title = &quot;The solarcell output test&quot;, subtitle = &quot;Output vs Fill Factor, by material type&quot;, x = &quot;Fill factor [%]&quot;, y = &quot;Output&quot; ) Visually this is the case, going from one level to the other is not changing the relationship between thickness and strenght - increasing thickness increases stenght. Visually the slopes are similar but the number of points is small. In a real case this verification could be extended with the correlation test for each level or/and a statistical test between slopes. We’re now reproducing in R the ancova case study from the book, still using the aov function. The way to feed the R function arguments is obtained from https://www.datanovia.com/en/lessons/ancova-in-r/ Three different machines produce a monofilament fiber for a textile company. The process engineer is interested in determining if there is a difference in the breaking strength of the fiber produced by the three machines. However, the strength of a fiber is related to its diameter, with thicker fibers being generally stronger than thinner ones. A random sample of five fiber specimens is selected from each machine. Ancova solarcell_ancova &lt;- aov( output ~ fillfactor + material, solarcell_fill ) summary(solarcell_ancova) Df Sum Sq Mean Sq F value Pr(&gt;F) fillfactor 1 2746.2 2746.2 119.933 2.96e-07 *** material 2 119.6 59.8 2.611 0.118 Residuals 11 251.9 22.9 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that in the formula the covariate goes first (and there is no interaction)! If you do not do this in order, you will get different results. material in this table corresponds to the adjusted material mean square Conclusions from the book in page 662: Comparing the adjusted treatment means with the unadjusted treatment means (the y i. ), we note that the adjusted means are much closer together, another indication that the covariance analysis was necessary. A basic assumption in the analysis of covariance is that the treatments do not influence the covariate x because the technique removes the effect of variations in the x i. . However, if the variability in the x i. is due in part to the treatments, then analysis of covariance removes part of the treatment effect. Thus, we must be reasonably sure that the treatments do not affect the values x ij. In some experiments this may be obvious from the nature of the covariate, whereas in others it may be more doubtful. In our example, there may be a difference in fiber diameter (x ij ) between the three machines. In such cases, Cochran and Cox (1957) suggest that an analysis of variance on the x ij values may be helpful in determining the validity of this assumption. …there is no reason to believe that machines produce fibers of different diameters. (I did not go further here as it goes beyond the scope of the assessment) Comparison with anova Below I’m doing the common approach we’ve been using at NSTC in design of experiments. solarcell_aov &lt;- aov(output ~ material, solarcell_fill) summary(solarcell_aov) Df Sum Sq Mean Sq F value Pr(&gt;F) material 2 1264 631.8 4.089 0.0442 * Residuals 12 1854 154.5 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The anova table obtained also corresponds correctly to the book example. Montgomery final observations: It is interesting to note what would have happened in this experiment if an analysis of covariance had not been performed, that is, if the breaking strength data (y) had been analyzed as a completely randomized single-factor experiment in which the covariate x was ignored. The analysis of variance of the breaking strength data is shown in Table 15.14. We immediately notice that the error estimate is much longer in the CRD analysis (17.17 versus 2.54). This is a reflection of the effectiveness of analysis of covariance in reducing error variability. We would also conclude, based on the CRD analysis, that machines differ significantly in the strength of fiber produced. This is exactly opposite the conclusion reached by the covariance analysis. If we suspected that the machines differed significantly in their effect on fiber strength, then we would try to equalize the strength output of the three machines. However, in this problem the machines do not differ in the strength of fiber produced after the linear effect of fiber diameter is removed. It would be helpful to reduce the within-machine fiber diameter variability because this would probably reduce the strength variability in the fiber. Potential applications In the scope of methods validations this approach could potentially be used in robustness validations when there is suspiction that a continuous variable is disturbing the measurement. Naturally this should not be applied everywhere but only where there would to be logical a physical or chemical reason behind as in the example with thickness and strenght. General factorial designs m factors n levels designs The juice production plant We’re comming back to our Juice Bottling context where a quality team was looking to put in operation a new measurement device for dry matter content in a juices bottling line. After a short brainstorming using the Ishikawa tool presented before the team has identified several potential influcing parameters on the equipment bias when compared with the reference equipement: the product itself, the drymatter level on the product (its target), the speed of the filling line and the poweder particle size. In order to evaluate such impact the team has prepared a mid size experiment design with three products, three levels of drymatter, two line speed levels and two particle size levels. First we load the DoE.base package: library(DoE.base) and then generate the doe with the fac.design function. Design generaction juice_doe &lt;- fac.design( randomize = FALSE, factor.names = list( product = c(&quot;beetroot&quot;, &quot;apple&quot;, &quot;carrot&quot;), drymatter_target = c(10, 15, 20), part = c(1, 2, 3), speed = c(20, 25), particle_size = c(250, 300)) ) Note that the DoE generated is more than just a tibble, it belongs to a specific class called design and has many other attributes just like an lm or aov S3 objects. class(juice_doe) [1] &quot;design&quot; &quot;data.frame&quot; The power and care given by the package authors become visible when we use an R generic function such as summary() with this object and we see it returns a tailor made output, in this case showing the levels of the different factors of our design: summary(juice_doe) Call: fac.design(randomize = FALSE, factor.names = list(product = c(&quot;beetroot&quot;, &quot;apple&quot;, &quot;carrot&quot;), drymatter_target = c(10, 15, 20), part = c(1, 2, 3), speed = c(20, 25), particle_size = c(250, 300))) Experimental design of type full factorial 108 runs Factor settings (scale ends): product drymatter_target part speed particle_size 1 beetroot 10 1 20 250 2 apple 15 2 25 300 3 carrot 20 3 Using this the team has simple copied the experiment plan to an spreadsheet to collect the data: juice_doe %&gt;% write_clip() and after a few day the file completed and ready for analysis looked like: juice_drymatter %&gt;% head() %&gt;% kable() product drymatter_TGT speed particle_size part drymatter_DRX drymatter_REF apple 10 20 250 1 9.80 10.05 apple 10 20 250 2 9.82 10.05 apple 10 20 250 3 9.82 10.05 apple 15 20 250 1 14.70 15.02 apple 15 20 250 2 14.70 15.02 apple 15 20 250 3 14.70 15.02 juice_drymatter &lt;- juice_drymatter %&gt;% mutate(bias = drymatter_DRX - drymatter_REF) Main effects plots As the number of factors and levels of a design increase, more thinking is required to obtain good visualisation of the data. Main effects plots consist usually of a scatterplot representing the experiment output as a function of one of the inputs. In a design like this with three different inputs three plots are required: drymatter_TGT_plot &lt;- juice_drymatter %&gt;% group_by(drymatter_TGT) %&gt;% summarise(bias_m_drymatter = mean(bias)) %&gt;% ggplot(aes(x = drymatter_TGT, y = bias_m_drymatter)) + geom_point() + geom_line() + coord_cartesian( xlim = c(9,21), ylim = c(-1,0), expand = TRUE) + labs( title = &quot;Juice bottling problem&quot;, subtitle = &quot;Main effects plots&quot;, x = &quot;drymatter_TGT [%]&quot;, y = &quot;Average bias [g]&quot; ) particle_size_plot &lt;- juice_drymatter %&gt;% group_by(particle_size) %&gt;% summarise(particle_size_bias_mean = mean(bias)) %&gt;% ggplot(aes(x = particle_size, y = particle_size_bias_mean)) + geom_point() + geom_line() + coord_cartesian( xlim = c(240,310), ylim = c(-1,0), expand = TRUE) + labs( x = &quot;particle_size&quot;, y = &quot;Average bias [g]&quot; ) speed_plot &lt;- juice_drymatter %&gt;% group_by(speed) %&gt;% summarise(speed_bias_mean = mean(bias)) %&gt;% ggplot(aes(x = speed, y = speed_bias_mean)) + geom_point() + geom_line() + coord_cartesian( xlim = c(19, 26), ylim = c(-1,0), expand = TRUE) + labs( x = &quot;Speed&quot;, y = &quot;Average bias [g]&quot; ) drymatter_TGT_plot + particle_size_plot + speed_plot This kind of plots gives already important insights in to the experiement outcome, even before any deeper analysis with a linear model and anova. In our case: higher particle_size and higher speed result in higher bias weight deviation beyond 10.5% drymatter_TGT level the bias weight is always higher than the target Interactions plots In designs like these with 3 factors we have 3 possible interactions (A-B, A-C, B-C) corresponding the the possible combination between them. This results in three interaction plots that we’re presenting below. The approach here goes beyond the interaction.plot function from the {stats} package presented previously in the two factors multiple levels case. We are developping here the plots with {ggplot2} which provides much more control on the plot attibutes but on the other hand requires that additional code is added to calculate the means by group. drymatter_TGT_particle_size_plot &lt;- juice_drymatter %&gt;% group_by(drymatter_TGT, particle_size) %&gt;% summarise(drymatter_TGT_bias_mean = mean(bias)) %&gt;% ggplot(aes(x = drymatter_TGT, y = drymatter_TGT_bias_mean)) + geom_point(aes(group = particle_size)) + geom_line(aes(group = particle_size, linetype = as_factor(particle_size))) + scale_linetype(name = &quot;particle_size&quot;) + coord_cartesian( xlim = c(9,21), ylim = c(-1,0), expand = TRUE) + labs( title = &quot;Juice bottling problem&quot;, subtitle = &quot;Interaction plots&quot;, x = &quot;drymatter_TGT&quot;, y = &quot;Average bias deviation [g]&quot; ) + theme_industRial() + theme(legend.justification=c(1,0), legend.position=c(1,0)) drymatter_TGT_speed_plot &lt;- juice_drymatter %&gt;% group_by(drymatter_TGT, speed) %&gt;% summarise(drymatter_TGT_bias_mean = mean(bias)) %&gt;% ggplot(aes(x = drymatter_TGT, y = drymatter_TGT_bias_mean)) + geom_point(aes(group = speed)) + geom_line(aes(group = speed, linetype = as_factor(speed))) + scale_linetype(name = &quot;Speed&quot;) + coord_cartesian( xlim = c(9, 21), ylim = c(-1,0), expand = TRUE) + labs( x = &quot;drymatter_TGT&quot;, y = &quot;Average bias deviation [g]&quot; ) + theme_industRial() + theme(legend.justification=c(1,0), legend.position=c(1,0)) speed_particle_size_plot &lt;- juice_drymatter %&gt;% group_by(speed, particle_size) %&gt;% summarise(speed_bias_mean = mean(bias)) %&gt;% ggplot(aes(x = speed, y = speed_bias_mean)) + geom_point(aes(group = particle_size)) + geom_line(aes(group = particle_size, linetype = as_factor(particle_size))) + scale_linetype(name = &quot;particle_size&quot;) + coord_cartesian( xlim = c(19, 26), ylim = c(-1,0), expand = TRUE) + labs( x = &quot;Speed&quot;, y = &quot;Average bias deviation [g]&quot; ) + theme_industRial() + theme(legend.justification=c(1,0), legend.position=c(1,0)) drymatter_TGT_particle_size_plot + drymatter_TGT_speed_plot + speed_particle_size_plot The plots indicate no interaction between the different factors as all lines do not intercept and are mostly parallel. In most cases the anova would be performed first and only the plot for the significant interactions would be plotted, if any. Anova with 3rd level interactions The sources of variation for the Anova table for three-factor fixed effects model are: A, B, C, AB, AC, BC, ABC. To be noted that like in the two-factors we must have at least two parts (n&gt;2) to determine the sum of squares due to error if all possible interactions are to be included in the model. We are now fully prepared for an assessment of the effect of the different factors with the anova. To reduce the amount of coding we’re inputing the model directly in the aov function: juice_drymatter_aov &lt;- aov( bias ~ drymatter_TGT * speed * particle_size, data = juice_drymatter) summary(juice_drymatter_aov) Df Sum Sq Mean Sq F value Pr(&gt;F) drymatter_TGT 1 1.3149 1.3149 486.057 &lt;2e-16 *** speed 1 0.0000 0.0000 0.000 0.985 particle_size 1 0.6241 0.6241 230.705 &lt;2e-16 *** drymatter_TGT:speed 1 0.0007 0.0007 0.272 0.603 drymatter_TGT:particle_size 1 0.0028 0.0028 1.040 0.310 speed:particle_size 1 0.0032 0.0032 1.191 0.278 drymatter_TGT:speed:particle_size 1 0.0039 0.0039 1.442 0.233 Residuals 100 0.2705 0.0027 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The observations of the plots are confirmed and completed with statistical input: we see that the percentage of drymatter_TGT and the particle_size significantly affect the bias volume (p &lt; 0.05). The drymatter_TGT-particle_size interactions are non significative. As expected the anova confirms strong influence of the dissolution level on the bias. From the analysis all interactions could be removed from the model in order to establish a predictive model. Two level designs Coding factors 2 factors 2 levels The \\(2^{k}\\) designs are particularly useful in the early stages of experimental work when many factors are likely to be investigated. It provides the smallest number of runs with which k factors can be studied in a complete factorial design. Consequently, these designs are widely used in factor screening experiments. The validity of the analysis depends on the following assumptions: the factors are fixed the designs are completely randomized the usual normality assumptions are satisfied the response is approximately linear over the range of the factor levels chosen Analysis Procedure for a 2 k Design Estimate factor effects Form initial model (full model) If the design is replicated, fit the full model If there is no replication, form the model using a normal probability plot of the effects Perform statistical testing (Anova) Refine model (remove non significant effects) Analyze residuals Interpret results DEF - Sparsity of effects principle: most systems are dominated by some of the main effects and low-order interactions, and most high-order interactions are negligible. In this first Case Study dedicated to \\(2^k\\) designs we’re going to explore the contrasts settings in the linear model functions. The PET clothing improvement plan In this case study factors have only 2 levels. Below we start by preparing our dataset: library(DoE.base) pet_doe &lt;- fac.design( randomize = FALSE, factor.names=list(A=c(&quot;-&quot;,&quot;+&quot;), B=c(&quot;-&quot;,&quot;+&quot;), replicate = c(&quot;I&quot;, &quot;II&quot;, &quot;III&quot;)) ) tensile_strength &lt;- c(64.4,82.8,41.4,71.3,57.5,73.6,43.7,69.0,62.1,73.6,52.9,66.7) pet_doe &lt;- bind_cols( pet_doe, &quot;tensile_strength&quot; = tensile_strength, ) Factors as +/- In this first model we’re using a design where the inputs levels have been defined as plus and minus, sometimes also called high and low. The actual naming is not important, what is critical is to ensure that those input parameters are coded as factors. pet_fct &lt;- pet_doe %&gt;% mutate(across(c(A,B), as_factor)) Another detail is to put the higher level as the reference otherwise we will get inverted signs in the lm output: pet_fct$A &lt;- relevel(pet_fct$A, ref=&quot;+&quot;) pet_fct$B &lt;- relevel(pet_fct$B, ref=&quot;+&quot;) and one final step is need which is the setup of the contrasts. As our design is ortogonal and we want the contrasts to add up to zero we have to indicate that on the factor so that the coefficients of the linear model are correctly calculated. The current definition of the contrasts is: contrasts(pet_fct$A) - + 0 - 1 So we change this with: contrasts(pet_fct$A) &lt;- &quot;contr.sum&quot; contrasts(pet_fct$B) &lt;- &quot;contr.sum&quot; contrasts(pet_fct$A) [,1] + 1 - -1 contrasts(pet_fct$A) [,1] + 1 - -1 Now we can run our linear model: pet_ctr_lm &lt;- lm( formula = tensile_strength ~ A * B, data = pet_fct ) summary(pet_ctr_lm) Call: lm.default(formula = tensile_strength ~ A * B, data = pet_fct) Residuals: Min 1Q Median 3Q Max -4.600 -3.067 -1.150 2.492 6.900 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 63.250 1.314 48.135 3.84e-11 *** A1 9.583 1.314 7.293 8.44e-05 *** B1 -5.750 1.314 -4.376 0.00236 ** A1:B1 1.917 1.314 1.459 0.18278 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.552 on 8 degrees of freedom Multiple R-squared: 0.903,\tAdjusted R-squared: 0.8666 F-statistic: 24.82 on 3 and 8 DF, p-value: 0.0002093 We can observe in the output that the p value of the effects is the same in the lm and in the the aov functions. This confirms that the contrasts have been correctly specified with contr.sum Note that we’ve had to adjust the contrasts in the lm function with contr.sum which applies to cases where the sum of the contrasts is zero (the R default is contr.treatment which applies to cases where the levels are coded as 0 and 1). and now going to apply a prediction: predict(pet_ctr_lm, newdata = list(A = &quot;+&quot;, B = &quot;+&quot;)) 1 69 Factors as +/- 1 In this example we convert the levels to factors still using the +/-1 notation. This will also be helpfull to apply what are called the Yates tables. coded &lt;- function(x) { ifelse(x == x[1], -1, 1) } We again convert them to factors and put the upper level as the reference. Regarding the contrasts we show a simpler and more direct approach now by defining them directly in the lm() function. pet_fct &lt;- pet_fct %&gt;% mutate(cA = coded(A), cB = coded(B)) pet_fct2 &lt;- pet_fct %&gt;% mutate(across(c(cA, cB), as_factor)) pet_fct2$cA &lt;- relevel(pet_fct2$cA, ref = &quot;1&quot;) pet_fct2$cB &lt;- relevel(pet_fct2$cB, ref = &quot;1&quot;) pet_ctr2_lm &lt;- lm( formula = tensile_strength ~ cA * cB, data = pet_fct2, contrasts = list(cA = &quot;contr.sum&quot;, cB = &quot;contr.sum&quot;) ) summary(pet_ctr2_lm) Call: lm.default(formula = tensile_strength ~ cA * cB, data = pet_fct2, contrasts = list(cA = &quot;contr.sum&quot;, cB = &quot;contr.sum&quot;)) Residuals: Min 1Q Median 3Q Max -4.600 -3.067 -1.150 2.492 6.900 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 63.250 1.314 48.135 3.84e-11 *** cA1 9.583 1.314 7.293 8.44e-05 *** cB1 -5.750 1.314 -4.376 0.00236 ** cA1:cB1 1.917 1.314 1.459 0.18278 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.552 on 8 degrees of freedom Multiple R-squared: 0.903,\tAdjusted R-squared: 0.8666 F-statistic: 24.82 on 3 and 8 DF, p-value: 0.0002093 Note that a coefficient in a regression equation is the change in the response when the corresponding variable changes by +1. Special attention to the + and - needs to be taken with the R output. As A or B changes from its low level to its high level, the coded variable changes by 1 − (−1) = +2, so the change in the response is twice the regression coefficient. So the effects and interaction(s) from their minumum to their maximum correspond to twice the values in the “Estimate” column. These regression coefficients are often called effects and interactions, even though they differ from the definitions used in the designs themeselves. Checking now with coded factors: predict(pet_ctr2_lm, newdata = list(cA = &quot;1&quot;, cB = &quot;1&quot;)) 1 69 Factors as +/- 1 numeric In this example we’re going to code the levels with +1/-1 but we’re going use the numeric coding: pet_num &lt;- pet_fct %&gt;% mutate(cA = coded(A), cB = coded(B)) pet_num_lm &lt;- lm( formula = tensile_strength ~ cA * cB, data = pet_num ) summary(pet_num_lm) Call: lm.default(formula = tensile_strength ~ cA * cB, data = pet_num) Residuals: Min 1Q Median 3Q Max -4.600 -3.067 -1.150 2.492 6.900 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 63.250 1.314 48.135 3.84e-11 *** cA 9.583 1.314 7.293 8.44e-05 *** cB -5.750 1.314 -4.376 0.00236 ** cA:cB 1.917 1.314 1.459 0.18278 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.552 on 8 degrees of freedom Multiple R-squared: 0.903,\tAdjusted R-squared: 0.8666 F-statistic: 24.82 on 3 and 8 DF, p-value: 0.0002093 In this case we did not define any contrasts. Looking into the lm We can see we’ve obtained exactly the same outputs. predict(pet_num_lm, newdata = list(cA = 1, cB = 1)) 1 69 As the inputs are coded as numeric this behaves just like the first simple linear model we’ve seen in the Case Study on One Factor with Multiple levels. In particular when we feed the predictions function with numeric values. This is very intuitive as it corresponds to the original units of the experiments (also called natural or engineering units). On the other hand coding the design variables provides another advange: generally, the engineering units are not directly comparable while coded variables are very effective for determining the relative size of factor effects. We can see that these three ways of coding the variable levels lead to equivalent results both in lm and prediction. Our preference goes to use numeric values as it is more intuitive and allows for easier prediction between the fixed levels. And now in order to better understand the coding of factors in this unit, we’re going to establish a simple regression plot of our data: pet_num %&gt;% unclass() %&gt;% as_tibble() %&gt;% mutate(cA = coded(A), cB = coded(B)) %&gt;% pivot_longer( cols = c(&quot;cA&quot;, &quot;cB&quot;), names_to = &quot;variable&quot;, values_to = &quot;level&quot;) %&gt;% ggplot() + geom_point(aes(x = level, y = tensile_strength)) + geom_smooth(aes(x = level, y = tensile_strength), method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) + facet_wrap(vars(variable)) Note that we had to extract the data from the S3 doe object, which we’ve done with using unclass() and then as_tibble() The intercept passes at 27.5 as seen on the lm summary. We’re going now to put the B factor at its maximum and replot: pet_num %&gt;% unclass() %&gt;% as_tibble() %&gt;% mutate(cA = coded(A), cB = coded(B)) %&gt;% filter(cB == 1) %&gt;% pivot_longer( cols = c(&quot;cA&quot;, &quot;cB&quot;), names_to = &quot;variable&quot;, values_to = &quot;level&quot;) %&gt;% ggplot() + geom_point(aes(x = level, y = tensile_strength)) + geom_smooth(aes(x = level, y = tensile_strength), method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) + coord_cartesian(xlim = c(-2, 2)) + scale_y_continuous(n.breaks = 10) + facet_wrap(vars(variable)) As seen on the plot the output of our prediction is 69 corresponding the high level of A when B is at 1. To be precise we need to multiply all the coefficients by the levels of the factors as : 63.250 + 9.583x(+1) - 5.750x(+1) + 1.917 sd bars in interaction plots Here we’re making a step further in the representation of interaction plots, we’re adding error bars to the means. There are many ways to do this and we’re providing a simple approach with the function plotMeans from the package RcmdrMisc. library(RcmdrMisc) We select standard error as argument for the error.bars argument. par(mfrow = c(1,1), bty = &quot;l&quot;) plotMeans(response = pet_fct$tensile_strength, factor2 = pet_fct$A, factor1 = pet_fct$B, error.bars = &quot;se&quot;, xlab = &quot;A - Reactant&quot;, legend.lab = &quot;B - Catalist\\n(error bars +/-se)&quot;, ylab = &quot;Tensile Strenght&quot;, col = viridis::viridis(12)[4], legend.pos = &quot;bottomright&quot;, main = &quot;The PET clothing improvement plan&quot;) Coding natural values 3 factors 2 levels The litium-ion battery charging time test A - temperature B - previous cycles (within warranty) C - voltage response - charging time [h] battery_charging %&gt;% head() %&gt;% kable() A B C D Replicate charging_time -1 -1 -1 -1 1 5.50 1 -1 -1 -1 1 6.69 -1 1 -1 -1 1 6.33 1 1 -1 -1 1 6.42 -1 -1 1 -1 1 10.37 1 -1 1 -1 1 7.49 lm and anova battery_lm &lt;- lm( formula = charging_time ~ A * B * C, data = battery_charging ) summary(battery_lm) Call: lm.default(formula = charging_time ~ A * B * C, data = battery_charging) Residuals: Min 1Q Median 3Q Max -2.0950 -1.0025 -0.5288 0.9287 2.9825 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.41156 0.26542 27.924 &lt; 2e-16 *** A 0.31469 0.26542 1.186 0.247370 B 0.06844 0.26542 0.258 0.798720 C 1.04031 0.26542 3.920 0.000646 *** A:B -0.08719 0.26542 -0.328 0.745387 A:C -0.80906 0.26542 -3.048 0.005532 ** B:C 0.02594 0.26542 0.098 0.922963 A:B:C 0.03281 0.26542 0.124 0.902640 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.501 on 24 degrees of freedom Multiple R-squared: 0.5225,\tAdjusted R-squared: 0.3832 F-statistic: 3.751 on 7 and 24 DF, p-value: 0.006963 battery_aov &lt;- aov(battery_lm) summary(battery_aov) Df Sum Sq Mean Sq F value Pr(&gt;F) A 1 3.17 3.17 1.406 0.247370 B 1 0.15 0.15 0.066 0.798720 C 1 34.63 34.63 15.363 0.000646 *** A:B 1 0.24 0.24 0.108 0.745387 A:C 1 20.95 20.95 9.292 0.005532 ** B:C 1 0.02 0.02 0.010 0.922963 A:B:C 1 0.03 0.03 0.015 0.902640 Residuals 24 54.10 2.25 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The main effects of Gap and Power are highly significant (both have very small P-values). The AC interaction is also highly significant; thus, there is a strong interaction between Gap and Power. R^2 and Adjusted R^2 The ordinary R^2 is 0.9661 and it measures the proportion of total variability explained by the model. A potential problem with this statistic is that it always increases as factors are added to the model, even if these factors are not significant. The adjusted R^2 is obtained by dividing the Sums of Squares by the degrees of freedom, and is adjusted for the size of the model, that is the number of factors. battery_reduced_lm &lt;- lm( formula = charging_time ~ A + C + A:C, data = battery_charging ) summary(battery_reduced_lm) Call: lm.default(formula = charging_time ~ A + C + A:C, data = battery_charging) Residuals: Min 1Q Median 3Q Max -2.1463 -0.9950 -0.4575 0.8650 2.9050 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.4116 0.2467 30.037 &lt; 2e-16 *** A 0.3147 0.2467 1.275 0.212663 C 1.0403 0.2467 4.216 0.000235 *** A:C -0.8091 0.2467 -3.279 0.002786 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.396 on 28 degrees of freedom Multiple R-squared: 0.5185,\tAdjusted R-squared: 0.4669 F-statistic: 10.05 on 3 and 28 DF, p-value: 0.0001157 Besides the base summary() function, R squared and adjusted R squared can also be easily retrieved with the glance function from the {broom} package. We’re extracting them here for the complete and for reduced model: glance(battery_lm)[1:2] %&gt;% bind_rows(glance(battery_reduced_lm)[1:2], .id = &quot;model&quot;) # A tibble: 2 x 3 model r.squared adj.r.squared &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 0.522 0.383 2 2 0.519 0.467 Adjusted R² has improved. Removing the nonsignificant terms from the full model has produced a final model that is likely to function more effectively as a predictor of new data. Coding natural values Now that we have model often we will want to predict the response at a certainly specific level between the coded factor levels of \\(\\pm\\) 1. To do that we need to convert that specific the natural value into a coded value. Lets calculate the coded value for the factor A (gap) of which the natural value is nA = 0.9, between the natural levels of nA = 0.8 and nA = 1.2. We choose to do this for a fixed level of C of 1, corresponding to its maximum of 325W. natural2coded &lt;- function(xA, lA, hA) {(xA - (lA + hA) / 2) / ((hA - lA) / 2)} # Converting natural value xA into coded value cA: lA &lt;- 0.8 hA &lt;- 1.2 xA &lt;- 0.9 cA &lt;- natural2coded(xA, lA, hA) cA [1] -0.5 To be noted that the opposite conversion looks like: coded2natural &lt;- function(cA, lA, hA) {cA * ((hA - lA) / 2) + ((lA + hA)/2)} # Converting back the coded value cA into its natural value xA lA &lt;- 0.8 hA &lt;- 1.2 cA &lt;- -0.5 nA &lt;- coded2natural(cA, lA, hA) nA [1] 0.9 Coded values prediction And now we can feed our linear model and make predictions: battery_new &lt;- tibble(A = cA, C = 1) pA &lt;- predict(battery_reduced_lm, battery_new) pA 1 8.699062 We can visualize this outcome as follows: battery_charging %&gt;% filter(C == 1) %&gt;% ggplot() + geom_point(aes(x = A, y = charging_time, color = as_factor(C))) + geom_smooth(aes(x = A, y = charging_time), method = &quot;lm&quot;) + geom_point(aes(x = cA, y = pA)) + scale_y_continuous(n.breaks = 10) + scale_color_discrete(guide = FALSE) + theme(plot.title = ggtext::element_markdown()) + labs( title = &quot;3^k factorial design&quot;, subtitle = &quot;Prediction with reduced model&quot;) Response surface plot We are introducing here response surface plots which is yet another way to visualize the experiment outputs as a function of the inputs. We’re doing this with the persp() function from the {rsm} package which provides an extremely fast rendering, easy parametrization and a readable output. To be noted that this function is an extension of the base R persp() consisting from the R point of view in an S3 method for the lm class. This allows to simply provide directly the lm object to the function to obtain the response surface. library(rsm) persp( battery_reduced_lm, C ~ A, bounds = list(A = c(-1,1), C = c(-1,1)), theta = -40, phi = 20, r = 10, zlab = &quot;Charging Time&quot;, main = &quot;Litium-ion battery\\ncharging time test&quot; ) Due to the interaction between factors A and C the surface is slightly bent. This is exactly what we observe in the interactions plots of which the one below corresponds to slicing the surface at the min and the max of Power: interaction.plot(x.factor = battery_charging$A, trace.factor = battery_charging$C, fun = mean, response = battery_charging$charging_time, legend = TRUE, xlab = &quot;A&quot;, trace.label = &quot;C&quot;, ylab = &quot;Charging Time&quot;, main = &quot;Litium-ion battery\\ncharging time test&quot;) Just like in the surface plot we can see here in the interaction plot that the response of yield on gap is different depending on the level of power. When power is high it decreases and when power is low it increases. As a reminder this is what is called an interaction between these two factors. Single replicate designs The litium-ion battery charging time test (cont.) m factors 2 levels Possible approaches: - graphical methods–normal and half-normal probability plots; no formal tests; - assume some high-order interactions are zero, and fit a model that excludes them; degrees of freedom go into error, so testing is possible (not recommended) Figure 7: electrical car platform battery_charging %&gt;% filter((Replicate == 1)) %&gt;% head() # A tibble: 6 x 6 A B C D Replicate charging_time &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 -1 -1 -1 -1 1 5.5 2 1 -1 -1 -1 1 6.69 3 -1 1 -1 -1 1 6.33 4 1 1 -1 -1 1 6.42 5 -1 -1 1 -1 1 10.4 6 1 -1 1 -1 1 7.49 lm battery_lm3 &lt;- lm( formula = charging_time ~ A * B * C * D, data = battery_charging %&gt;% filter(Replicate ==1)) summary(battery_lm3) Call: lm.default(formula = charging_time ~ A * B * C * D, data = battery_charging %&gt;% filter(Replicate == 1)) Residuals: ALL 16 residuals are 0: no residual degrees of freedom! Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.76062 NA NA NA A -0.50812 NA NA NA B 0.03688 NA NA NA C 1.53063 NA NA NA D 0.15563 NA NA NA A:B -0.12438 NA NA NA A:C -0.76813 NA NA NA B:C -0.01062 NA NA NA A:D 0.12437 NA NA NA B:D -0.05563 NA NA NA C:D 0.16062 NA NA NA A:B:C 0.02812 NA NA NA A:B:D 0.08562 NA NA NA A:C:D 0.18438 NA NA NA B:C:D 0.03688 NA NA NA A:B:C:D -0.03688 NA NA NA Residual standard error: NaN on 0 degrees of freedom Multiple R-squared: 1,\tAdjusted R-squared: NaN F-statistic: NaN on 15 and 0 DF, p-value: NA We can see that being a single replicate design no statistics have been calculated for the effects in the model. A recommended approach in this case is to look into the normal probability plot of the model effects. qqPlot Here we are going to prepare this plot with the function qqPlot() from the {car} package: library(car) battery_eff3 &lt;- battery_lm3$coefficients[2:16] battery_eff_names2 &lt;- names((battery_lm3$coefficients)[2:16]) main_effects_plot &lt;- qqPlot( battery_eff3, envelope = 0.70, id = list( method = &quot;y&quot;, n = 5, cex = 1, col = carPalette()[1], location = &quot;lr&quot;), grid = FALSE, col = &quot;black&quot;, col.lines = &quot;black&quot;, main = &quot;Chemical vessel - Normal plot of effects 2&quot; ) In plot we can see that the effects that have the highest influence on the output are the effects A, C and D and their interactions. We can still confirm these observations with a calculation of the percentage contribution of each effect as follows: battery_lm_tidy3 &lt;- battery_lm3 %&gt;% tidy() %&gt;% filter(term != &quot;(Intercept)&quot;) %&gt;% mutate( effect_estimate = -2 * estimate, effect_estimate_sum = sum(effect_estimate), effect_contribution_perc = abs((effect_estimate/effect_estimate_sum)*100) %&gt;% round(2) ) battery_lm_tidy3 %&gt;% select(term, effect_estimate, effect_contribution_perc) %&gt;% arrange(desc(effect_contribution_perc)) %&gt;% head(8) %&gt;% kable() term effect_estimate effect_contribution_perc C -3.06125 182.35 A:C 1.53625 91.51 A 1.01625 60.54 A:C:D -0.36875 21.97 C:D -0.32125 19.14 D -0.31125 18.54 A:B 0.24875 14.82 A:D -0.24875 14.82 Reduced model Following the previous analysis we are removing the factor B from the model and keeping only the 2nd order interactions assuming the system also respects the sparcity of effects principle. battery_red_lm3 &lt;- lm( formula = charging_time ~ A + C + A:C, data = battery_charging) summary(battery_red_lm3) Call: lm.default(formula = charging_time ~ A + C + A:C, data = battery_charging) Residuals: Min 1Q Median 3Q Max -2.1463 -0.9950 -0.4575 0.8650 2.9050 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.4116 0.2467 30.037 &lt; 2e-16 *** A 0.3147 0.2467 1.275 0.212663 C 1.0403 0.2467 4.216 0.000235 *** A:C -0.8091 0.2467 -3.279 0.002786 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.396 on 28 degrees of freedom Multiple R-squared: 0.5185,\tAdjusted R-squared: 0.4669 F-statistic: 10.05 on 3 and 28 DF, p-value: 0.0001157 We can now see that we’ve regained degrees of freedom and obtained a sort of hidden replication allowing to calculate statistics and error terms on the model. Residuals analysis Checking the residuals we see the significant effect of the remaining interactions. The residuals are not completely normal but the in the standardized residuals the deviations are contained within 1.2 sd. par(mfrow = c(2,2)) plot(battery_red_lm3) We can now establish the main effects and interaction plots and conclude on the optimal settings to maximize the output: A and D should be on the max and C on the min. "],
["SPC.html", "Statistical Process Control xbar-R charts Cpk charts I-MR charts", " Statistical Process Control Keeping the variability of an industrial process under control is one of the most important objectives in manufacturing. Based on expert knowledge or on detailed functional analysis the product and process parameters that are critical to quality are identified and selected for close follow-up. The most common and effective way for such follow-up is the Statistical Process Control which is done by using control charts. The syringe injection molding process Figure 1: A clean room for syringe injection molding with fully automatised handling xbar-R charts There are many types of control charts and in this case study we’re demonstrating the xbar and R charts. These two charts are often used together and are suited to the control the mean and the variability of a continuous variable. Bamako Lightening is a company that manufactures lamps. The weight of each lamp is critical to the quality of the product. The Production Operator monitors the production process using xbar and R-charts. Samples are taken of six lamps every hour and their means and ranges plotted on control charts. Data is available representing samples taken a period of 25 hours of production. Looking at the first five lines to confirm and assess the quality of our data for further processing. head(syringe_diameter) %&gt;% kable() Hour Sample1 Sample2 Sample3 Sample4 Sample5 Sample6 Hour1 5.331433 5.339867 5.324400 5.336267 5.322833 5.318133 Hour2 5.324033 5.321433 5.314200 5.323733 5.341967 5.339233 Hour3 5.326267 5.340433 5.313567 5.356533 5.338700 5.356967 Hour4 5.355267 5.360000 5.317133 5.331933 5.344600 5.347400 Hour5 5.337867 5.326367 5.314967 5.313433 5.337467 5.340700 Hour6 5.343167 5.335233 5.323833 5.346267 5.333967 5.320533 We see that in this table each line corresponds to a sampling hour and each column corresponds to a sample number. We’re now going to pass this data to the control chart plotting function qcc(). As this function takes a dataset of observations so we’re removing the Hour column with the select function from tidyverse: syringe_clean &lt;- syringe_diameter %&gt;% select(-Hour) %&gt;% mutate(across(starts_with(&quot;S&quot;), round, 2)) Now we load the qcc package that has the required quality control tools: Calibration run In order to establish a control chart it is recommended to run a “calibration run.” The calibration run is used to calculate the control limits before entering “regular production.” Using the first 10 samples we call the qcc() function to make the required calculations. Mean chart library(qcc) syringe_xbar &lt;- qcc( syringe_clean[1:10, ], type = &quot;xbar&quot;, title = &quot;Lamp weight \\n xbar chart&quot;, xlab = &quot;Sample group&quot;, plot = FALSE ) Before we step ahead and simply plot the SPC chart and interpret the results lets look a bit in detail in the calculations done to established the Control Chart. To do this we’re going to go in the details of what we’ve obtained in the previous chunk. A first step is to read the begining of the qcc() help file typing ?qcc in the console. It says \"Create an object of class ‘qcc’ to perform statistical process control’ (in R technical terms function is a helper that generates an S3 R object). The key point here is that this means we can inspect the calculations separately from the plot itself. We can start by confirming the class and the type of the qcc object: class(syringe_xbar) [1] &quot;qcc&quot; typeof(syringe_xbar) [1] &quot;list&quot; It is confirmed it is an object of class qcc with the R type list. Looking into the structure of the list: str(syringe_xbar) List of 11 $ call : language qcc(data = syringe_clean[1:10, ], type = &quot;xbar&quot;, plot = FALSE, title = &quot;Lamp weight \\n xbar chart&quot;, xlab = &quot;Sample group&quot;) $ type : chr &quot;xbar&quot; $ data.name : chr &quot;syringe_clean[1:10, ]&quot; $ data : num [1:10, 1:6] 5.33 5.32 5.33 5.36 5.34 5.34 5.3 5.32 5.34 5.36 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ Group : chr [1:10] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... .. ..$ Samples: chr [1:6] &quot;Sample1&quot; &quot;Sample2&quot; &quot;Sample3&quot; &quot;Sample4&quot; ... $ statistics: Named num [1:10] 5.33 5.33 5.34 5.34 5.33 ... ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... $ sizes : int [1:10] 6 6 6 6 6 6 6 6 6 6 $ center : num 5.33 $ std.dev : num 0.0142 $ nsigmas : num 3 $ limits : num [1, 1:2] 5.32 5.35 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr &quot;&quot; .. ..$ : chr [1:2] &quot;LCL&quot; &quot;UCL&quot; $ violations:List of 2 ..$ beyond.limits : int(0) ..$ violating.runs: num(0) - attr(*, &quot;class&quot;)= chr &quot;qcc&quot; The output is not easy to read but we present it here just to show that inside the list there are several tables with the statistical analysis required for our plot. If we want like to see for instance the standard deviation we can extract it separately: syringe_xbar$std.dev [1] 0.01420679 And if we want like to see a summary of all the data stored in the object we could apply the summary method: summary(syringe_xbar) Call: qcc(data = syringe_clean[1:10, ], type = &quot;xbar&quot;, plot = FALSE, title = &quot;Lamp weight \\n xbar chart&quot;, xlab = &quot;Sample group&quot;) xbar chart for syringe_clean[1:10, ] Summary of group statistics: Min. 1st Qu. Median Mean 3rd Qu. Max. 5.325000 5.328333 5.333333 5.333333 5.337500 5.343333 Group sample size: 6 Number of groups: 10 Center of group statistics: 5.333333 Standard deviation: 0.01420679 Control limits: LCL UCL 5.315934 5.350733 We are now ready to finally we can see this all together in a plot: plot(syringe_xbar) Range chart Using the same 10 first samples we also obtain the corresponding R chart: syringe_R &lt;- qcc( syringe_clean[1:10, ], type = &quot;R&quot;, title = &quot;Lamp weight \\n R chart&quot;, xlab = &quot;Sample group&quot; ) Regular production Now that the calibration data has been plotted we can consider that the control limits are defined. They can become fixed and reused in new plots for the future production runs. Samples from those future runs can then be assessed against this limits and the control chart rules can be verified (in this example the shewhart rules are used). We now add the remaining data points to our chart by specifying which lines we’re refering too in our dataframe in the ‘newdata’ argument: syringe_xbar &lt;- qcc( data = syringe_clean[1:10, ], newdata = syringe_clean[11:25,], type = &quot;xbar&quot;, title = &quot;Lamp weight \\n xbar chart&quot;, xlab = &quot;Sample group&quot; ) We can see that the data point corresponding to the average of the measurements of the samplegroup 17 is plotted in red because it is outside of the control limits. Now we plot the R chart to assess the variability: syringe_R &lt;- qcc( data = syringe_clean[1:10, ], newdata = syringe_clean[11:25,], type = &quot;R&quot;, title = &quot;Lamp weight \\n R chart&quot;, xlab = &quot;Sample group&quot; ) In this case all the points are within the previously defined control limits. Warnings and specification limits More tight controls can be put in place by clearly identifying warning limits in a narrower range than the control limits. These measures need to be accompaigned by clear decision criteria and proper training to avoid the typical problem of overeacting and destabilizing the process by introducing unintented special causes of variation. Control limits We add warning limits in the plot with as follows: warn.limits &lt;- limits.xbar( syringe_xbar$center, syringe_xbar$std.dev, syringe_xbar$sizes, 2 ) plot( syringe_xbar, restore.par = FALSE, title = &quot;Lamp weight \\n xbar chart&quot;, xlab = &quot;Sample group&quot;) abline(h = warn.limits, lty = 3, col = &quot;chocolate&quot;) A manufacturing process under control has a variation that is lower than the product specifications and ideally it is centered. Therefore it is usually good practice to follow the control chart rules refering to the process control limits. In some cases nevertheless there may be desired or interesting to add the specification limits. This can be done as follows, first we establish the specifications: spec_max &lt;- 5.6 spec_min &lt;- 5.3 spec_tgt &lt;- (spec_max - spec_min) / 2 + spec_min specs &lt;- c(spec_min, spec_tgt, spec_max) and replot the control chart with visible specification limits and targets: plot( syringe_xbar, restore.par = FALSE, title = &quot;Lamp weight \\n xbar chart&quot;, xlab = &quot;Sample group&quot;, ylim = c(specs[1], specs[3]) ) abline(h = specs, lty = 3, col = &quot;red&quot;) In the previous example we see a situation that happens in practice and that requires action: the data plotted is still within the min max specification limits for this relativelly small number of data points. Furthermore the variation is overall well contained within the process limits. Nevertheless we see it is extremelly off centered when compared with the product specification. A process capability study should help determining the causes for this offcentering and help correcting it. Adapted from Bass (2007) In this chapter we’re going to go more in depth in the study of the manufacturing process variability. We’re going to make a comparison between the product specifications and the process variability. We’re looking for opportunities to tigthen the product specifications. Tightening a product specification without increasing the cost of a manufacturing cost can be a source of competitive advantage. Cpk charts Off spec syringe_long &lt;- syringe_diameter %&gt;% pivot_longer(cols = starts_with(&quot;Sample&quot;), names_to = &quot;sample&quot;, values_to = &quot;value&quot;) variables syringe_mean = syringe_long %&gt;% pull(value) %&gt;% mean() syringe_sd = syringe_long %&gt;% pull(value) %&gt;% sd() syringe_n &lt;- length(syringe_long) theor_n = 1000000 calculation: probability of being between the limits off_spec &lt;- function(UCL, LCL, mean, sd) { round(100 - ((stats::pnorm(UCL, mean, sd) - stats::pnorm(LCL, mean, sd))*100), 2) } syringe_off_spec &lt;- off_spec(spec_max, spec_min, syringe_mean, syringe_sd) syringe_theor &lt;- rnorm(n = theor_n, mean = syringe_mean, sd = syringe_sd) %&gt;% as_tibble() plot_subtitle &lt;- paste( &quot;Spec: [&quot;, spec_min, &quot;;&quot;, spec_max, &quot;], Proportion off-spec = &quot;, signif(syringe_off_spec, digits = 2), &quot;%&quot; ) Note that we deliberately twick the plot colors to make it look like the plots from minitab and from the qcc package. We provide this theme in the book companion package industRial with the name theme_qcc. syringe_long %&gt;% ggplot(aes(x = value, y = ..density..)) + geom_histogram( bins = 30, fill = &quot;white&quot;, color = &quot;grey20&quot;) + geom_density(data = syringe_theor, linetype = 2) + geom_vline(xintercept = {spec_min}, color = &quot;red&quot;, linetype = 3) + geom_vline(xintercept = {spec_max}, color = &quot;red&quot;, linetype = 3) + geom_vline(xintercept = {spec_tgt}, color = &quot;red&quot;, linetype = 2) + scale_x_continuous(n.breaks = 10) + theme_qcc() + labs( title = &quot;Out of specification (Expected)&quot;, subtitle = {plot_subtitle}) By looking at the histogram of the Bamako lightning dataset we confirm the extreme offcentering of the production. We also see that although there are no measurements beyond the lower specification limit (LSL) it is very likely this will happen soon. We can also calculate the Cpk process_Cpk &lt;- function(UCL, LCL, mean, sd) { pmin( (abs(mean - abs(LCL)) / (3 * sd)), (abs((abs(UCL) - mean)) / (3 * sd)) ) } process_Cpk(spec_max, spec_min, syringe_mean, syringe_sd) [1] 0.7158694 And convert the percentage out of spec in parts per million. We’re not considering the 1.5 shift that sometimes is presented in the literature but rather making a simple direct conversion of the proportion out of spec found before: formatC(((syringe_off_spec) * 10000), format = &quot;d&quot;, big.mark = &quot;&#39;&quot;) [1] &quot;15&#39;900&quot; The expected population below the LSL is 1,3% which is very high for industry standards. In fact this corresponds to 15’900 parts per million (ppm) whereas a common target would be 1 ppm. Naturally these figures are indicative and they depend of the context criteria such as severity of the problem, cost, difficulty to eliminate the problem and so on. We can now establish a simple table using the functions created before, to present the expected percentage that falls within certain limits. To make it useful as a reference table we’re putting this limits from \\(\\pm\\) 1 to \\(\\pm\\) 6 standard deviations sigma_limits &lt;- tibble( UCL = c(1, 2, 3, 4, 5, 6), LCL = -UCL, mean = 0, sd = 1 ) sigma_limits %&gt;% mutate( in_spec_percent = 100 - off_spec(UCL, LCL, mean, sd), Cpk = process_Cpk(UCL, LCL, mean, sd), ppm_defects = formatC( off_spec(UCL, LCL, mean, sd) * 10000, format = &quot;d&quot;, big.mark = &quot;&#39;&quot;)) %&gt;% kable(align = &quot;c&quot;, digits = 2) UCL LCL mean sd in_spec_percent Cpk ppm_defects 1 -1 0 1 68.27 0.33 317’300 2 -2 0 1 95.45 0.67 45’500 3 -3 0 1 99.73 1.00 2’700 4 -4 0 1 99.99 1.33 100 5 -5 0 1 100.00 1.67 0 6 -6 0 1 100.00 2.00 0 Capability chart syringe_cpk &lt;- process.capability( syringe_xbar, breaks = 10, spec.limits = c(spec_min, spec_max), target = spec_tgt, digits = 2, print = FALSE, std.dev = syringe_sd ) A fine tuning of the forecast of the number of expected parts out of specification can be done with the parameter std.dev. The input value will be used in the probability distribution function. Different approaches can be considered: calculating the sandard deviation within each subgroup or the standard deviation of the entire population and also correcting the standard deviation dividing by n or by n - 1. In this example we re-use the standard deviation calculated on the entire set of datapoints as the group is small but for a case with more data it would be interesting to used the subgroups that tend to give smaller standard deviations. I-MR charts In this final chapter we’re exploring the development of custom functions for summary statistics and timeseries plotting. All these functions are available on the book companion package {industRial} for exploration and further development. They don’t pretend to be used as such for real life applications. For that we recommend the functions from the package {QCC} presented before. The objective here is to show a workflow and demonstrate some possibilities that the {tidyverse} offers to make completely customized functions. To encourage this exploration we’re not presenting here the complete code for each function but propose to check it with the R functionality for function code exploration. We see often the recommendation to read R source code and we can only support it as an excellent way to develop our skilset. Lets start with the simple function that calculates the percentage of parts out of specification given the specification limits, the process mean and standard deviation. This function was presented in the previous case study and since it is loaded in memory we can read its content with the R function body(): body(off_spec) { round(100 - ((stats::pnorm(UCL, mean, sd) - stats::pnorm(LCL, mean, sd)) * 100), 2) } we can see that it uses simple functions from the package {stats}. We can also explicitly request to see the arguments it takes with formals(): dput(formals(off_spec)) as.pairlist(alist(UCL = , LCL = , mean = , sd = )) and for a complete review we can open the function help page with: ?off_spec Lets give some data and use the function: off_spec(0.981, 0.819, 0.943, 0.019) [1] 2.28 we get 2.28% parts out of spec. We’ll see this calculation in action in a moment. Process stats The tablet weight control procedure tablet_weight &lt;- tablet_weight %&gt;% janitor::clean_names(case = &quot;snake&quot;) We’re now going to use the function process stats to calculate several statistical data for this dataset. As mentionned we encourage the reader to explore the code with body(process_stats) and dput(formals(process_stats)) as there is a wealth of details in how to calculate process control limits, moving ranges and the like. weight_statistics_data &lt;- process_stats(tablet_weight, 9) this being done we can now convert this data into an easy readable format for reporting of for a future integration in a shiny app for example. We’re exploring the package {gt} that has a specific very neat look rather different from the {kable} package used in most of the book. process_stats_table(weight_statistics_data) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #jpncqvnjiw .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #jpncqvnjiw .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #jpncqvnjiw .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #jpncqvnjiw .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #jpncqvnjiw .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #jpncqvnjiw .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #jpncqvnjiw .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #jpncqvnjiw .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #jpncqvnjiw .gt_column_spanner_outer:first-child { padding-left: 0; } #jpncqvnjiw .gt_column_spanner_outer:last-child { padding-right: 0; } #jpncqvnjiw .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #jpncqvnjiw .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #jpncqvnjiw .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #jpncqvnjiw .gt_from_md > :first-child { margin-top: 0; } #jpncqvnjiw .gt_from_md > :last-child { margin-bottom: 0; } #jpncqvnjiw .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #jpncqvnjiw .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #jpncqvnjiw .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #jpncqvnjiw .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #jpncqvnjiw .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #jpncqvnjiw .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #jpncqvnjiw .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #jpncqvnjiw .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #jpncqvnjiw .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #jpncqvnjiw .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #jpncqvnjiw .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #jpncqvnjiw .gt_sourcenote { font-size: 90%; padding: 4px; } #jpncqvnjiw .gt_left { text-align: left; } #jpncqvnjiw .gt_center { text-align: center; } #jpncqvnjiw .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #jpncqvnjiw .gt_font_normal { font-weight: normal; } #jpncqvnjiw .gt_font_bold { font-weight: bold; } #jpncqvnjiw .gt_font_italic { font-style: italic; } #jpncqvnjiw .gt_super { font-size: 65%; } #jpncqvnjiw .gt_footnote_marks { font-style: italic; font-size: 65%; } Process Summary Statistics Variable Value Unit Weight mean 0.94 g Spec target 0.90 g Spec min 0.82 g Spec max 0.98 g Out of spec 2.39 % Sample size 137 parts Individual chart The data set being available we’re feeding it into the chart_I() function: chart_I(weight_statistics_data) Moving range chart The companion of the I chart is the MR chart, where MR stands for moving range. This chart can be called with: chart_IMR(weight_statistics_data) Capability chart II And a final chart for this session the capability chart: chart_Cpk(weight_statistics_data) "],
["glossary.html", "Glossary Statistics DFSS MSA DOE", " Glossary Statistics Statistic concepts are picked up and applied throught the Cases Studies on a needed basis. To get a better understanding of how they fit together we are reminding below some definitions coming from Yakir (2011). For a deep and comprehensive course on statistics we recommend the free online kahn academy courses. Notation conventions The arithmetic mean of a series of values x1, x2, …, xn is often denoted by placing an “overbar” over the symbol, e.g. \\(\\bar{x}\\) , pronounced “x bar.” Some commonly used symbols for sample statistics are: sample mean \\(\\bar{x}\\), sample standard deviation s. Some commonly used symbols for population parameters: population mean μ, population standard deviation σ. Random variables are usually written in upper case roman letters: \\(X, Y\\), etc. Particular realizations of a random variable are written in corresponding lower case letters. For example, x1, x2, …, xn could be a sample corresponding to the random variable \\(X\\). A cumulative probability is formally written P(\\(X\\)≤x) to differentiate the random variable from its realization. Greek letters (e.g. θ, β) are commonly used to denote unknown parameters (population parameters). Placing a hat, or caret, over a true parameter denotes an estimator of it, e.g., \\(\\hat{θ}\\) is an estimator for θ. Descriptive statistics Statistic: A numerical characteristic of the data. A statistic estimates the corresponding population parameter. Population: The collection, or set, of all individuals, objects, or measurements whose properties are being studied. Sample: A portion of the population understudy. A sample is representative if it characterizes the population being studied. Frequency: The number of times a value occurs in the data. Relative Frequency: The ratio between the frequency and the size of data. \\(f / n\\) Median: A number that separates ordered data into halves. Mean: A number that measures the central tendency. A common name for mean is ‘average.’ Sample size: \\(n\\) Sample mean: \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\sum_{x}(x\\times f_x / n)\\) Population size: \\(N\\) Population mean: \\(\\bar{\\mu} = \\frac{\\sum_{i=1}^{N}x_i}{N}\\) Variance: Mean of the squared deviations from the mean. Sample variance: \\(s^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})^2 = \\frac{n}{n-1}\\sum_{x}((x-\\bar{x})^2\\times(f_x/n))\\) Standard Deviation: A number that is equal to the square root of the variance and measures how far data values are from their mean. Sample standard deviation: \\(\\sqrt[]{s^{2}}\\) Probability Random Variable: The probabilistic model for the value of a measurement, before the measurement is taken (e.g. Binomial, Poisson, Uniform, Exponential, Normal). It is denoted with latin capitals \\(X, Y\\) and \\(Z\\) Expectation: The central value for a random variable. The expectation of the random variable X is marked by E(\\(X\\)). Variance: The (squared) spread of a random variable. The variance of the random variable X is marked by Var(\\(X\\)). Normal Random Variable: A bell-shaped distribution that is frequently used to model a measurement. The distribution is marked with Normal(\\(\\mu,\\sigma^2\\)). Standard Normal Distribution: The Normal(0,1). The distribution of stan- dardized Normal measurement. Percentile: Given a percent p · 100% (or a probability p), the value x is the percentile of a random variable X if it satisfies the equation P\\((X ≤ x) = p\\). Sampling distribution Random Sample: The probabilistic model for the values of a measurements in the sample, before the measurement is taken. Sampling Distribution: The distribution of a random sample. Sampling Distribution of a Statistic: A statistic is a function of the data; i.e. a formula applied to the data. The statistic becomes a random variable when the formula is applied to a random sample. The distribution of this random variable, which is inherited from the distribution of the sample, is its sampling distribution. Sampling Distribution of the Sample Average: The distribution of the sample average, considered as a random variable. The Law of Large Numbers: A mathematical result regarding the sampling distribution of the sample average. States that the distribution of the av- erage of measurements is highly concentrated in the vicinity of the expec- tation of a measurement when the sample size is large. The Central Limit Theorem: A mathematical result regarding the sampling distribution of the sample average. States that the distribution of the average is approximately Normal when the sample size is large. (note: the central limit theorem is a key notion for understanding industrial measurement and its consequences will be applied in most case studies) Expectation of the sample average: the expectation of the sample mean is equal to the theoretical expectation of its components E\\((\\bar{X})\\) = E(\\(X\\)) Variance of the sample average: the variance of the sample average is equal to the variance of each of the components, divided by the sample size Var(\\(X\\)) = Var\\((X)/n\\) Statistical Inference Statistical Inference: Methods for gaining insight regarding the population parameters from the observed data. Point Estimation: An attempt to obtain the best guess of the value of a population parameter. An estimator is a statistic that produces such a guess. The estimate is the observed value of the estimator. Confidence Interval: An interval that is most likely to contain the population parameter. The confidence level of the interval is the sampling probability that the confidence interval contains the parameter value. Hypothesis Testing: A method for determining between two hypothesis, with one of the two being the currently accepted hypothesis. A determination is based on the value of the test statistic. The probability of falsely rejecting the currently accepted hypothesis is the significance level of the test. Comparing Samples: Samples emerge from different populations or under different experimental conditions. Statistical inference may be used to compare the distributions of the samples to each other. Regression: Relates different variables that are measured on the same sample. Regression models are used to describe the effect of one of the variables on the distribution of the other one. The former is called the explanatory variable and the later is called the response. Bias: The difference between the expectation of the estimator and the value of the parameter. An estimator is unbiased if the bias is equal to zero. Otherwise, it is biased. Mean Square Error (MSE): A measure of the concentration of the distribu- tion of the estimator about the value of the parameter. The mean square error of an estimator is equal to the sum of the variance and the square of the bias. If the estimator is unbiased then the mean square error is equal to the variance. Confidence Level: The sampling probability that random confidence intervals contain the parameter value. The confidence level of an observed interval indicates that it was constructed using a formula that produces, when applied to random samples, such random intervals. Null Hypothesis (\\(H0\\)): A sub-collection that emerges in response to the sit- uation when the phenomena is absent. The established scientific theory that is being challenged. The hypothesis which is worse to erroneously reject. Alternative Hypothesis (\\(H1\\)): A sub-collection that emerges in response to the presence of the investigated phenomena. The new scientific theory that challenges the currently established theory. Test Statistic: A statistic that summarizes the data in the sample in order to decide between the two alternative. Rejection Region: A set of values that the test statistic may obtain. If the observed value of the test statistic belongs to the rejection region then the null hypothesis is rejected. Otherwise, the null hypothesis is not rejected. Type I Error: The null hypothesis is correct but it is rejected by the test. Type II Error: The alternative hypothesis holds but the null hypothesis is not rejected by the test. Significance Level: The probability of a Type I error. The probability, com- puted under the null hypothesis, of rejecting the null hypothesis. The test is constructed to have a given significance level. A commonly used significance level is 5%. Statistical Power: The probability, computed under the alternative hypoth- esis, of rejecting the null hypothesis. The statistical power is equal to 1 minus the probability of a Type II error. \\(p\\)-value: A form of a test statistic. It is associated with a specific test statistic and a structure of the rejection region. The p-value is equal to the signif- icance level of the test in which the observed value of the statistic serves as the threshold. DFSS Quality tools have been grouped under varied names and methodologies being Six Sigma one of the most comprehensives ones. One way of summarising the Six Sigma framework is presented below in a step by step approach with definitions. Each steps consists of an analyis of the product development and production process that progressively refines the final product specifications. For a more detailed description we recommend reviewing the comprehensive Six Sigma certification reference book by Roderik A.Munro and J.Zrymiak (2015). Voice of Customer 1. Product brief List of the product features expected by the customer (internal or external), including qualitative indication of the acceptance limits. 2. Functional analysis Translation of the product attributes into lower level functions including interactions between product components and requirements induced by each component on the others. 3. Failure modes and effects analysis (FMEA) List of critical product features with failure causes, effects, detection and action plans, rated and sorted by criticality. 4. Product specifications and parts drawings Implementation of the product components into unique formulations and drawings including detailed values and tolerances of it characteristics (physical, chemical or electric or others). Voice of Process 1. Process mapping A visual diagram of the production process with inputs and outputs for each step. 2. Process FMEA List of critical production process steps with failure causes, effects, detection and action plans, rated and sorted by criticality. 3. Quality Control plan List of control points including measurement method, sample size, frequency and acceptance criteria. When needed, critical control points are handled by Statistical Process Control (SPC) 4. Measurement system analysis A thorough assessment of a measurement process, and typically includes a specially designed experiment that seeks to identify the components of variation in that measurement process. 5. Process capability analysis Comparison of the variability of a production process with its engineered specifications. MSA It is a fact that different communities utilize different methodologies and terminologies on the domain of measurement uncertainty. Unfortunately these differences are still too often overlapping, see J E Muelaner (2015) for detailed comparison. In our text we opt for the industry terminology, in particular to the norm ISO 5725, the practical application guides from Automotive Industry Action Group (2010) and some articles on Minitab (2019b) which itself is based on the AIAG guidelines. Variance components assess the amount of variation contributed by each source of measurement error, plus the contribution of part-to-part variability. The sum of the individual variance components equals the total variation. total gage r&amp;R: the sum of the repeatability and the reproducibility variance components. part: The variation that comes from the parts, with 5 levels in this case. operator: The variation that comes from the operators, with 3 levels in this case. replicants, n: number of replications corresponding to the number of times each part is measured by each operator. repeatability (or error, or residuals): The variation that is not explained by part, operator, or the operator and part interaction. It represents how much variability is caused by the measurement device (the same operator measures the same part many times, using the same gage). The repeatability can be measured directly from the Anova table from the residual mean squares. reproducibility: how much variation is caused by the differences between operators (different operators measure the same part many times, using the same gage). operators: the operators part of the reproducibility is the operators variation minus the interaction divided by the number of different parts times the replicants (zero if negative). parts:operators: The variation that comes from the operator and part interaction. An interaction exists when an operator measures different parts differently. The interaction part of of the reproducibility is the interaction minus the repeatability divided by the number of replicants (zero if negative). part-to-part: the variability due to different parts. Ideally, very little should be due to repeatability and reproducibility. Differences between parts should account for most of the variability (when the %Contribution from part-to-part variation is high, the measurement system can reliably distinguish between parts). The sum of the individual variance components equals the total variation. Accuracy/Uncertainty: Combination of precision and trueness. In the ISO 5725 both terms are equivalent. Precision: Combination of repeatability and reproducibility Trueness: difference between the mean of many measurements and the reference value. In ISO 5725 the term bias has been replaced by trueness. DOE Below key definitions from Montgomery (2012), complemented with Wikipedia article details on the same topics. Randomization: both the allocation of the experimental material and the order in which the individual runs of the experiment are to be performed are randomly determined. Replicate: independent repeat run of each factor combination. Factorial design: in each complete trial or replicate of the experiment all possible combinations of the levels of the factors are investigated. Crossed factors: factors arranged in a factorial design. Coded variable: the \\(\\pm\\) 1 coding for the low and high levels of the factors. Coded variables are very effective for determining the relative size of factor effects. In almost all situations, the coded unit analysis is preferable. Contrast: a linear combination of parameters in the form \\(\\tau=\\sum_{i=1}^{a}c_i\\mu_i\\) where the contrast constants \\(c_1,c_2, ..., c_a\\) sum to zero; that is, \\(\\sum_{i=1}^{a}c_i=0\\). Orthogonal contrasts: two contrasts with coefficients \\({c_i}\\) and \\({d_i}\\) are orthogonal if: \\(\\sum_{i=1}^{a}c_id_i\\). In a balanced one-way analysis of variance, using orthogonal contrasts has the advantage of completely partitioning the treatment sum of squares into non-overlapping additive components that represent the variation due to each contrast. Contrasts then allow for the comparison between the different means. sparsity of effects principle: states that most systems are dominated by some of the main effects and low-order interactions, and most high-order interactions are negligible. "],
["imprint.html", "Imprint", " Imprint Many packages are available for editing documentation, from notes to blogs up to complete websites. In this book we’ve opted to use the R package {Bookdown} from Yihui Xie further customized with a layout developed by Matthew J. C. Crump. An important aspect to ensure reproducibility of the examples along the time and between users is to have the same programming setup. We’re showing below our setup at the time of rendering the book. devtools::session_info()[[1]] ## Error in get(genname, envir = envir) : objet &#39;testthat_print&#39; introuvable ## setting value ## version R version 4.0.2 (2020-06-22) ## os Windows 10 x64 ## system x86_64, mingw32 ## ui RTerm ## language (EN) ## collate French_Switzerland.1252 ## ctype French_Switzerland.1252 ## tz Europe/Berlin ## date 2021-05-21 Disclaimer This book presents a variety of software tools and recommended approaches for industrial data analysis. It is incumbent upon the user to execute judgment in their use. The author does not provide any guarantee, expressed or implied, with regard to the general or specific applicability of the software, the range of errors that may be associated with it, or the appropriateness of using them in any subsequent calculation, design, or decision process. The author accepts no responsibility for damages, if any, suffered by any reader or user of this handbook as a result of decisions made or actions taken on information contained therein. Licence This book and its companion package are made available under a GPLv3 licence granting end users the freedom to run, study, share, and modify the software. "],
["references.html", "References", " References A good mastership of the vast domain of Industrial Data Science can take several years and can only be obtained by a strong combination of theory and practice. As mentionned in the introduction chapter, our book is focused on the practice and in this bibliography we find some the necessary supporting theory. The list below collects websites, books and articles referenced throughout this book. It is a curated set of some of the most relevant works available today in Six Sigma, Statistics, Data Science and programming with R. Automotive Industry Action Group, AIAG -, ed. 2010. https://www.aiag.org/store/publications/details?ProductCode=MSA-4. Bass, Issa. 2007. Six Sigma Statistics with Excel and Minitab. 1st ed. McGraw-Hill. Bell, Stéphanie. 2001. The Beginers Guide to Uncertainty of Measurement. 2nd ed. Teddington, Middlesex, United Kingdom: National Phisical Laboratory. https://www.npl.co.uk/special-pages/guides/gpg11_uncertainty. Broc, Guillaume. 2016. Stats Faciles Avec r. 1th ed. deBoeck. Emilio L. Cano, Andrés Redchuk, Javier M. Moguerza. 2012. Six Sigma with r. 1th ed. Use r! Springer New York Heidelberg Dordrecht London: Springer. Emilio L. Cano • Javier M. Moguerza, Mariano Prieto Corcoba. 2015. Quality Control with r. 1th ed. Use r! Springer New York Heidelberg Dordrecht London: Springer. J E Muelaner, M Chappell, A Francis. 2015. “A Hybrid Measurement Systems Analysis and Uncertainty of Measurement Approach for Industrial Measurement in the Light Controlled Factory.” Laboratory for Integrated Metrology Applications, Dep. Of Mechanical Engineering, University of Bath, Bath, UK. Minitab. 2016. “Understanding Analysis of Variance (ANOVA) and the f-Test.” https://blog.minitab.com/en/adventures-in-statistics-2/understanding-analysis-of-variance-anova-and-the-f-test. ———. 2019a. “Interpret the Key Results for 2 Variances.” https://support.minitab.com/en-us/minitab/18/help-and-how-to/statistics/basic-statistics/how-to/2-variances/interpret-the-results/key-results/. ———. 2019b. “What Is a Gage Repeatability and Reproducibility (r&amp;r) Study?” https://support.minitab.com/en-us/minitab/18/help-and-how-to/quality-and-process-improvement/measurement-system-analysis/supporting-topics/gage-r-r-analyses/what-is-a-gage-r-r-study/. Montgomery, Douglas C. 2012. Design and Analysis of Experiments. 8th ed. Wiley. Roderik A.Munro, Govindarajan Ramu, and Daniel J.Zrymiak. 2015. The Certified Six Sigma Green Belt Handbook. 2nd ed. Milwaukee, Winsconsin: ASQ Quality Press. Scrucca, Luca. 2004. “Qcc: An r Package for Quality Control Charting and Statistical Process Control.” R News 4/1: 11–17. Yakir, Benjamin. 2011. Introduction to Statistical Thinking (with r, Without Calculus). Department of Statistics, The Hebrew University of Jerusalem. "]
]
